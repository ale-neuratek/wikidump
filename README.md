# üß† WikiDump - Procesador de Wikipedia Optimizado

## üìã Estado Actual del Proyecto

**‚úÖ FUNCIONAL** - Pipeline completo implementado y validado
- ‚ö° Procesamiento optimizado para datasets masivos
- üè∑Ô∏è Categorizaci√≥n inteligente autom√°tica  
- üí¨ Generaci√≥n de conversaciones usando content_manager
- üß† Categor√≠a "consciencia" autom√°tica (NO conscious.txt)
- üìä Logging detallado cada 1-5 minutos
- üîß Configuraci√≥n adaptativa seg√∫n tama√±o del dataset

## üóÇÔ∏è Archivos Esenciales

### üì¶ Procesamiento Principal
- **`adaptive_processor.py`** - Coordinador optimizado del pipeline
- **`simple_processor.py`** - Procesador masivo paralelo
- **`content_manager.py`** - Gestor de contenido y categorizaci√≥n
- **`hardware_configs.py`** - Configuraci√≥n adaptativa de hardware

### ÔøΩÔøΩÔ∏è Pipeline Completo  
- **`caroline_ultra_extractor_hybrid.py`** - Extractor de art√≠culos (Etapa 1)
- **`main_wikidump_processor.py`** - Pipeline principal (Etapas 1+2)

### ‚öôÔ∏è Configuraci√≥n
- **`requirements.txt`** - Dependencias Python
- **`init.sh`** - Script de configuraci√≥n completa (NO commiteado)

## üöÄ Uso R√°pido

### Opci√≥n 1: Solo Etapa 2 (Generar Conversaciones)
```bash
python adaptive_processor.py --input data_ultra_hybrid --output wiki_conversations
```

### Opci√≥n 2: Pipeline Completo (XML ‚Üí Conversaciones)
```bash
python main_wikidump_processor.py --xml data_wiki/eswiki.xml --output wiki_conversations
```

## üìä Rendimiento Validado

- **623,708 conversaciones** generadas en pruebas
- **48 archivos JSONL** creados correctamente  
- **8 categor√≠as principales** descubiertas autom√°ticamente
- **344MB** de datos de entrenamiento generados
- **~70 art√≠culos/segundo** de throughput

## ‚ö†Ô∏è Notas Importantes

### ‚úÖ Funcionalidades Implementadas
- [x] NO genera conscious.txt (eliminado)
- [x] Categor√≠a "consciencia" generada al final 
- [x] Todos los art√≠culos procesados via content_manager
- [x] Logging con timestamps cada 1-5 minutos
- [x] Configuraci√≥n adaptativa seg√∫n dataset
- [x] Procesamiento masivo optimizado

### üîß Configuraci√≥n Pendiente  
- ‚ö†Ô∏è **Cola llena (101 reintentos)** - Requiere afinaci√≥n de `hardware_configs.py`
- üìà Optimizaci√≥n para datasets >1M art√≠culos

## üèóÔ∏è Arquitectura

```
INPUT (XML/JSONL) 
    ‚Üì
caroline_ultra_extractor_hybrid.py (Etapa 1)
    ‚Üì
data_ultra_hybrid/ (JSONL)
    ‚Üì
adaptive_processor.py (Coordinador)
    ‚Üì
simple_processor.py (Procesamiento Masivo)
    ‚Üì
content_manager.py (Categorizaci√≥n + Conversaciones)
    ‚Üì
OUTPUT (Conversaciones JSONL + Categor√≠a Consciencia)
```

## üìÅ Estructura de Salida

```
wiki_conversations/
‚îú‚îÄ‚îÄ arte/
‚îÇ   ‚îî‚îÄ‚îÄ conversaciones_arte_*.jsonl
‚îú‚îÄ‚îÄ geografia/
‚îÇ   ‚îî‚îÄ‚îÄ conversaciones_geografia_*.jsonl
‚îú‚îÄ‚îÄ historia/
‚îÇ   ‚îî‚îÄ‚îÄ conversaciones_historia_*.jsonl
‚îú‚îÄ‚îÄ consciencia/           # ‚Üê Nueva categor√≠a especial
‚îÇ   ‚îú‚îÄ‚îÄ consciencia_0001.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ metadata_consciencia.json
‚îî‚îÄ‚îÄ estadisticas/
    ‚îî‚îÄ‚îÄ resumen_procesamiento.json
```

## üîÑ Pr√≥ximos Pasos

1. **Afinaci√≥n de hardware_configs.py** para eliminar warnings de cola
2. **Optimizaci√≥n para datasets masivos** (>1M art√≠culos)
3. **Validaci√≥n en hardware GH200/8xH100**

---

**Estado:** ‚úÖ Listo para producci√≥n con configuraci√≥n actual  
**√öltima actualizaci√≥n:** 2025-07-16  
**Commit:** Pipeline funcional completo
