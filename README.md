# üß† WikiDump - Procesador de Wikipedia Optimizado

## üìã Estado Actual del Proyecto

**‚úÖ FUNCIONAL** - Pipeline completo implementado y validado
- ‚ö° Procesamiento optimizado para datasets masivos
- üè∑Ô∏è Categorizaci√≥n inteligente autom√°tica  
- üí¨ Generaci√≥n de conversaciones usando content_manager
- üß† Categor√≠a "consciencia" autom√°tica (NO conscious.txt)
- üìä Logging detallado cada 1-5 minutos
- üîß Configuraci√≥n adaptativa seg√∫n tama√±o del dataset

## üóÇÔ∏è Archivos Esenciales

### üì¶ Procesamiento Principal
- **`adaptive_processor.py`** - Coordinador optimizado del pipeline
- **`simple_processor.py`** - Procesador masivo paralelo
- **`content_manager.py`** - Gestor de contenido y categorizaci√≥n
- **`hardware_configs.py`** - Configuraci√≥n adaptativa de hardware

### ÔøΩÔøΩÔ∏è Pipeline Completo  
- **`caroline_ultra_extractor_hybrid.py`** - Extractor de art√≠culos (Etapa 1)
- **`main_wikidump_processor.py`** - Pipeline principal (Etapas 1+2)

### ‚öôÔ∏è Configuraci√≥n
- **`requirements.txt`** - Dependencias Python
- **`init.sh`** - Script de configuraci√≥n completa (NO commiteado)

## üöÄ Uso R√°pido

### Opci√≥n 1: Solo Etapa 2 (Generar Conversaciones)
```bash
python adaptive_processor.py --input data_ultra_hybrid --output wiki_conversations
```

### Opci√≥n 2: Pipeline Completo (XML ‚Üí Conversaciones)
```bash
python main_wikidump_processor.py --xml data_wiki/eswiki.xml --output wiki_conversations
```

### Opci√≥n 3: Publicaci√≥n en Hugging Face
```bash
# Desde init.sh (autom√°tico tras procesamiento)
./init.sh

# Independiente (para datasets ya procesados)
./publish_to_hf.sh wiki_conversations ale-neuratek/wikidump
```

## ü§ó Publicaci√≥n Autom√°tica en Hugging Face

### ‚ú® Caracter√≠sticas
- **üîê Repositorio privado**: ale-neuratek/wikidump
- **üìä Metadatos autom√°ticos**: README con estad√≠sticas detalladas
- **üóÇÔ∏è Estructura preservada**: Mantiene organizaci√≥n por categor√≠as
- **üîí Autenticaci√≥n SSH**: Configuraci√≥n segura autom√°tica
- **üìà An√°lisis de contenido**: Cuenta conversaciones, categor√≠as, tama√±o

### üõ†Ô∏è Configuraci√≥n Requerida
1. **Cuenta Hugging Face**: https://huggingface.co
2. **Token de acceso**: Con permisos de escritura
3. **Git LFS**: Instalaci√≥n autom√°tica incluida

### üì¶ Scripts Disponibles
- **`init.sh`**: Pipeline completo + publicaci√≥n autom√°tica
- **`publish_to_hf.sh`**: Publicaci√≥n independiente de datasets existentes

## üìä Rendimiento Validado

- **623,708 conversaciones** generadas en pruebas
- **48 archivos JSONL** creados correctamente  
- **8 categor√≠as principales** descubiertas autom√°ticamente
- **344MB** de datos de entrenamiento generados
- **~70 art√≠culos/segundo** de throughput

## ‚ö†Ô∏è Notas Importantes

### ‚úÖ Funcionalidades Implementadas
- [x] NO genera conscious.txt (eliminado)
- [x] Categor√≠a "consciencia" generada al final 
- [x] Todos los art√≠culos procesados via content_manager
- [x] Logging con timestamps cada 1-5 minutos
- [x] Configuraci√≥n adaptativa seg√∫n dataset
- [x] Procesamiento masivo optimizado

### üîß Configuraci√≥n Pendiente  
- ‚ö†Ô∏è **Cola llena (101 reintentos)** - Requiere afinaci√≥n de `hardware_configs.py`
- üìà Optimizaci√≥n para datasets >1M art√≠culos

## üîß Optimizaciones de Hardware

### üéØ Configuraci√≥n Adaptativa Autom√°tica
El sistema ajusta autom√°ticamente la configuraci√≥n seg√∫n:
- **Tama√±o del dataset** (art√≠culos estimados)
- **Hardware detectado** (GH200, 8xH100, Est√°ndar)
- **Riesgo de cuellos de botella** en colas

### üöÄ Optimizaciones Anti-Bloqueo para Datasets Masivos
Para datasets >1.3M art√≠culos se activan autom√°ticamente:
- ‚ö° **Queue retries**: 100 ‚Üí 300-1000 (seg√∫n tama√±o)
- ‚è±Ô∏è **Queue timeout**: 2.0s ‚Üí 0.1-0.05s (m√°s agresivo)
- üóÇÔ∏è **Queue size**: Expandido 1.5x-3x (seg√∫n dataset)
- üíæ **Auto-flush**: M√°s frecuente (cada 5-10% del dataset)
- üîÑ **Timeouts extendidos**: Para finalizaci√≥n limpia

### üìä Hardware Soportado
- **GH200**: 288 workers, 400GB buffer, 150K art√≠culos/s
- **8xH100**: 512 workers, 600GB buffer, 250K art√≠culos/s  
- **Est√°ndar**: Auto-detectado seg√∫n recursos

## üèóÔ∏è Arquitectura

```
INPUT (XML/JSONL) 
    ‚Üì
caroline_ultra_extractor_hybrid.py (Etapa 1)
    ‚Üì
data_ultra_hybrid/ (JSONL)
    ‚Üì
adaptive_processor.py (Coordinador)
    ‚Üì
simple_processor.py (Procesamiento Masivo)
    ‚Üì
content_manager.py (Categorizaci√≥n + Conversaciones)
    ‚Üì
OUTPUT (Conversaciones JSONL + Categor√≠a Consciencia)
```

## üìÅ Estructura de Salida

```
wiki_conversations/
‚îú‚îÄ‚îÄ arte/
‚îÇ   ‚îî‚îÄ‚îÄ conversaciones_arte_*.jsonl
‚îú‚îÄ‚îÄ geografia/
‚îÇ   ‚îî‚îÄ‚îÄ conversaciones_geografia_*.jsonl
‚îú‚îÄ‚îÄ historia/
‚îÇ   ‚îî‚îÄ‚îÄ conversaciones_historia_*.jsonl
‚îú‚îÄ‚îÄ consciencia/           # ‚Üê Nueva categor√≠a especial
‚îÇ   ‚îú‚îÄ‚îÄ consciencia_0001.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ metadata_consciencia.json
‚îî‚îÄ‚îÄ estadisticas/
    ‚îî‚îÄ‚îÄ resumen_procesamiento.json
```

## üîÑ Pr√≥ximos Pasos

1. **Afinaci√≥n de hardware_configs.py** para eliminar warnings de cola
2. **Optimizaci√≥n para datasets masivos** (>1M art√≠culos)
3. **Validaci√≥n en hardware GH200/8xH100**
4. **‚úÖ Publicaci√≥n autom√°tica en Hugging Face** (COMPLETADO)

---

**Estado:** ‚úÖ Listo para producci√≥n con publicaci√≥n autom√°tica  
**√öltima actualizaci√≥n:** 2025-07-16  
**Commit:** Pipeline funcional completo + Hugging Face integration
