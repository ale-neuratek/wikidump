#!/usr/bin/env python3
"""
üöÄ ADAPTIVE WIKI EXTRACTOR - Ultra-optimizado con inteligencia adaptativa
=========================================================================
Combina SAX parser confiable con optimizaciones masivas de rendimiento
y configuraci√≥n adaptativa seg√∫n el tama√±o del dataset.

CARACTER√çSTICAS:
- Configuraci√≥n adaptativa autom√°tica seg√∫n hardware detectado
- Estimaci√≥n inteligente de tama√±o de dataset
- Optimizaci√≥n din√°mica de workers y batches
- Logging detallado con timestamps
- Terminaci√≥n limpia con timeouts inteligentes
- Recuperaci√≥n autom√°tica ante errores
"""

import xml.sax
import re
import time
import os
import gc
import psutil
import hashlib
import sys
import argparse
import json
import threading
import queue
import mmap
import io
from datetime import datetime
from pathlib import Path
from typing import List, Set, Dict, Optional, Tuple, Iterator
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal

# Patrones precompilados una sola vez (optimizaci√≥n cr√≠tica)
PRECOMPILED_PATTERNS = {
    'cleanup': re.compile(r'\{\{[^}]*\}\}|<ref[^>]*>.*?</ref>|<[^>]*>', re.DOTALL),
    'links': re.compile(r'\[\[([^|\]]*\|)?([^\]]*)\]\]'),
    'whitespace': re.compile(r'\s+'),
    'spanish': re.compile(r'[a-z√°√©√≠√≥√∫√±√º√ß]', re.IGNORECASE)
}

# Importar configuraciones din√°micas por hardware
from hardware_configs import get_hardware_config, print_hardware_info, optimize_for_queue_issues, diagnose_dataset_configuration

class AdaptiveExtractorLogger:
    """Logger adaptativo con timestamps inteligentes"""
    
    def __init__(self, log_file: str = "extraction.log"):
        self.log_file = log_file
        self.start_time = time.time()
        self.log_interval = 60  # 1 minuto
        self.last_log_time = time.time()
        
        # Limpiar log anterior
        if Path(self.log_file).exists():
            Path(self.log_file).unlink()
    
    def log(self, message: str, force: bool = False):
        """Log con timestamp si ha pasado el intervalo o es forzado"""
        current_time = time.time()
        
        if force or (current_time - self.last_log_time) >= self.log_interval:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            elapsed = (current_time - self.start_time) / 3600  # horas
            
            log_entry = f"[{timestamp}] T+{elapsed:.1f}h: {message}"
            
            # Log a archivo
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry + '\n')
            
            # Log a consola
            print(log_entry)
            
            if not force:
                self.last_log_time = current_time
PRECOMPILED_PATTERNS = {
    'cleanup': re.compile(r'\{\{[^}]*\}\}|<ref[^>]*>.*?</ref>|<[^>]*>', re.DOTALL),
    'links': re.compile(r'\[\[([^|\]]*\|)?([^\]]*)\]\]'),
    'whitespace': re.compile(r'\s+'),
    'spanish': re.compile(r'[a-z√°√©√≠√≥√∫√±√º√ß]', re.IGNORECASE)
}

# Importar configuraciones din√°micas por hardware
from hardware_configs import get_hardware_config, print_hardware_info, optimize_for_queue_issues, diagnose_dataset_configuration

class AdaptiveExtractorConfig:
    """Configuraci√≥n adaptativa inteligente para el extractor"""
    
    def __init__(self, xml_path: str):
        self.xml_path = Path(xml_path)
        self.logger = AdaptiveExtractorLogger()
        
        # An√°lisis del archivo XML
        self.file_size_gb = self.xml_path.stat().st_size / (1024**3)
        
        # Configuraci√≥n base por hardware
        self.hardware_config = get_hardware_config()
        
        # Estimaci√≥n inteligente de par√°metros
        self._estimate_optimal_config()
        
        self.logger.log(f"üß† CONFIGURACI√ìN ADAPTATIVA DETERMINADA", force=True)
        self.logger.log(f"   üìÅ Archivo XML: {self.file_size_gb:.1f}GB", force=True)
        self.logger.log(f"   üë• Workers totales: {self.optimal_workers}", force=True)
        self.logger.log(f"   üì¶ Batch size: {self.optimal_batch_size:,}", force=True)
        self.logger.log(f"   üóÇÔ∏è Queue size: {self.optimal_queue_size:,}", force=True)
        
    def _estimate_optimal_config(self):
        """Estima configuraci√≥n √≥ptima seg√∫n el tama√±o del archivo"""
        base_workers = self.hardware_config['MAX_WORKERS']
        
        # Configuraci√≥n adaptativa seg√∫n tama√±o del archivo
        if self.file_size_gb > 15.0:  # Archivos muy grandes (como el nuestro de 19.8GB)
            self.optimal_workers = min(base_workers, 288)  # M√°ximo paralelismo
            self.optimal_batch_size = min(self.hardware_config['BATCH_SIZE'], 50000)  # Batches m√°s peque√±os para mejor distribuci√≥n
            self.optimal_queue_size = self.hardware_config['QUEUE_SIZE'] * 2  # Colas m√°s grandes
            self.optimal_timeout = 0.1  # Timeout m√°s agresivo
            self.optimal_flush_threshold = 100000  # Flush m√°s frecuente
            self.logger.log(f"   üéØ Configuraci√≥n para archivo MUY GRANDE detectada", force=True)
            
        elif self.file_size_gb > 5.0:  # Archivos grandes
            self.optimal_workers = min(base_workers, 200)
            self.optimal_batch_size = self.hardware_config['BATCH_SIZE'] // 2
            self.optimal_queue_size = int(self.hardware_config['QUEUE_SIZE'] * 1.5)
            self.optimal_timeout = 0.2
            self.optimal_flush_threshold = 150000
            self.logger.log(f"   üéØ Configuraci√≥n para archivo GRANDE detectada", force=True)
            
        elif self.file_size_gb > 1.0:  # Archivos medianos
            self.optimal_workers = min(base_workers, 100)
            self.optimal_batch_size = self.hardware_config['BATCH_SIZE'] // 4
            self.optimal_queue_size = self.hardware_config['QUEUE_SIZE']
            self.optimal_timeout = 0.5
            self.optimal_flush_threshold = 200000
            self.logger.log(f"   üéØ Configuraci√≥n para archivo MEDIANO detectada", force=True)
            
        else:  # Archivos peque√±os
            self.optimal_workers = min(base_workers, 50)
            self.optimal_batch_size = self.hardware_config['BATCH_SIZE'] // 8
            self.optimal_queue_size = self.hardware_config['QUEUE_SIZE'] // 2
            self.optimal_timeout = 1.0
            self.optimal_flush_threshold = 300000
            self.logger.log(f"   üéØ Configuraci√≥n para archivo PEQUE√ëO detectada", force=True)
        
        # Aplicar optimizaciones espec√≠ficas para colas si es necesario
        estimated_articles = int(self.file_size_gb * 100000)  # ~100k art√≠culos por GB
        if estimated_articles > 1000000:  # M√°s de 1M art√≠culos
            optimized_config = optimize_for_queue_issues(self.hardware_config, estimated_articles)
            
            # Ajustar par√°metros con las optimizaciones
            self.optimal_queue_size = min(optimized_config['QUEUE_SIZE'], self.optimal_queue_size * 2)
            self.optimal_timeout = min(optimized_config.get('QUEUE_TIMEOUT', 1.0), self.optimal_timeout)
            
            self.logger.log(f"   ‚ö° Optimizaciones de cola aplicadas para {estimated_articles:,} art√≠culos", force=True)

# Configuraci√≥n adaptativa seg√∫n hardware detectado
ADAPTIVE_CONFIG = None  # Se inicializar√° en main()

class AdaptiveUltraProcessor:
    """Procesador ultra-optimizado con configuraci√≥n adaptativa e inteligencia mejorada"""
    
    def __init__(self, output_dir: str = "data_ultra_hybrid", config: AdaptiveExtractorConfig = None):
        self.config = config or ADAPTIVE_CONFIG
        self.logger = self.config.logger
        
        # Configuraci√≥n adaptativa
        self.num_workers = self.config.optimal_workers
        self.batch_size = self.config.optimal_batch_size
        self.queue_size = self.config.optimal_queue_size
        self.worker_timeout = self.config.optimal_timeout
        self.flush_threshold = self.config.optimal_flush_threshold
        
        # Pools especializados con configuraci√≥n adaptativa
        extraction_workers = self.num_workers // 3
        processing_workers = self.num_workers // 3  
        output_workers = self.num_workers - extraction_workers - processing_workers  # Resto para output
        
        self.extraction_pool = ThreadPoolExecutor(max_workers=extraction_workers, thread_name_prefix="extract")
        self.processing_pool = ThreadPoolExecutor(max_workers=processing_workers, thread_name_prefix="process")
        self.output_pool = ThreadPoolExecutor(max_workers=output_workers, thread_name_prefix="output")
        
        # Colas de trabajo con tama√±o adaptativo
        self.raw_batch_queue = queue.Queue(maxsize=self.queue_size)
        self.processed_queue = queue.Queue(maxsize=self.queue_size)
        self.output_queue = queue.Queue(maxsize=self.queue_size)
        
        # Estado y estad√≠sticas mejoradas
        self.running = True
        self.stats = {
            'batches_sent': 0,
            'batches_processed': 0,
            'articles_processed': 0,
            'batches_written': 0,
            'start_time': time.time(),
            'last_stats_time': time.time()
        }
        
        # Output management mejorado
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.file_counter = 0
        
        # Buffers lockless per-worker mejorados
        self.worker_buffers = defaultdict(list)
        self.worker_file_counters = defaultdict(int)
        
        self.logger.log(f"üöÄ ADAPTIVE ULTRA-PROCESSOR INICIADO:", force=True)
        self.logger.log(f"   üîÑ Workers: Extract({extraction_workers}), Process({processing_workers}), Output({output_workers})", force=True)
        self.logger.log(f"   üì¶ Batch size: {self.batch_size:,}", force=True)
        self.logger.log(f"   üóÇÔ∏è Queue size: {self.queue_size:,}", force=True)
        self.logger.log(f"   üìÅ Output dir: {self.output_dir}", force=True)
        self.logger.log(f"   ‚è±Ô∏è Worker timeout: {self.worker_timeout}s", force=True)
    
    def log_progress_intelligent(self, force: bool = False):
        """Logging inteligente de progreso con intervalos adaptativos"""
        current_time = time.time()
        
        # Logging adaptativo: m√°s frecuente al inicio, menos despu√©s
        elapsed = current_time - self.stats['start_time']
        if elapsed < 300:  # Primeros 5 minutos: cada minuto
            interval = 60
        elif elapsed < 1800:  # Siguientes 25 minutos: cada 2 minutos
            interval = 120
        else:  # Despu√©s: cada 5 minutos
            interval = 300
        
        if force or (current_time - self.stats['last_stats_time']) >= interval:
            self._print_detailed_progress()
            self.stats['last_stats_time'] = current_time
    
    def _print_detailed_progress(self):
        """Progreso detallado con m√©tricas de rendimiento"""
        current_time = time.time()
        elapsed = current_time - self.stats['start_time']
        
        articles_rate = self.stats['articles_processed'] / elapsed if elapsed > 0 else 0
        
        # Estado de colas
        raw_size = self.raw_batch_queue.qsize()
        proc_size = self.processed_queue.qsize()
        out_size = self.output_queue.qsize()
        
        # M√©tricas de rendimiento
        total_queue_usage = (raw_size + proc_size + out_size) / (self.queue_size * 3) * 100
        
        self.logger.log(f"ÔøΩ PROGRESO ADAPTATIVO:", force=True)
        self.logger.log(f"   üìö Art√≠culos procesados: {self.stats['articles_processed']:,}", force=True)
        self.logger.log(f"   ÔøΩ Velocidad: {articles_rate:.0f} art√≠culos/s", force=True)
        self.logger.log(f"   üì¶ Batches: Enviados({self.stats['batches_sent']:,}) Procesados({self.stats['batches_processed']:,}) Escritos({self.stats['batches_written']:,})", force=True)
        self.logger.log(f"   ÔøΩÔ∏è Colas: Raw({raw_size}) Proc({proc_size}) Out({out_size}) - Uso: {total_queue_usage:.1f}%", force=True)
        self.logger.log(f"   ‚è±Ô∏è Tiempo transcurrido: {elapsed/60:.1f}min", force=True)
        
        # Alertas de rendimiento
        if total_queue_usage > 80:
            self.logger.log(f"   ‚ö†Ô∏è Colas saturadas al {total_queue_usage:.1f}%", force=True)
        if articles_rate > 0 and articles_rate < 1000:
            self.logger.log(f"   ‚ö†Ô∏è Velocidad baja: {articles_rate:.0f} art√≠culos/s", force=True)
    
    def start_workers(self):
        """Inicia todos los pools de workers especializados"""
        print(f"üöÄ Iniciando workers especializados...")
        
        # Workers de extracci√≥n (reciben batches del SAX parser)
        for i in range(self.num_workers // 3):
            self.extraction_pool.submit(self._extraction_worker, i)
        
        # Workers de procesamiento (limpian y validan art√≠culos)
        for i in range(self.num_workers // 3):
            self.processing_pool.submit(self._processing_worker, i)
        
        # Workers de salida (escriben a disco)
        for i in range(self.num_workers // 3):
            self.output_pool.submit(self._output_worker, i)
        
        print(f"‚úÖ {self.num_workers} workers especializados activos")
    
    def _extraction_worker(self, worker_id: int):
        """Worker especializado en extracci√≥n r√°pida de datos b√°sicos"""
        start_time = time.time()
        while self.running and (time.time() - start_time < 3600):  # Max 1 hora
            try:
                raw_batch = self.raw_batch_queue.get(timeout=self.worker_timeout)
                if raw_batch is None:
                    self.logger.log(f"üîÑ Extractor-{worker_id}: Se√±al de parada")
                    break
                
                # Extracci√≥n ultra-r√°pida (solo filtros b√°sicos)
                extracted = []
                for title, text in raw_batch:
                    # Filtros ultra-r√°pidos sin regex
                    if (len(text) > 200 and 
                        ':' not in title and 
                        'wikipedia:' not in title.lower() and
                        'plantilla:' not in title.lower()):
                        extracted.append((title, text))
                
                if extracted:
                    try:
                        self.processed_queue.put(extracted, timeout=0.1)
                        self.stats['batches_sent'] += 1
                    except queue.Full:
                        self.logger.log(f"‚ö†Ô∏è Extractor-{worker_id}: Cola procesamiento llena, descartando batch")
                
                self.raw_batch_queue.task_done()
                
            except queue.Empty:
                if not self.running:
                    break
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error extractor {worker_id}: {e}")
                break
        
        print(f"‚úÖ Extractor-{worker_id}: Terminado")
    
    def _processing_worker(self, worker_id: int):
        """Worker especializado en procesamiento intensivo con regex"""
        # Patrones locales para thread-safety
        patterns = PRECOMPILED_PATTERNS
        start_time = time.time()
        
        while self.running and (time.time() - start_time < 3600):  # Max 1 hora
            try:
                batch = self.processed_queue.get(timeout=self.worker_timeout)
                if batch is None:
                    self.logger.log(f"üîÑ Processor-{worker_id}: Se√±al de parada")
                    break
                
                # Procesamiento intensivo ultra-optimizado
                processed_articles = []
                
                for title, text in batch:
                    try:
                        # Limpieza ultra-r√°pida en un solo paso
                        cleaned = patterns['cleanup'].sub('', text)
                        cleaned = patterns['links'].sub(r'\2', cleaned)
                        cleaned = patterns['whitespace'].sub(' ', cleaned).strip()
                        
                        if len(cleaned) < 100:
                            continue
                        
                        # Verificaci√≥n de idioma ultra-r√°pida
                        sample = cleaned[:400]
                        spanish_count = patterns['spanish'].findall(sample)
                        if len(spanish_count) / len(sample) < 0.25:
                            continue
                        
                        # Crear art√≠culo optimizado
                        article = {
                            'title': title.strip(),
                            'content': cleaned,
                            'length': len(cleaned),
                            'worker_id': worker_id,
                            'hash': hashlib.sha256(f"{title}{cleaned[:30]}".encode()).hexdigest()[:8]
                        }
                        
                        processed_articles.append(article)
                        
                    except Exception:
                        continue  # Skip art√≠culos problem√°ticos sin logging
                
                if processed_articles:
                    try:
                        self.output_queue.put(processed_articles, timeout=0.1)
                        self.stats['articles_processed'] += len(processed_articles)
                        self.stats['batches_processed'] += 1
                    except queue.Full:
                        print(f"‚ö†Ô∏è Processor-{worker_id}: Cola output llena, descartando {len(processed_articles)} art√≠culos")
                
                self.processed_queue.task_done()
                
            except queue.Empty:
                if not self.running:
                    break
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error processor {worker_id}: {e}")
                break
        
        print(f"‚úÖ Processor-{worker_id}: Terminado")
    
    def _output_worker(self, worker_id: int):
        """Worker especializado en escritura ultra-r√°pida a disco con timeout estricto"""
        write_buffer = []
        last_flush_time = time.time()
        start_time = time.time()  # Para timeout absoluto
        MAX_WORKER_TIME = 3600  # 1 hora m√°ximo por worker
        
        while self.running and (time.time() - start_time < MAX_WORKER_TIME):
            try:
                articles = self.output_queue.get(timeout=0.5)  # Timeout m√°s corto
                if articles is None:
                    print(f"üíæ Worker-{worker_id}: Se√±al de parada recibida")
                    break
                
                write_buffer.extend(articles)
                
                # Escribir cuando el buffer est√© lleno O haya pasado tiempo
                current_time = time.time()
                should_flush = (len(write_buffer) >= 200000 or  # 200K art√≠culos por archivo (ultra-masivo)
                               (write_buffer and current_time - last_flush_time > 15))  # O cada 15 segundos (m√°s agresivo)
                
                if should_flush:
                    self._write_buffer_ultra_fast(write_buffer, worker_id)
                    write_buffer.clear()
                    last_flush_time = current_time
                
                self.output_queue.task_done()
                
            except queue.Empty:
                # Escribir buffer pendiente en timeout si hay datos
                current_time = time.time()
                if write_buffer and (current_time - last_flush_time > 10):  # Flush cada 10s en timeout
                    self._write_buffer_ultra_fast(write_buffer, worker_id)
                    write_buffer.clear()
                    last_flush_time = current_time
                
                # Verificar si debemos terminar por falta de trabajo
                if not self.running:
                    break
                    
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error output {worker_id}: {e}")
                # En caso de error, intentar escribir buffer y salir
                if write_buffer:
                    try:
                        self._write_buffer_ultra_fast(write_buffer, worker_id)
                    except:
                        pass
                break
        
        # Escribir buffer final SIEMPRE
        if write_buffer:
            print(f"üíæ Worker-{worker_id}: Escribiendo buffer final ({len(write_buffer)} art√≠culos)")
            try:
                self._write_buffer_ultra_fast(write_buffer, worker_id)
            except Exception as e:
                print(f"‚ùå Worker-{worker_id}: Error en buffer final: {e}")
        
        elapsed = time.time() - start_time
        print(f"‚úÖ Worker-{worker_id}: Terminado despu√©s de {elapsed:.1f}s")
    
    def _write_buffer_ultra_fast(self, articles: List[Dict], worker_id: int):
        """Escritura ultra-r√°pida a disco con buffers grandes LOCKLESS (mejorado con patterns del billion_parameters)"""
        try:
            # Counter at√≥mico sin lock expl√≠cito (per-worker)
            current_file_num = self.worker_file_counters[worker_id]
            self.worker_file_counters[worker_id] += 1
            
            output_file = self.output_dir / f"articles_hybrid_{worker_id}_{current_file_num:04d}.jsonl"
            
            # Buffer ultra-masivo de escritura (8MB inspirado en billion_parameters)
            with open(output_file, 'w', encoding='utf-8', buffering=8*1024*1024) as f:  # 8MB buffer
                for article in articles:
                    json.dump(article, f, ensure_ascii=False, separators=(',', ':'))
                    f.write('\n')
            
            # Stats at√≥micos sin lock
            self.stats['batches_written'] += 1
            
            print(f"üíæ Worker-{worker_id}: {len(articles):,} art√≠culos ‚Üí {output_file.name}")
            
        except Exception as e:
            print(f"‚ùå Error escritura lockless: {e}")
    
    def add_batch(self, batch: List[Tuple[str, str]]):
        """A√±ade batch al pipeline (thread-safe) con retry agresivo"""
        for attempt in range(5):  # 5 intentos
            try:
                timeout = 0.1 + (attempt * 0.05)  # Timeout incremental
                self.raw_batch_queue.put(batch, timeout=timeout)
                return True
            except queue.Full:
                if attempt == 4:  # √öltimo intento
                    print(f"‚ö†Ô∏è Cola saturada despu√©s de 5 intentos, skipping batch de {len(batch)} p√°ginas")
                    return False
                time.sleep(0.01)  # Breve pausa antes del retry
        return False
    
    def stop_workers(self):
        """Detiene workers de forma ULTRA-AGRESIVA sin esperas"""
        print(f"ÔøΩ DETENCI√ìN ULTRA-AGRESIVA DE WORKERS...")
        self.running = False
        
        # 1. Enviar se√±ales de parada masivas (sin timeout)
        print("üì§ Flooding colas con se√±ales de parada...")
        for _ in range(self.num_workers * 3):  # Triple de se√±ales
            try:
                self.raw_batch_queue.put_nowait(None)
                self.processed_queue.put_nowait(None)
                self.output_queue.put_nowait(None)
            except:
                pass  # Ignorar errores de cola llena
        
        # 2. Vaciar todas las colas agresivamente
        print("üóëÔ∏è Vaciando colas agresivamente...")
        
        def drain_queue(q, name):
            count = 0
            try:
                while not q.empty():
                    q.get_nowait()
                    count += 1
                    if count > 1000:  # L√≠mite de seguridad
                        break
            except:
                pass
            return count
        
        raw_drained = drain_queue(self.raw_batch_queue, "Raw")
        proc_drained = drain_queue(self.processed_queue, "Processed")
        out_drained = drain_queue(self.output_queue, "Output")
        
        print(f"üóëÔ∏è Vaciadas: Raw({raw_drained}), Proc({proc_drained}), Out({out_drained})")
        
        # 3. Shutdown inmediato de pools (sin esperar)
        print("üõë Shutdown inmediato de pools...")
        
        pools = [
            ("Extraction", self.extraction_pool),
            ("Processing", self.processing_pool), 
            ("Output", self.output_pool)
        ]
        
        for name, pool in pools:
            try:
                pool.shutdown(wait=False)  # NO ESPERAR
                print(f"‚úÖ {name} pool ‚Üí shutdown iniciado")
            except Exception as e:
                print(f"‚ö†Ô∏è {name} pool error: {e}")
        
        # 4. Verificaci√≥n r√°pida de threads con timeout forzado
        import threading
        start_wait = time.time()
        
        while time.time() - start_wait < ULTRA_CONFIG['FORCE_EXIT_TIMEOUT']:
            active_count = threading.active_count()
            
            if active_count <= 10:  # Solo threads del sistema
                print(f"‚úÖ Workers terminados limpiamente ({active_count} threads)")
                break
            
            time.sleep(0.5)
        
        elapsed = time.time() - start_wait
        if elapsed >= ULTRA_CONFIG['FORCE_EXIT_TIMEOUT']:
            print(f"üö® TIMEOUT ALCANZADO - Forzando terminaci√≥n despu√©s de {elapsed:.1f}s")
            
            # Matar pools agresivamente
            for name, pool in pools:
                try:
                    pool._threads.clear()  # Forzar limpieza de threads
                except:
                    pass
        
        final_active = threading.active_count()
        print(f"üßµ Threads finales: {final_active}")
        
        # 5. Forzar garbage collection
        import gc
        gc.collect()
        
        print(f"‚úÖ DETENCI√ìN COMPLETADA en {elapsed:.1f}s")
    
    def get_stats(self):
        """Obtiene estad√≠sticas actuales"""
        return self.stats.copy()

class UltraFastXMLHandler(xml.sax.ContentHandler):
    """Handler SAX ultra-optimizado con env√≠o masivo a workers y finalizaci√≥n autom√°tica"""
    
    def __init__(self, processor: AdaptiveUltraProcessor):
        super().__init__()
        self.processor = processor
        
        # Estado del parser
        self.current_element = ""
        self.current_page = {}
        self.in_page = False
        self.content_buffer = ""
        
        # Batch management ultra-eficiente
        self.page_batch = []
        self.batch_size = processor.batch_size
        
        # Contadores y estado
        self.total_pages_seen = 0
        self.total_batches_sent = 0
        self.start_time = time.time()
        self.last_page_time = time.time()  # Para detectar finalizaci√≥n
        self.xml_finished = False  # Flag de finalizaci√≥n del XML
        
        processor.logger.log(f"üìñ ULTRA-FAST XML HANDLER:", force=True)
        processor.logger.log(f"   üì¶ Batch size: {self.batch_size:,}", force=True)
        processor.logger.log(f"   üöÄ Turbo mode: ‚úÖ", force=True)
    
    def startElement(self, name, attrs):
        self.current_element = name
        if name == 'page':
            self.in_page = True
            self.current_page = {}
        elif name == 'text':
            self.content_buffer = ""
    
    def endDocument(self):
        """Se llama cuando el XML ha terminado de procesarse"""
        print(f"üèÅ XML COMPLETADO - Iniciando finalizaci√≥n autom√°tica...")
        self.xml_finished = True
        
        # Enviar √∫ltimo batch inmediatamente
        if self.page_batch:
            print(f"üì¶ Enviando √∫ltimo batch: {len(self.page_batch)} p√°ginas")
            self._send_batch_to_workers()
        
        print(f"‚úÖ XML procesado completamente")
    
    def endElement(self, name):
        if name == 'page' and self.in_page:
            self._process_page()
            self.in_page = False
            self.current_page = {}
        elif name in ['title', 'text'] and self.in_page:
            self.current_page[name] = self.content_buffer
            self.content_buffer = ""
        self.current_element = ""
    
    def characters(self, content):
        if self.current_element in ['title', 'text']:
            self.content_buffer += content
    
    def _process_page(self):
        """Procesa p√°gina de forma ultra-eficiente"""
        self.total_pages_seen += 1
        self.last_page_time = time.time()  # Actualizar tiempo de √∫ltima p√°gina
        
        title = self.current_page.get('title', '').strip()
        text = self.current_page.get('text', '').strip()
        
        # Filtro ultra-r√°pido sin regex
        if title and text and len(text) > 150:
            self.page_batch.append((title, text))
        
        # Enviar batch cuando est√© lleno
        if len(self.page_batch) >= self.batch_size:
            self._send_batch_to_workers()
        
        # Progreso cada 200K p√°ginas
        if self.total_pages_seen % 200000 == 0:
            self._print_ultra_progress()
    
    def _send_batch_to_workers(self):
        """Env√≠a batch a workers de forma persistente"""
        if not self.page_batch:
            return
        
        # Env√≠o persistente con retry
        max_attempts = 10
        for attempt in range(max_attempts):
            if self.processor.add_batch(self.page_batch.copy()):
                self.total_batches_sent += 1
                self.page_batch.clear()
                return
            
            # Si falla, esperar progresivamente m√°s tiempo
            wait_time = 0.01 * (2 ** attempt)  # Backoff exponencial
            time.sleep(min(wait_time, 1.0))  # M√°ximo 1 segundo
            
            # En el intento 5, reducir batch size
            if attempt == 5 and len(self.page_batch) > 10000:
                print(f"‚ö†Ô∏è Reduciendo batch size de {len(self.page_batch)} a 10000")
                self.page_batch = self.page_batch[:10000]
        
        # Si despu√©s de todos los intentos no se puede enviar
        print(f"‚ùå No se pudo enviar batch despu√©s de {max_attempts} intentos, descartando {len(self.page_batch)} p√°ginas")
        self.page_batch.clear()
    
    def _print_ultra_progress(self):
        """Progreso ultra-r√°pido con informaci√≥n de colas"""
        elapsed = time.time() - self.start_time
        pages_rate = self.total_pages_seen / elapsed if elapsed > 0 else 0
        
        processor_stats = self.processor.get_stats()
        
        # Informaci√≥n de colas para debugging
        raw_queue_size = self.processor.raw_batch_queue.qsize()
        processed_queue_size = self.processor.processed_queue.qsize()
        output_queue_size = self.processor.output_queue.qsize()
        
        print(f"üöÄ ULTRA-FAST PROGRESS:")
        print(f"   üìñ P√°ginas: {self.total_pages_seen:,} ({pages_rate:.0f} p/s)")
        print(f"   üì¶ Batches enviados: {self.total_batches_sent:,}")
        print(f"   üìö Art√≠culos procesados: {processor_stats['articles_processed']:,}")
        print(f"   üóÇÔ∏è Colas: Raw({raw_queue_size}), Proc({processed_queue_size}), Out({output_queue_size})")
        print(f"   ‚è±Ô∏è Tiempo: {elapsed/60:.1f}min")
        
        # Verificar si alcanzamos el objetivo
        if pages_rate >= ULTRA_CONFIG['TARGET_SPEED']:
            print(f"üéØ ‚úÖ OBJETIVO ALCANZADO: {pages_rate:.0f} >= {ULTRA_CONFIG['TARGET_SPEED']:,} p/s")
        
        # Advertir si las colas est√°n muy llenas
        max_queue_size = ULTRA_CONFIG['QUEUE_SIZE']
        if raw_queue_size > max_queue_size * 0.8:
            print(f"‚ö†Ô∏è Cola raw cerca del l√≠mite: {raw_queue_size}/{max_queue_size}")
        if output_queue_size > max_queue_size * 0.8:
            print(f"‚ö†Ô∏è Cola output cerca del l√≠mite: {output_queue_size}/{max_queue_size}")
    
    def finalize_processing(self):
        """Finaliza el procesamiento con timeout estricto"""
        print(f"üîÑ INICIANDO FINALIZACI√ìN CON TIMEOUT...")
        
        # 1. Enviar √∫ltimo batch si existe
        if self.page_batch:
            print(f"üì¶ √öltimo batch: {len(self.page_batch)} p√°ginas")
            try:
                self.processor.raw_batch_queue.put(self.page_batch.copy(), timeout=2.0)
                print(f"‚úÖ √öltimo batch enviado")
            except:
                print(f"‚ö†Ô∏è √öltimo batch descartado por timeout")
            self.page_batch.clear()
        
        # 2. Esperar con timeout muy agresivo
        max_wait = ULTRA_CONFIG['MAX_FINALIZATION_TIME']
        print(f"‚è≥ Esperando finalizaci√≥n (m√°ximo {max_wait}s)...")
        
        start_wait = time.time()
        last_articles = self.processor.get_stats()['articles_processed']
        stable_count = 0
        
        while time.time() - start_wait < max_wait:
            time.sleep(1)
            current_stats = self.processor.get_stats()
            current_articles = current_stats['articles_processed']
            
            # Estado de colas
            raw_size = self.processor.raw_batch_queue.qsize()
            proc_size = self.processor.processed_queue.qsize()
            out_size = self.processor.output_queue.qsize()
            
            elapsed_wait = time.time() - start_wait
            print(f"üìä T+{elapsed_wait:.0f}s: {current_articles:,} art√≠culos | Colas: R({raw_size}) P({proc_size}) O({out_size})")
            
            # Verificar estabilidad
            if current_articles == last_articles:
                stable_count += 1
            else:
                stable_count = 0
                last_articles = current_articles
            
            # Condiciones de finalizaci√≥n m√°s agresivas
            if raw_size == 0 and proc_size == 0 and out_size == 0 and stable_count >= 2:
                print(f"‚úÖ Procesamiento completado (colas vac√≠as)")
                break
            elif stable_count >= 3:  # Solo 3 segundos de estabilidad
                print(f"‚úÖ Procesamiento estabilizado")
                break
        
        # 3. Finalizaci√≥n forzada
        elapsed_total = time.time() - start_wait
        if elapsed_total >= max_wait:
            print(f"ÔøΩ TIMEOUT DE FINALIZACI√ìN ({elapsed_total:.1f}s) - Continuando con terminaci√≥n")
        
        # Estad√≠sticas finales
        final_stats = self.processor.get_stats()
        print(f"üìä RESUMEN FINAL:")
        print(f"   üìñ P√°ginas XML: {self.total_pages_seen:,}")
        print(f"   üì¶ Batches enviados: {self.total_batches_sent:,}")
        print(f"   üìö Art√≠culos procesados: {final_stats['articles_processed']:,}")
        print(f"   üíæ Archivos escritos: {final_stats['batches_written']:,}")
        
        # Marcar workers para detenci√≥n
        self.processor.running = False
        print(f"ÔøΩ Workers marcados para detenci√≥n")

def setup_system_for_ultra_performance():
    """Configura el sistema para m√°ximo rendimiento"""
    print("‚ö° CONFIGURANDO SISTEMA PARA ULTRA-RENDIMIENTO...")
    
    try:
        # L√≠mites del sistema
        import resource
        resource.setrlimit(resource.RLIMIT_NOFILE, (100000, 100000))
        
        # Variables de entorno optimizadas
        os.environ['PYTHONUNBUFFERED'] = '1'
        os.environ['PYTHONDONTWRITEBYTECODE'] = '1'
        os.environ['OMP_NUM_THREADS'] = str(ULTRA_CONFIG['MAX_WORKERS'])
        
        # Configurar afinidad de CPU
        process = psutil.Process()
        available_cpus = list(range(min(ULTRA_CONFIG['MAX_WORKERS'], os.cpu_count())))
        process.cpu_affinity(available_cpus)
        
        # Configurar prioridad alta
        try:
            process.nice(-10)  # Prioridad alta
        except:
            pass
        
        print(f"‚úÖ Sistema configurado:")
        print(f"   üîÑ CPU cores: {len(available_cpus)}")
        print(f"   üìÅ File descriptors: 100,000")
        print(f"   ‚ö° Prioridad: Alta")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Configuraci√≥n parcial: {e}")

def main():
    """Funci√≥n principal con procesamiento adaptativo ultra-optimizado"""
    parser = argparse.ArgumentParser(description="Extractor Adaptativo Ultra-Optimizado")
    parser.add_argument("--xml", type=str, required=True, help="Archivo XML de Wikipedia")
    parser.add_argument("--output", type=str, default="data_ultra_hybrid", help="Directorio de salida")
    
    args = parser.parse_args()
    
    # Verificar archivo
    xml_path = Path(args.xml)
    if not xml_path.exists():
        print(f"‚ùå Archivo XML no encontrado: {args.xml}")
        return 1
    
    print(f"üöÄ EXTRACTOR ADAPTATIVO ULTRA-OPTIMIZADO")
    print("=" * 80)
    
    # Crear configuraci√≥n adaptativa basada en el archivo XML
    config = AdaptiveExtractorConfig.from_xml_file(str(xml_path))
    
    print(f"üìÅ XML: {xml_path.name} ({xml_path.stat().st_size / (1024**3):.1f}GB)")
    print(f"üéØ OBJETIVO: {config.target_speed:,} p√°ginas/segundo")
    print(f"‚ö° CONFIGURACI√ìN ADAPTATIVA:")
    print(f"   üë• Workers: {config.optimal_workers}")
    print(f"   üì¶ Batch size: {config.optimal_batch_size:,}")
    print(f"   üóÇÔ∏è Queue size: {config.optimal_queue_size:,}")
    print(f"   ‚è±Ô∏è Timeout: {config.optimal_timeout}s")
    print("=" * 80)
    
    try:
        # Crear procesador adaptativo ultra-optimizado
        processor = AdaptiveUltraProcessor(args.output, config)
        
        # Iniciar workers especializados
        processor.start_workers()
        
        # Crear handler SAX ultra-r√°pido
        handler = UltraFastXMLHandler(processor)
        
        # Manejo de se√±ales con terminaci√≥n forzada
        def signal_handler(signum, frame):
            config.logger.log(f"‚ö†Ô∏è Se√±al {signum} recibida, terminando INMEDIATAMENTE...", force=True)
            processor.running = False
            processor.stop_workers()
            
            # Esperar m√°ximo 5 segundos
            cleanup_start = time.time()
            while time.time() - cleanup_start < 5:
                import threading
                if threading.active_count() <= 5:
                    break
                time.sleep(0.5)
            
            config.logger.log(f"üö® TERMINACI√ìN FORZADA POR SE√ëAL", force=True)
            os._exit(1)  # Salida inmediata
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # Procesar XML con detecci√≥n autom√°tica de finalizaci√≥n
        start_time = time.time()
        config.logger.log(f"üöÄ Iniciando procesamiento adaptativo ultra-optimizado...", force=True)
        
        try:
            # SAX parsing (confiable) + workers ultra-optimizados
            xml.sax.parse(str(xml_path), handler)
            config.logger.log(f"‚úÖ SAX Parser completado exitosamente", force=True)
        except Exception as e:
            config.logger.log(f"‚ö†Ô∏è SAX Parser termin√≥ con excepci√≥n: {e}", force=True)
        
        # Finalizar con detecci√≥n inteligente
        config.logger.log(f"üèÅ XML TERMINADO - Iniciando finalizaci√≥n inteligente...", force=True)
        handler.finalize_processing()
        
        config.logger.log(f"üõë DETENIENDO WORKERS...", force=True)
        processor.stop_workers()
        
        # Breve pausa para permitir que los workers terminen limpiamente
        config.logger.log(f"‚è≥ Pausa de limpieza (2 segundos)...", force=True)
        time.sleep(2)
        
        # Estad√≠sticas finales
        elapsed = time.time() - start_time
        final_stats = processor.get_stats()
        
        pages_rate = handler.total_pages_seen / elapsed if elapsed > 0 else 0
        articles_rate = final_stats['articles_processed'] / elapsed if elapsed > 0 else 0
        
        config.logger.log(f"üéâ PROCESAMIENTO ADAPTATIVO COMPLETADO:", force=True)
        config.logger.log(f"   üìñ P√°ginas procesadas: {handler.total_pages_seen:,}", force=True)
        config.logger.log(f"   üìö Art√≠culos v√°lidos: {final_stats['articles_processed']:,}", force=True)
        config.logger.log(f"   üì¶ Batches escritos: {final_stats['batches_written']:,}", force=True)
        config.logger.log(f"   ‚è±Ô∏è Tiempo total: {elapsed/3600:.2f}h", force=True)
        config.logger.log(f"   üöÄ Velocidad p√°ginas: {pages_rate:.0f} p/s", force=True)
        config.logger.log(f"   üöÄ Velocidad art√≠culos: {articles_rate:.0f} a/s", force=True)
        
        # Evaluar resultado
        if pages_rate >= config.target_speed:
            config.logger.log(f"üéØ ‚úÖ OBJETIVO ALCANZADO: {pages_rate:.0f} >= {config.target_speed:,} p/s", force=True)
        else:
            improvement = pages_rate / 600  # vs versi√≥n anterior
            config.logger.log(f"üìà MEJORA CONSEGUIDA: {improvement:.1f}x m√°s r√°pido", force=True)
            config.logger.log(f"üìä Progreso hacia objetivo: {(pages_rate / config.target_speed) * 100:.1f}%", force=True)
        
        # Verificaci√≥n final de finalizaci√≥n limpia
        import threading
        active_threads = threading.active_count()
        if active_threads <= 5:  # Main + threads del sistema
            config.logger.log(f"‚úÖ FINALIZACI√ìN LIMPIA - {active_threads} threads activos", force=True)
        else:
            config.logger.log(f"‚ö†Ô∏è FINALIZACI√ìN PARCIAL - {active_threads} threads a√∫n activos", force=True)
            config.logger.log(f"üö® Esperando 10s adicionales para limpieza...", force=True)
            
            # Esperar limpieza final con timeout
            cleanup_start = time.time()
            while time.time() - cleanup_start < 10:
                current_threads = threading.active_count()
                if current_threads <= 5:
                    config.logger.log(f"‚úÖ LIMPIEZA COMPLETADA - {current_threads} threads", force=True)
                    break
                time.sleep(1)
            
            # Si a√∫n hay threads activos, forzar salida
            final_threads = threading.active_count()
            if final_threads > 5:
                config.logger.log(f"üö® FORZANDO SALIDA - {final_threads} threads persistentes", force=True)
                
                # Garbage collection final agresivo
                import gc
                gc.collect()
                
                config.logger.log(f"üöÄ PROCESO TERMINANDO FORZADAMENTE...", force=True)
                os._exit(0)  # Salida forzada sin cleanup adicional
        
        # Forzar garbage collection final
        import gc
        gc.collect()
        
        config.logger.log(f"üéØ PROCESO PRINCIPAL TERMINANDO LIMPIAMENTE...", force=True)
        return 0
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Interrumpido por usuario")
        processor.stop_workers()
        return 130
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
