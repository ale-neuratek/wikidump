#!/usr/bin/env python3
"""
üöÄ MAIN WIKIDUMP PROCESSOR - Pipeline Completo de Procesamiento
===============================================================
Script principal que ejecuta el pipeline completo de procesamiento de Wikipedia
para m√°xima eficiencia en hardware de alto rendimiento (GH200, 8xH100)

PIPELINE:
1. ETAPA 1: Caroline Ultra Extractor (XML ‚Üí JSONL intermedios)
2. ETAPA 2: Adaptive Processor (JSONL ‚Üí Conversaciones + Consciencia)

OPTIMIZACIONES:
- Configuraci√≥n adaptativa por hardware detectado
- Monitoreo en tiempo real del rendimiento
- Recuperaci√≥n autom√°tica ante fallos
- Validaci√≥n de calidad de datos
- Identificaci√≥n temporal integrada (es/fue)
"""

import os
import sys
import time
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
import psutil
import signal

# Importar configuraciones de hardware
from hardware_configs import get_hardware_config, print_hardware_info, detect_hardware

class WikidumpMainProcessor:
    """Procesador principal del pipeline completo de Wikidump"""
    
    def __init__(self, xml_path: str = None, jsonl_dir: str = None, output_dir: str = "wiki_conversations_complete", 
                 skip_stage1: bool = False, questions_per_category: int = 10):
        # Validar inputs exclusivos
        if xml_path and jsonl_dir:
            raise ValueError("No se puede especificar tanto xml_path como jsonl_dir")
        if not xml_path and not jsonl_dir:
            # Default a data_test_ultra_hybrid si no se especifica nada
            jsonl_dir = "data_test_ultra_hybrid"
            skip_stage1 = True
        
        self.xml_path = Path(xml_path) if xml_path else None
        self.jsonl_dir = Path(jsonl_dir) if jsonl_dir else None
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Configuraci√≥n de preguntas por categor√≠a
        self.questions_per_category = questions_per_category
        self.base_questions_count = min(5, questions_per_category)  # 3-5 preguntas base
        self.specific_questions_count = questions_per_category - self.base_questions_count
        
        # Rutas para cada etapa
        self.stage1_output = self.jsonl_dir if self.jsonl_dir else Path("data_ultra_hybrid")
        self.stage2_output = self.output_dir
        
        # Estado del pipeline
        self.skip_stage1 = skip_stage1 or (self.jsonl_dir is not None)
        self.hardware_type = detect_hardware()
        self.hardware_config = get_hardware_config()
        
        # Logging
        self.log_file = "main_processing.log"
        self.start_time = time.time()
        
        # Configurar manejo de se√±ales
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.running = True
        self.current_stage = 0
        
    def _signal_handler(self, signum, frame):
        """Manejo elegante de se√±ales de terminaci√≥n"""
        print(f"\n‚ö†Ô∏è Se√±al {signum} recibida durante la etapa {self.current_stage}")
        print("üõë Terminando procesamiento de manera elegante...")
        self.running = False
        
    def print_pipeline_info(self):
        """Imprime informaci√≥n del pipeline y hardware"""
        print("="*80)
        print("üöÄ WIKIDUMP MAIN PROCESSOR - Pipeline Completo")
        print("="*80)
        
        print_hardware_info()
        
        print(f"\nüìÅ CONFIGURACI√ìN DEL PIPELINE:")
        if self.xml_path:
            print(f"   üìÑ XML Input: {self.xml_path} ({self.xml_path.stat().st_size / (1024**3):.1f}GB)")
        if self.jsonl_dir:
            print(f"   üìÇ JSONL Input Directory: {self.jsonl_dir}")
        print(f"   üìÇ Stage 1 Output: {self.stage1_output}")
        print(f"   üìÇ Stage 2 Output: {self.stage2_output}")
        print(f"   ‚è≠Ô∏è  Skip Stage 1: {'S√≠' if self.skip_stage1 else 'No'}")
        
        print(f"\n‚ö° CONFIGURACI√ìN DE RENDIMIENTO:")
        print(f"   üîÑ Max Workers: {self.hardware_config['MAX_WORKERS']}")
        print(f"   üì¶ Batch Size: {self.hardware_config['BATCH_SIZE']:,}")
        print(f"   üéØ Target Speed: {self.hardware_config['TARGET_SPEED']:,} p√°ginas/segundo")
        print(f"   üíæ Memory Buffer: {self.hardware_config['MEMORY_BUFFER_GB']}GB")
        
        print(f"\n‚ùì CONFIGURACI√ìN DE PREGUNTAS:")
        print(f"   üìä Preguntas por categor√≠a: {self.questions_per_category}")
        print(f"   üìã Preguntas base: {self.base_questions_count}")
        print(f"   üéØ Preguntas espec√≠ficas: {self.specific_questions_count}")
        
        print("="*80)
        
    def stage1_xml_to_jsonl(self) -> bool:
        """Ejecuta Stage 1: XML ‚Üí JSONL usando Caroline Ultra Extractor"""
        if self.skip_stage1:
            print("‚è≠Ô∏è STAGE 1 OMITIDA - Verificando archivos JSONL existentes...")
            return self._validate_stage1_output()
            
        print("\nüöÄ INICIANDO STAGE 1: XML ‚Üí JSONL (Caroline Ultra Extractor)")
        print("="*60)
        
        self.current_stage = 1
        start_time = time.time()
        
        try:
            # Ejecutar caroline_ultra_extractor_hybrid.py
            cmd = [
                sys.executable, 
                "caroline_ultra_extractor_hybrid.py",
                "--xml", str(self.xml_path),
                "--output", str(self.stage1_output)
            ]
            
            print(f"üìã Ejecutando: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                cwd=Path.cwd(),
                capture_output=False,
                text=True,
                check=True
            )
            
            elapsed = time.time() - start_time
            print(f"\n‚úÖ STAGE 1 COMPLETADA en {elapsed:.1f}s")
            
            return self._validate_stage1_output()
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå ERROR en Stage 1: {e}")
            return False
        except KeyboardInterrupt:
            print(f"\n‚ö†Ô∏è Stage 1 interrumpida por el usuario")
            return False
    
    def stage2_jsonl_to_datasets(self) -> bool:
        """Ejecuta Stage 2: JSONL ‚Üí Conversaciones usando Adaptive Processor"""
        print("\nüß† INICIANDO STAGE 2: JSONL ‚Üí Conversaciones (Adaptive Processor)")
        print("="*60)
        
        self.current_stage = 2
        start_time = time.time()
        
        try:
            # Ejecutar adaptive_processor.py con par√°metro de preguntas
            cmd = [
                sys.executable,
                "adaptive_processor.py",
                str(self.stage1_output),
                str(self.stage2_output),
                "--questions-per-article", str(self.questions_per_category)
            ]
            
            print(f"üìã Ejecutando: {' '.join(cmd)}")
            print(f"üìä Preguntas por art√≠culo: {self.questions_per_category} (Base: {self.base_questions_count}, Espec√≠ficas: {self.specific_questions_count})")
            
            # Crear proceso para capturar salida en tiempo real
            process = subprocess.Popen(
                cmd,
                cwd=Path.cwd(),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                encoding='utf-8',
                errors='replace',  # Reemplazar caracteres inv√°lidos
                bufsize=1
            )
            
            # Mostrar salida en tiempo real
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    print(output.strip())
            
            # Esperar que termine y obtener c√≥digo de salida
            return_code = process.poll()
            
            elapsed = time.time() - start_time
            
            if return_code == 0:
                print(f"\n‚úÖ STAGE 2 COMPLETADA en {elapsed:.1f}s")
                return self._validate_stage2_output()
            else:
                print(f"\n‚ùå STAGE 2 FALL√ì con c√≥digo {return_code}")
                return False
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå ERROR en Stage 2: {e}")
            return False
        except KeyboardInterrupt:
            print(f"\n‚ö†Ô∏è Stage 2 interrumpida por el usuario")
            return False
    
    def _validate_stage1_output(self) -> bool:
        """Valida la salida de Stage 1"""
        if not self.stage1_output.exists():
            print(f"‚ùå Directorio de Stage 1 no existe: {self.stage1_output}")
            return False
            
        jsonl_files = list(self.stage1_output.glob("articles_hybrid_*.jsonl"))
        if not jsonl_files:
            print(f"‚ùå No se encontraron archivos JSONL en {self.stage1_output}")
            return False
            
        total_size = sum(f.stat().st_size for f in jsonl_files)
        total_articles = 0
        
        try:
            # Contar art√≠culos r√°pidamente
            import subprocess
            result = subprocess.run(
                f"wc -l {self.stage1_output}/*.jsonl | tail -1",
                shell=True, capture_output=True, text=True
            )
            if result.returncode == 0:
                total_articles = int(result.stdout.strip().split()[0])
        except:
            pass
            
        print(f"‚úÖ Stage 1 Output Validado:")
        print(f"   üìÅ Archivos JSONL: {len(jsonl_files)}")
        print(f"   üìä Tama√±o total: {total_size / (1024**3):.1f}GB")
        print(f"   üìÑ Art√≠culos estimados: {total_articles:,}")
        
        return True
    
    def _validate_stage2_output(self) -> bool:
        """Valida la salida de Stage 2"""
        if not self.stage2_output.exists():
            print(f"‚ùå Directorio de Stage 2 no existe: {self.stage2_output}")
            return False
            
        # Buscar archivos de conversaciones
        conversation_files = list(self.stage2_output.glob("**/*.jsonl"))
        categories_dir = self.stage2_output / "categorias"
        consciencia_dir = self.stage2_output / "consciencia"
        
        print(f"‚úÖ Stage 2 Output Validado:")
        print(f"   üìÅ Archivos de conversaciones: {len(conversation_files)}")
        print(f"   üìÇ Directorio base: {self.stage2_output}")
        print(f"   üè∑Ô∏è Categor√≠as: {'‚úÖ' if categories_dir.exists() else '‚ùå'}")
        print(f"   üß† Consciencia: {'‚úÖ' if consciencia_dir.exists() else '‚ùå'}")
        
        # Contar conversaciones totales
        total_conversations = 0
        for file in conversation_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    total_conversations += sum(1 for line in f if line.strip())
            except:
                pass
        
        print(f"   ÔøΩ Total conversaciones: {total_conversations:,}")
        
        return len(conversation_files) > 0
    
    def print_final_summary(self, total_time: float, stage1_success: bool, stage2_success: bool):
        """Imprime resumen final del procesamiento"""
        print("\n" + "="*80)
        print("üéâ PROCESAMIENTO COMPLETADO - Resumen Final")
        print("="*80)
        
        print(f"‚è±Ô∏è  TIEMPO TOTAL: {total_time:.1f}s ({total_time/60:.1f} minutos)")
        print(f"üñ•Ô∏è  HARDWARE: {self.hardware_type}")
        print(f"üìä ESTADO DE LAS ETAPAS:")
        print(f"   Stage 1 (XML‚ÜíJSONL): {'‚úÖ' if stage1_success else '‚ùå'}")
        print(f"   Stage 2 (JSONL‚ÜíConversaciones): {'‚úÖ' if stage2_success else '‚ùå'}")
        
        if stage1_success and stage2_success:
            print(f"\nüéØ PIPELINE COMPLETADO CON √âXITO")
            print(f"üìÇ Conversaciones disponibles en: {self.stage2_output}")
            print(f"üß† Incluye identificaci√≥n temporal (es/fue)")
            print(f"üè∑Ô∏è Con categorizaci√≥n autom√°tica")
            print(f"üé≠ Generaci√≥n de consciencia incluida")
        else:
            print(f"\n‚ö†Ô∏è PIPELINE COMPLETADO CON ERRORES")
            
        print("="*80)
    
    def run(self) -> bool:
        """Ejecuta el pipeline completo"""
        if not self.running:
            return False
            
        self.print_pipeline_info()
        
        start_time = time.time()
        stage1_success = False
        stage2_success = False
        
        try:
            # Stage 1: XML ‚Üí JSONL
            if self.running:
                stage1_success = self.stage1_xml_to_jsonl()
            
            # Stage 2: JSONL ‚Üí Datasets (solo si Stage 1 fue exitoso)
            if self.running and stage1_success:
                stage2_success = self.stage2_jsonl_to_datasets()
            
            total_time = time.time() - start_time
            self.print_final_summary(total_time, stage1_success, stage2_success)
            
            return stage1_success and stage2_success
            
        except Exception as e:
            print(f"\n‚ùå ERROR CR√çTICO EN EL PIPELINE: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    parser = argparse.ArgumentParser(
        description="üöÄ Wikidump Main Processor - Pipeline Completo de Procesamiento",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
EJEMPLOS:
  # Procesar XML completo (ambas etapas) - RECOMENDADO
  python3 main_wikidump_processor.py --xml data_wiki/eswiki-20250601-pages-articles-multistream.xml
  
  # Solo ejecutar Stage 2 usando directorio JSONL preprocesado
  python3 main_wikidump_processor.py --jsonl data_ultra_hybrid
  
  # Usar directorio por defecto (data_test_ultra_hybrid)
  python3 main_wikidump_processor.py
  
  # Especificar directorio de salida personalizado
  python3 main_wikidump_processor.py --jsonl data_ultra_hybrid --output wiki_conversaciones_custom
        """
    )
    
    # Grupo mutuamente exclusivo para XML o JSONL
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument(
        "--xml", 
        help="Ruta al archivo XML de Wikipedia"
    )
    
    input_group.add_argument(
        "--jsonl",
        help="Directorio con archivos JSONL preprocesados (Stage 1 ya ejecutado)"
    )
    
    parser.add_argument(
        "--output", 
        default="wiki_conversations_complete",
        help="Directorio de salida para las conversaciones finales (default: wiki_conversations_complete)"
    )
    
    parser.add_argument(
        "--skip-stage1", 
        action="store_true",
        help="Omitir Stage 1 y usar archivos JSONL existentes (deprecated: use --jsonl instead)"
    )
    
    parser.add_argument(
        "--questions-per-category",
        type=int,
        default=10,
        help="N√∫mero de preguntas por categor√≠a (default: 10, incluye 3-5 base + espec√≠ficas)"
    )
    
    args = parser.parse_args()
    
    # Manejar l√≥gica de argumentos
    if args.skip_stage1 and not args.xml and not args.jsonl:
        # Comportamiento legacy: usar data_ultra_hybrid
        args.jsonl = "data_ultra_hybrid"
    
    # Crear y ejecutar el procesador principal
    processor = WikidumpMainProcessor(
        xml_path=args.xml,
        jsonl_dir=args.jsonl,
        output_dir=args.output,
        skip_stage1=args.skip_stage1,
        questions_per_category=args.questions_per_category
    )
    
    success = processor.run()
    
    # Salir con c√≥digo apropiado
    exit_code = 0 if success else 1
    print(f"\nüèÅ Saliendo con c√≥digo: {exit_code}")
    sys.exit(exit_code)

if __name__ == "__main__":
    main()
