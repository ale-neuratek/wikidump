#!/usr/bin/env python3
"""
üß† FULL DATASET FOR TRAINING - Procesador Completo de Dataset de Entrenamiento
================================================================================
Basado en caroline_dataset_optimizer.py para crear datasets de entrenamiento perfectos

OBJETIVOS:
- Procesar 100% de los 1.5M art√≠culos h√≠bridos del servidor
- Corregir categorizaci√≥n defectuosa (ej: "Catch a Fire" debe ir a arte/m√∫sica)
- Crear carpeta "consciencia" con datasets organizados por categor√≠a
- Generar conversaciones de entrenamiento con subcategor√≠as espec√≠ficas
- Sistema de categorizaci√≥n inteligente basado en contenido real

ESTRUCTURA DE SALIDA:
consciencia/
‚îú‚îÄ‚îÄ conscious.txt (base fundamental del conocimiento)
‚îú‚îÄ‚îÄ categorias/
‚îÇ   ‚îú‚îÄ‚îÄ arte/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversaciones_arte_0001.jsonl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversaciones_arte_0002.jsonl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata_arte.json
‚îÇ   ‚îú‚îÄ‚îÄ ciencias/
‚îÇ   ‚îú‚îÄ‚îÄ geografia/
‚îÇ   ‚îî‚îÄ‚îÄ ... (hasta 32 categor√≠as principales)
‚îî‚îÄ‚îÄ estadisticas/
    ‚îú‚îÄ‚îÄ distribucion_categorias.json
    ‚îú‚îÄ‚îÄ problemas_categorizacion.json
    ‚îî‚îÄ‚îÄ resumen_procesamiento.json
"""

import json
import re
import os
import sys
import time
import hashlib
import threading
import queue
import gc
import signal
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Set, Iterator
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback

# Importar el gestor de contenido externo
from content_manager import ContentManager

# Configuraci√≥n ULTRA-OPTIMIZADA para servidor 64-core, 450GB RAM
MASSIVE_PARALLEL_CONFIG = {
    # CONFIGURACI√ìN BASE - optimizada para servidor 450GB RAM
    'BASE_MAX_WORKERS': min(120, os.cpu_count() * 2),  # Incrementado para aprovechar mejor los recursos
    'BASE_CATEGORY_WORKERS': 8,   # Incrementado para mayor paralelismo
    'BASE_CONVERSATION_WORKERS': 24,  # Incrementado para mayor throughput
    'BASE_OUTPUT_WORKERS': 16,  # Incrementado para mejor escritura
    
    # BATCHING OPTIMIZADO - configuraci√≥n solicitada
    'BASE_BATCH_SIZE': 100000,  # Batch size de 100,000 como solicitado
    'BASE_QUEUE_SIZE': 1000,    # Queue size de 1,000 como solicitado
    'BASE_MEMORY_BUFFER_GB': 200,  # Buffer MASIVO aprovechando 450GB RAM (~44% uso)
    
    # PROCESAMIENTO 100% ILIMITADO (optimizado)
    'PROCESS_ALL_ARTICLES': True,  # SIN L√çMITES - Procesar TODO
    'CONVERSATIONS_PER_ARTICLE': 4,  # 4 conversaciones por art√≠culo
    'UNLIMITED_PROCESSING': True,  # Modo completamente ilimitado
    'STREAMING_MODE': True,  # Modo streaming para datasets masivos
    
    # ARCHIVOS DE SALIDA OPTIMIZADOS para 450GB RAM
    'BASE_ARTICLES_PER_OUTPUT_FILE': 100000,  # 100K art√≠culos por archivo (incrementado)
    'BASE_CONVERSATIONS_PER_OUTPUT_FILE': 400000,  # 400K conversaciones por archivo (incrementado)
    'MAX_FILE_SIZE_MB': 2048,  # M√°ximo 2GB por archivo (incrementado)
    
    # CLASES DE DATASETS para configuraci√≥n adaptativa (ultra-optimizada para 450GB RAM)
    'DATASET_SIZE_CLASSES': {
        'SMALL': {'max_gb': 10, 'multiplier': 1.0, 'name': 'Peque√±o'},
        'MEDIUM': {'max_gb': 50, 'multiplier': 1.5, 'name': 'Mediano'},   # Incrementado multiplicador
        'LARGE': {'max_gb': 200, 'multiplier': 2.0, 'name': 'Grande'},   # Incrementado l√≠mite y multiplicador
        'EXTREME': {'max_gb': float('inf'), 'multiplier': 2.5, 'name': 'Extremo'}  # Incrementado multiplicador para usar 450GB
    },
    
    # OPTIMIZACIONES MASIVAS PARA FLUJO CONTINUO con 450GB RAM
    'PARALLEL_CATEGORY_PROCESSING': True,
    'BASE_AUTO_FLUSH_THRESHOLD': 10000,  # Flush threshold a√∫n m√°s peque√±o para flujo continuo (reducido de 25,000)
    'ADAPTIVE_CONFIGURATION': True,  # Configuraci√≥n adaptativa
    'TURBO_MODE': True,
    'NO_LOCKS_MODE': True,  # Modo sin locks para rendimiento
    'ULTRA_AGGRESSIVE_MODE': False,  # Mantener desactivado para estabilidad
    'STREAMING_WORKERS': True,  # Workers en modo streaming
    
    # CARACTER√çSTICAS ESPEC√çFICAS PARA WIKIPEDIA
    'WIKIPEDIA_MODE': True,
    'ENABLE_PROGRESS_ESTIMATION': True,
    'AGGRESSIVE_MEMORY_MANAGEMENT': True,
    'AUTO_DETECT_DATASET_SIZE': True,
}

class MassiveParallelDatasetProcessor:
    """Procesador masivo paralelo sin locks para m√°ximo rendimiento del servidor"""
    
    def __init__(self, input_dir: str, output_dir: str = "consciencia"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Detectar tama√±o del dataset autom√°ticamente
        self.dataset_info = self._analyze_dataset_size()
        
        # Configuraci√≥n adaptativa basada en el tama√±o del dataset
        self._configure_adaptive_settings()
        
        # Inicializar gestor de contenido externo PRIMERO
        self.content_manager = ContentManager()
        
        # Crear estructura de carpetas
        self.categories_dir = self.output_dir / "categorias"
        self.stats_dir = self.output_dir / "estadisticas"
        self.categories_dir.mkdir(exist_ok=True)
        self.stats_dir.mkdir(exist_ok=True)
        
        # Crear carpetas por categor√≠a
        self.category_dirs = {}
        for category in self.get_categories():
            category_dir = self.categories_dir / category
            category_dir.mkdir(exist_ok=True)
            self.category_dirs[category] = category_dir
        
        # Workers adaptativos para datasets masivos
        self.category_workers = self.adaptive_config['CATEGORY_WORKERS']
        self.conversation_workers = self.adaptive_config['CONVERSATION_WORKERS']
        self.output_workers = self.adaptive_config['OUTPUT_WORKERS']
        
        # Thread pools especializados con capacidad adaptativa
        self.category_pool = ThreadPoolExecutor(max_workers=self.category_workers, thread_name_prefix="category")
        self.conversation_pool = ThreadPoolExecutor(max_workers=self.conversation_workers, thread_name_prefix="conversation")
        self.output_pool = ThreadPoolExecutor(max_workers=self.output_workers, thread_name_prefix="output")
        
        # Colas especializadas para volumen masivo adaptativo (SIN LOCKS)
        queue_size = self.adaptive_config['QUEUE_SIZE']
        self.article_queue = queue.Queue(maxsize=queue_size)
        self.category_queues = {category: queue.Queue(maxsize=queue_size) 
                               for category in self.get_categories()}
        
        # Buffers por categor√≠a (thread-safe usando defaultdict) - SIN LOCKS EXPL√çCITOS
        self.category_buffers = {category: defaultdict(list) for category in self.get_categories()}
        self.category_counters = {category: 0 for category in self.get_categories()}
        self.file_counters = {category: 0 for category in self.get_categories()}
        
        # Estado y estad√≠sticas para datasets masivos (thread-safe at√≥mico)
        self.running = True
        self.stats = {
            'articles_processed': 0,
            'articles_categorized': 0,
            'conversations_generated': 0,
            'files_written': 0,
            'processing_errors': 0,
            'start_time': time.time(),
            'estimated_total_articles': self.dataset_info['estimated_articles'],
            'dataset_size_gb': self.dataset_info['total_size_gb']
        }
        
        # Lista de categorizaciones problem√°ticas (sin lock para mejor rendimiento)
        self.problematic_categorizations = []
        
        # Descubrir archivos h√≠bridos
        self.hybrid_files = self._discover_hybrid_files()
        
        print(f"üß† MASSIVE PARALLEL DATASET PROCESSOR INICIADO - SIN LOCKS")
        print(f"   üìÇ Directorio entrada: {self.input_dir}")
        print(f"   üìÅ Archivos h√≠bridos encontrados: {len(self.hybrid_files)}")
        print(f"   üìä Dataset size: {self.dataset_info['total_size_gb']:.1f}GB")
        print(f"   üìö Art√≠culos estimados: {self.dataset_info['estimated_articles']:,}")
        print(f"   üìÇ Categor√≠as: {len(self.get_categories())}")
        print(f"   üéØ Procesamiento: ILIMITADO (100% del dataset)")
        print(f"   üî¢ Conversaciones/art√≠culo: {MASSIVE_PARALLEL_CONFIG['CONVERSATIONS_PER_ARTICLE']}")
        print(f"   üîÑ Workers masivos: Cat({self.category_workers}), Conv({self.conversation_workers}), Out({self.output_workers})")
        print(f"   üíæ Buffer memoria: {self.adaptive_config['MEMORY_BUFFER_GB']}GB")
        print(f"   üöÄ Modo: SIN LOCKS para m√°ximo rendimiento")
        print(f"   üìÅ Directorio salida: {self.output_dir}")
        
        # Mostrar estimaciones para el dataset completo
        estimated_conversations = self.dataset_info['estimated_articles'] * MASSIVE_PARALLEL_CONFIG['CONVERSATIONS_PER_ARTICLE']
        estimated_size_gb = estimated_conversations * 0.5 / 1024 / 1024  # ~0.5KB por conversaci√≥n
        print(f"\nüéØ ESTIMACIONES PARA DATASET COMPLETO:")
        print(f"   üí¨ Conversaciones estimadas: {estimated_conversations:,}")
        print(f"   üìÅ Archivos de salida estimados: {estimated_conversations // self.adaptive_config['CONVERSATIONS_PER_OUTPUT_FILE']:,}")
        print(f"   üíæ Tama√±o salida estimado: {estimated_size_gb:.1f}GB")
        
    def get_categories(self) -> List[str]:
        """Obtiene lista de categor√≠as principales del gestor de contenido"""
        return self.content_manager.get_categories()
        
    def _analyze_dataset_size(self) -> Dict:
        """Analiza el tama√±o del dataset para configuraci√≥n adaptativa"""
        total_size_bytes = 0
        total_files = 0
        estimated_articles = 0
        
        # Buscar archivos h√≠bridos para an√°lisis
        search_dirs = [self.input_dir, Path(".")]
        
        for search_dir in search_dirs:
            if search_dir.exists():
                files = list(search_dir.glob("articles_hybrid_*.jsonl"))
                for file_path in files:
                    try:
                        size = file_path.stat().st_size
                        total_size_bytes += size
                        total_files += 1
                        # Estimar art√≠culos por archivo (promedio ~2KB por art√≠culo en JSON)
                        file_articles = size // 2048
                        estimated_articles += file_articles
                    except Exception:
                        continue
        
        total_size_gb = total_size_bytes / (1024**3)
        
        # Clasificar el dataset seg√∫n los rangos
        dataset_class = "SMALL"
        for class_name, class_info in MASSIVE_PARALLEL_CONFIG['DATASET_SIZE_CLASSES'].items():
            if total_size_gb <= class_info['max_gb']:
                dataset_class = class_name
                break
        
        return {
            'total_size_bytes': total_size_bytes,
            'total_size_gb': total_size_gb,
            'total_files': total_files,
            'estimated_articles': estimated_articles,
            'dataset_class': dataset_class
        }
        
    def _configure_adaptive_settings(self):
        """Configura settings adaptativos basados en h√≠brido extractor (divisi√≥n por 3)"""
        dataset_class = self.dataset_info['dataset_class']
        multiplier = MASSIVE_PARALLEL_CONFIG['DATASET_SIZE_CLASSES'][dataset_class]['multiplier']
        cpu_count = os.cpu_count()
        
        # Calcular total de workers como h√≠brido extractor (con l√≠mite por CPU)
        total_workers = min(
            int(MASSIVE_PARALLEL_CONFIG['BASE_MAX_WORKERS'] * multiplier),
            cpu_count * 2  # L√≠mite m√°s estricto: 2x CPU cores m√°ximo
        )
        
        # Divisi√≥n por 3 especializada (patr√≥n h√≠brido extractor)
        self.adaptive_config = {
            'CATEGORY_WORKERS': total_workers // 3,      # Categorizaci√≥n
            'CONVERSATION_WORKERS': total_workers // 3,  # Procesamiento
            'OUTPUT_WORKERS': total_workers // 3,        # Escritura
            'BATCH_SIZE': int(MASSIVE_PARALLEL_CONFIG['BASE_BATCH_SIZE'] * multiplier),
            'QUEUE_SIZE': int(MASSIVE_PARALLEL_CONFIG['BASE_QUEUE_SIZE'] * multiplier),
            'MEMORY_BUFFER_GB': int(MASSIVE_PARALLEL_CONFIG['BASE_MEMORY_BUFFER_GB'] * multiplier),
            'ARTICLES_PER_OUTPUT_FILE': int(MASSIVE_PARALLEL_CONFIG['BASE_ARTICLES_PER_OUTPUT_FILE'] * multiplier),
            'CONVERSATIONS_PER_OUTPUT_FILE': int(MASSIVE_PARALLEL_CONFIG['BASE_CONVERSATIONS_PER_OUTPUT_FILE'] * multiplier),
            'AUTO_FLUSH_THRESHOLD': int(MASSIVE_PARALLEL_CONFIG['BASE_AUTO_FLUSH_THRESHOLD'] * multiplier),
        }
        
        print(f"‚öôÔ∏è CONFIGURACI√ìN OPTIMIZADA (patr√≥n h√≠brido extractor) para dataset {dataset_class}:")
        print(f"   üîÑ Total workers: {total_workers} (divisi√≥n por 3)")
        print(f"   üéØ Workers especializados:")
        print(f"      ‚Ä¢ Categorizaci√≥n: {self.adaptive_config['CATEGORY_WORKERS']}")
        print(f"      ‚Ä¢ Procesamiento: {self.adaptive_config['CONVERSATION_WORKERS']}")
        print(f"      ‚Ä¢ Escritura: {self.adaptive_config['OUTPUT_WORKERS']}")
        print(f"   üì¶ Configuraci√≥n de lotes optimizada:")
        print(f"      ‚Ä¢ Batch size: {self.adaptive_config['BATCH_SIZE']:,}")
        print(f"      ‚Ä¢ Queue size: {self.adaptive_config['QUEUE_SIZE']:,}")
        print(f"      ‚Ä¢ Auto-flush: {self.adaptive_config['AUTO_FLUSH_THRESHOLD']:,}")
        print(f"   üíæ Gesti√≥n de memoria realista:")
        print(f"      ‚Ä¢ Buffer: {self.adaptive_config['MEMORY_BUFFER_GB']:,}GB")
        
    def _discover_hybrid_files(self) -> List[Path]:
        """Descubre autom√°ticamente archivos generados por caroline_ultra_extractor_hybrid.py"""
        hybrid_files = []
        search_dirs = [self.input_dir, Path(".")]
        
        for search_dir in search_dirs:
            if search_dir.exists():
                files = list(search_dir.glob("articles_hybrid_*.jsonl"))
                hybrid_files.extend(files)
        
        # Remover duplicados y ordenar
        unique_files = list(set(hybrid_files))
        unique_files.sort()
        
        if not unique_files:
            print(f"‚ö†Ô∏è No se encontraron archivos h√≠bridos.")
        
        return unique_files
    
    def start_workers(self):
        """Inicia pools de workers especializados (patr√≥n h√≠brido extractor optimizado)"""
        print(f"üöÄ Iniciando workers especializados (patr√≥n h√≠brido extractor)...")
        
        # Workers de categorizaci√≥n (procesan art√≠culos y los clasifican)
        for i in range(self.category_workers):
            self.category_pool.submit(self._category_worker, i)
        
        # Workers de procesamiento de conversaciones (generan conversaciones)
        for i in range(self.conversation_workers):
            self.conversation_pool.submit(self._conversation_worker_universal, i)
        
        # Workers de escritura (escriben a disco)
        for i in range(self.output_workers):
            self.output_pool.submit(self._output_worker_universal, i)
        
        total_workers = self.category_workers + self.conversation_workers + self.output_workers
        print(f"‚úÖ {total_workers} workers especializados activos (patr√≥n h√≠brido)")
        print(f"   üìÇ Categorizaci√≥n: {self.category_workers} workers")
        print(f"   üí¨ Procesamiento: {self.conversation_workers} workers")
        print(f"   üìÅ Escritura: {self.output_workers} workers")
    
    def _category_worker(self, worker_id: int):
        """Worker de categorizaci√≥n ULTRA-OPTIMIZADO que distribuye art√≠culos SIN LOCKS"""
        while self.running:
            try:
                article_batch = self.article_queue.get(timeout=0.1)
                if article_batch is None:
                    break
                
                # OPTIMIZACI√ìN: Procesar todo el batch de una vez
                categorized_articles = defaultdict(list)
                
                for article in article_batch:
                    if not self.running:
                        break
                        
                    # Procesar art√≠culo usando ContentManager (SIN LOCKS)
                    result = self.content_manager.process_article(article)
                    
                    if result:
                        enhanced_article = {
                            **article,
                            'category': result['category'],
                            'subcategory': result['subcategory'],
                            'confidence': result['confidence'],
                            'conversations': result['conversations']
                        }
                        categorized_articles[result['category']].append(enhanced_article)
                    
                    self.stats['articles_processed'] += 1  # At√≥mico sin lock
                
                # Enviar art√≠culos categorizados a sus colas espec√≠ficas
                for category, articles in categorized_articles.items():
                    for article in articles:
                        try:
                            self.category_queues[category].put_nowait(article)
                            self.stats['articles_categorized'] += 1  # At√≥mico sin lock
                        except queue.Full:
                            # Si la cola est√° llena, usar categor√≠a general
                            try:
                                article['category'] = 'general'
                                self.category_queues['general'].put_nowait(article)
                            except queue.Full:
                                pass  # Skip si todas las colas est√°n llenas
                
                self.article_queue.task_done()
                
            except queue.Empty:
                if not self.running:
                    break
                continue
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error categor√≠a worker {worker_id}: {e}")
                if not self.running:
                    break
    
    def _conversation_worker(self, category: str, worker_id: int):
        """Worker especializado en generaci√≥n de conversaciones SIN LOCKS"""
        while self.running:
            try:
                article = self.category_queues[category].get(timeout=0.05)
                if article is None:
                    break
                
                if not self.running:
                    break
                
                # Usar conversaciones ya generadas por ContentManager
                conversations = article.get('conversations', [])
                
                # Si no hay conversaciones pre-generadas, usar ContentManager
                if not conversations:
                    result = self.content_manager.process_article(article)
                    conversations = result['conversations'] if result else []
                
                if conversations and self.running:
                    # A√±adir a buffer espec√≠fico del worker (SIN LOCKS EXPL√çCITOS)
                    self.category_buffers[category][worker_id].extend(conversations)
                    self.category_counters[category] += len(conversations)  # At√≥mico
                    
                    # Auto-flush si el buffer del worker est√° lleno
                    if len(self.category_buffers[category][worker_id]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                        self._flush_category_worker_buffer(category, worker_id)
                
                self.stats['conversations_generated'] += len(conversations)  # At√≥mico sin lock
                
                self.category_queues[category].task_done()
                
            except queue.Empty:
                if not self.running:
                    break
                continue
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error conversaci√≥n worker {category}-{worker_id}: {e}")
                if not self.running:
                    break
    
    def _conversation_worker_universal(self, worker_id: int):
        """Worker universal de conversaciones ULTRA-OPTIMIZADO (patr√≥n h√≠brido extractor)"""
        local_buffer = defaultdict(list)  # Buffer local para reducir contenci√≥n
        local_counter = 0
        
        while self.running:
            try:
                # Procesar desde todas las colas de categor√≠as de forma universal
                articles_processed = 0
                
                # OPTIMIZACI√ìN: Procesar m√∫ltiples art√≠culos por iteraci√≥n
                for category in self.get_categories():
                    try:
                        # Intentar obtener m√∫ltiples art√≠culos de una vez
                        batch_articles = []
                        for _ in range(min(10, self.category_queues[category].qsize())):  # Hasta 10 art√≠culos por batch
                            try:
                                article = self.category_queues[category].get_nowait()
                                if article is not None:
                                    batch_articles.append((category, article))
                                    self.category_queues[category].task_done()
                            except queue.Empty:
                                break
                        
                        # Procesar batch de art√≠culos
                        for cat, article in batch_articles:
                            if not self.running:
                                break
                                
                            # Usar conversaciones ya generadas por ContentManager
                            conversations = article.get('conversations', [])
                            
                            # Si no hay conversaciones pre-generadas, usar ContentManager
                            if not conversations:
                                result = self.content_manager.process_article(article)
                                conversations = result['conversations'] if result else []
                            
                            if conversations and self.running:
                                # A√±adir a buffer local para reducir contenci√≥n
                                for conv in conversations:
                                    conv['source_article'] = article.get('title', '')
                                    conv['worker_id'] = worker_id
                                
                                local_buffer[cat].extend(conversations)
                                local_counter += len(conversations)
                                articles_processed += 1
                                
                                # Flush local cuando alcance threshold
                                if len(local_buffer[cat]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                                    self.category_buffers[cat][worker_id].extend(local_buffer[cat])
                                    self.category_counters[cat] += len(local_buffer[cat])  # At√≥mico
                                    local_buffer[cat].clear()
                                    
                                    # Auto-flush si el buffer principal est√° lleno
                                    if len(self.category_buffers[cat][worker_id]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                                        self._flush_category_worker_buffer(cat, worker_id)
                            
                            self.stats['conversations_generated'] += len(conversations)  # At√≥mico sin lock
                        
                        if batch_articles:
                            break  # Pasar a siguiente worker si proces√≥ algo
                            
                    except Exception as e:
                        if self.running:
                            print(f"‚ö†Ô∏è Error batch processing {category}: {e}")
                        continue
                
                # Si no proces√≥ art√≠culos, flush buffers locales y pausa breve
                if articles_processed == 0:
                    # Flush buffers locales pendientes
                    for cat, buffer in local_buffer.items():
                        if buffer:
                            self.category_buffers[cat][worker_id].extend(buffer)
                            self.category_counters[cat] += len(buffer)
                            buffer.clear()
                    
                    if not self.running:
                        break
                    time.sleep(0.001)  # Pausa ultra-corta - 1ms para flujo continuo
                    
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error conversaci√≥n worker universal {worker_id}: {e}")
                if not self.running:
                    break
        
        # Flush final de buffers locales
        for cat, buffer in local_buffer.items():
            if buffer:
                self.category_buffers[cat][worker_id].extend(buffer)
                self.category_counters[cat] += len(buffer)
    
    def _output_worker_universal(self, worker_id: int):
        """Worker universal de escritura paralela SIN LOCKS (patr√≥n h√≠brido extractor optimizado)"""
        while self.running:
            try:
                # Verificar buffers de todas las categor√≠as para este worker
                for category in self.get_categories():
                    if len(self.category_buffers[category][worker_id]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                        self._flush_category_worker_buffer(category, worker_id)
                
                # Sleep ultra-corto para m√°xima responsividad con 450GB RAM
                time.sleep(0.01)
                
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error output worker universal {worker_id}: {e}")
                if not self.running:
                    break
    
    def _flush_category_worker_buffer(self, category: str, worker_id: int):
        """Escribe buffer espec√≠fico de un worker a archivo JSONL SIN LOCKS"""
        if not self.category_buffers[category][worker_id]:
            return
        
        try:
            # Crear archivo espec√≠fico por categor√≠a y worker
            output_file = self.category_dirs[category] / f"conversaciones_{category}_{worker_id}_{self.file_counters[category]:04d}.jsonl"
            
            # Escribir conversaciones con buffering ULTRA-MASIVO para 450GB RAM
            with open(output_file, 'w', encoding='utf-8', buffering=64*1024*1024) as f:  # 64MB buffer
                for conversation in self.category_buffers[category][worker_id]:
                    # Crear registro de conversaci√≥n completo
                    conversation_record = {
                        'id': f"{category}_{worker_id}_{self.file_counters[category]}_{len(self.category_buffers[category][worker_id])}",
                        'category': category,
                        'subcategory': conversation.get('subcategory', 'general'),
                        'article_title': conversation.get('source_article', ''),
                        'conversation': [
                            {'role': 'user', 'content': conversation['question']},
                            {'role': 'assistant', 'content': conversation['answer']}
                        ],
                        'metadata': {
                            'source_article': conversation.get('source_article', ''),
                            'category': category,
                            'subcategory': conversation.get('subcategory', 'general'),
                            'conversation_type': conversation.get('conversation_type', 'general'),
                            'generation_date': datetime.now().isoformat(),
                            'worker_id': worker_id
                        }
                    }
                    f.write(json.dumps(conversation_record, ensure_ascii=False) + '\n')
            
            conversations_written = len(self.category_buffers[category][worker_id])
            conversations_written = len(self.category_buffers[category][worker_id])
            self.category_buffers[category][worker_id].clear()
            self.file_counters[category] += 1
            
            self.stats['files_written'] += 1  # At√≥mico sin lock
            
            print(f"üíæ {category}-W{worker_id}: {conversations_written:,} conversaciones ‚Üí {output_file.name}")
            
        except Exception as e:
            print(f"‚ùå Error flush {category}-{worker_id}: {e}")
    
    def load_hybrid_files_batch(self, batch_size: int = None) -> Iterator[List[Dict]]:
        """Carga art√≠culos de archivos h√≠bridos en batches - MODO STREAMING SIMPLIFICADO Y ROBUSTO"""
        if batch_size is None:
            batch_size = self.adaptive_config['BATCH_SIZE']
        
        total_articles_processed = 0
        
        print(f"üìö MODO STREAMING SIMPLIFICADO: {len(self.hybrid_files)} archivos, batch_size={batch_size:,}")
        
        # SIMPLIFICACI√ìN: Procesar archivos secuencialmente pero con batches grandes
        current_batch = []
        
        for file_idx, hybrid_file in enumerate(self.hybrid_files, 1):
            try:
                file_size_mb = hybrid_file.stat().st_size / 1024 / 1024
                print(f"üìñ Procesando archivo {file_idx}/{len(self.hybrid_files)}: {hybrid_file.name} ({file_size_mb:.1f}MB)")
                
                file_articles = 0
                with open(hybrid_file, 'r', encoding='utf-8', buffering=128*1024*1024) as f:  # 128MB buffer para 450GB RAM
                    for line_num, line in enumerate(f, 1):
                        if not line.strip():
                            continue
                        
                        try:
                            article = json.loads(line)
                            if (article.get('title') and 
                                article.get('content') and 
                                len(article.get('content', '')) > 100):
                                current_batch.append(article)
                                file_articles += 1
                                total_articles_processed += 1
                                
                                # Enviar batch cuando est√© lleno
                                if len(current_batch) >= batch_size:
                                    print(f"üì® Enviando batch: {len(current_batch):,} art√≠culos (total: {total_articles_processed:,})")
                                    yield current_batch
                                    current_batch = []
                                
                                # FLUJO CONTINUO: Enviar batch parcial cada 5000 art√≠culos para evitar esperas
                                elif len(current_batch) % 5000 == 0 and len(current_batch) > 0:
                                    print(f"üì§ Enviando batch parcial: {len(current_batch):,} art√≠culos (flujo continuo)")
                                    yield current_batch.copy()  # Enviar copia para flujo continuo
                                    # NO limpiar current_batch aqu√≠ para mantener el batch principal
                                    
                        except json.JSONDecodeError:
                            continue
                        except Exception:
                            continue
                        
                        # Progreso cada 20K l√≠neas por archivo
                        if line_num % 50000 == 0:
                            print(f"   ÔøΩ Procesadas {line_num:,} l√≠neas, {file_articles:,} art√≠culos v√°lidos")
                
                print(f"‚úÖ {hybrid_file.name}: {file_articles:,} art√≠culos v√°lidos procesados")
                
                # Gesti√≥n de memoria cada 2 archivos (m√°s frecuente para 450GB RAM)
                if file_idx % 2 == 0:  # Cada 2 archivos
                    gc.collect()
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error procesando {hybrid_file.name}: {e}")
                continue
        
        # Enviar √∫ltimo batch si tiene contenido
        if current_batch:
            print(f"üì® Enviando batch final: {len(current_batch):,} art√≠culos")
            yield current_batch
        
        print(f"‚úÖ STREAMING COMPLETADO: {total_articles_processed:,} art√≠culos de {len(self.hybrid_files)} archivos")
    
    def add_article_batch(self, articles: List[Dict]) -> bool:
        """A√±ade batch de art√≠culos al pipeline SIN LOCKS"""
        try:
            self.article_queue.put(articles, timeout=0.1)
            return True
        except queue.Full:
            return False
    
    def process_all_files_massive(self):
        """Procesa TODOS los archivos h√≠bridos con enfoque masivo paralelo SIN LOCKS"""
        if not self.hybrid_files:
            print(f"‚ùå No se encontraron archivos h√≠bridos para procesar")
            return False
        
        print(f"üöÄ INICIANDO PROCESAMIENTO MASIVO PARALELO SIN LOCKS")
        print(f"   üìÅ Archivos h√≠bridos: {len(self.hybrid_files)}")
        print(f"   üéØ Modo: PROCESAMIENTO COMPLETO (100% art√≠culos)")
        print(f"   üî¢ Conversaciones por art√≠culo: {MASSIVE_PARALLEL_CONFIG['CONVERSATIONS_PER_ARTICLE']}")
        print(f"   üöÄ Paralelizaci√≥n: SIN LOCKS para m√°ximo rendimiento")
        
        # Iniciar workers especializados
        self.start_workers()
        
        # Manejo de se√±ales
        def signal_handler(signum, frame):
            print(f"\n‚ö†Ô∏è Se√±al recibida, finalizando...")
            self._flush_all_category_buffers()
            self.stop_workers()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # Procesar archivos en batches
        start_time = time.time()
        total_batches = 0
        
        print(f"üîÑ Iniciando iteraci√≥n de batches...")
        
        try:
            for batch in self.load_hybrid_files_batch(self.adaptive_config['BATCH_SIZE']):
                print(f"üì¶ Recibido batch #{total_batches + 1} con {len(batch):,} art√≠culos")
                
                # Enviar batch a workers (sin l√≠mites - procesamos todo)
                attempts = 0
                while not self.add_article_batch(batch):
                    attempts += 1
                    if attempts > 100:  # Evitar bucle infinito
                        print(f"‚ö†Ô∏è Cola llena despu√©s de {attempts} intentos, continuando...")
                        break
                    time.sleep(0.01)
                
                total_batches += 1
                print(f"‚úÖ Batch #{total_batches} enviado a workers")
                
                # Progreso cada 10 batches (m√°s frecuente para diagn√≥stico de flujo continuo)
                if total_batches % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = self.stats['articles_processed'] / elapsed if elapsed > 0 else 0
                    
                    print(f"üìä Progreso Masivo: {total_batches:,} batches | "
                          f"{self.stats['articles_processed']:,} procesados | "
                          f"{self.stats['articles_categorized']:,} categorizados | "
                          f"{self.stats['conversations_generated']:,} conversaciones | "
                          f"{rate:.0f} a/s")
                    
                    # Diagn√≥stico de colas
                    total_queue_size = sum(q.qsize() for q in self.category_queues.values())
                    article_queue_size = self.article_queue.qsize()
                    print(f"üîç Estado colas: Art√≠culos({article_queue_size}) Categor√≠as({total_queue_size})")
            
            print(f"üéâ Todos los batches procesados: {total_batches:,} batches totales")
            
        except Exception as e:
            print(f"‚ùå Error durante procesamiento de batches: {e}")
            import traceback
            traceback.print_exc()
        
        return True
    
    def _flush_all_category_buffers(self):
        """Flushea todos los buffers de todas las categor√≠as y workers"""
        print(f"üîÑ Finalizando escritura de todos los buffers...")
        
        for category in self.get_categories():
            for worker_id in self.category_buffers[category].keys():
                if self.category_buffers[category][worker_id]:
                    self._flush_category_worker_buffer(category, worker_id)
    
    def stop_workers(self):
        """Detiene todos los workers de forma robusta"""
        print(f"üõë DETENIENDO WORKERS...")
        self.running = False
        
        # Shutdown de pools
        self.category_pool.shutdown(wait=False)
        self.conversation_pool.shutdown(wait=False)
        self.output_pool.shutdown(wait=False)
        
        print(f"‚úÖ Workers detenidos")
        
    def discover_hybrid_files(self) -> List[Path]:
        """Descubre todos los archivos h√≠bridos disponibles"""
        return self._discover_hybrid_files()
        
    def process_all_files(self):
        """Procesa todos los archivos h√≠bridos con paralelizaci√≥n masiva"""
        if not self.hybrid_files:
            print("‚ùå No se encontraron archivos h√≠bridos")
            return False
            
        print(f"\nüöÄ INICIANDO PROCESAMIENTO MASIVO SIN LOCKS")
        print(f"   üìÅ Archivos a procesar: {len(self.hybrid_files)}")
        print(f"   üéØ Categorizaci√≥n inteligente: ACTIVADA")
        print(f"   üìù Conversaciones de entrenamiento: ACTIVADAS")
        print(f"   üöÄ Paralelizaci√≥n: MASIVA SIN LOCKS")
        
        # Procesar con sistema masivo paralelo
        success = self.process_all_files_massive()
        
        if success:
            # Finalizar procesamiento
            self.finalize_processing()
        
        return success
        
    def process_hybrid_file(self, file_path: Path):
        """Procesa un archivo h√≠brido individual"""
        articles_in_file = 0
        conversations_in_file = 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                    
                try:
                    article = json.loads(line)
                    
                    # Procesar art√≠culo
                    result = self.process_single_article(article)
                    
                    if result:
                        articles_in_file += 1
                        conversations_in_file += len(result['conversations'])
                        
                        # Guardar resultado
                        self.save_article_result(result)
                        
                except json.JSONDecodeError as e:
                    if line_num % 10000 == 0:  # Log error cada 10K l√≠neas
                        print(f"‚ö†Ô∏è Error JSON l√≠nea {line_num}: {e}")
                    continue
                except Exception as e:
                    print(f"‚ö†Ô∏è Error procesando art√≠culo l√≠nea {line_num}: {e}")
                    continue
                    
                # Progreso cada 1000 art√≠culos
                if articles_in_file % 1000 == 0:
                    print(f"   üìä Progreso: {articles_in_file:,} art√≠culos, {conversations_in_file:,} conversaciones")
                    
        print(f"   ‚úÖ Completado: {articles_in_file:,} art√≠culos, {conversations_in_file:,} conversaciones")
        
        # Actualizar estad√≠sticas globales
        self.stats['files_processed'] += 1
        self.stats['articles_processed'] += articles_in_file
        self.stats['conversations_generated'] += conversations_in_file
            
    def process_single_article(self, article: Dict) -> Optional[Dict]:
        """Procesa un art√≠culo individual usando el ContentManager externo"""
        title = article.get('title', '').strip()
        content = article.get('content', '').strip()
        
        if not title or not content:
            return None
            
        # Procesar art√≠culo usando el ContentManager externo
        result = self.content_manager.process_article(article)
        
        if not result:
            return None
            
        # Reestructurar para mantener compatibilidad con el formato existente
        formatted_result = {
            'id': f"{result['category']}_{hashlib.md5(title.encode()).hexdigest()[:8]}",
            'title': result['title'],
            'content': content,
            'category': result['category'],
            'subcategory': result['subcategory'],
            'categorization_confidence': result['confidence'],
            'conversations': result['conversations'],
            'metadata': {
                'source': 'content_manager',
                'processed_at': result['metadata']['processed_at'],
                'content_length': result['metadata']['content_length'],
                'conversations_count': result['metadata']['conversations_count'],
                'original_article': article
            }
        }
        
        # Actualizar contadores (sin lock para mejor rendimiento)
        self.category_counters[result['category']] += 1
        self.stats['articles_categorized'] += 1
        self.stats['conversations_generated'] += len(result['conversations'])
            
        # Registrar problemas de confianza baja
        if result['confidence'] < 0.5:
            self.problematic_categorizations.append({
                'title': title,
                'category': result['category'],
                'confidence': result['confidence'],
                'reason': 'low_confidence'
            })
            
        return formatted_result
        
    def save_article_result(self, result: Dict):
        """Guarda el resultado de un art√≠culo en la categor√≠a correspondiente"""
        category = result['category']
        
        # Crear directorio de categor√≠a si no existe
        category_dir = self.categories_dir / category
        category_dir.mkdir(exist_ok=True)
        
        # Determinar archivo de salida
        file_counter = self.file_counters[category]
        conversations_per_file = 10000  # 10K conversaciones por archivo
        
        if file_counter % conversations_per_file == 0:
            file_num = (file_counter // conversations_per_file) + 1
            current_file = category_dir / f"conversaciones_{category}_{file_num:04d}.jsonl"
        else:
            file_num = (file_counter // conversations_per_file) + 1
            current_file = category_dir / f"conversaciones_{category}_{file_num:04d}.jsonl"
            
        # Escribir conversaciones individuales
        with open(current_file, 'a', encoding='utf-8') as f:
            for conv in result['conversations']:
                conversation_record = {
                    'id': f"{result['id']}_{len(result['conversations'])}",
                    'category': category,
                    'subcategory': result['subcategory'],
                    'article_title': result['title'],
                    'conversation': [
                        {'role': 'user', 'content': conv['question']},
                        {'role': 'assistant', 'content': conv['answer']}
                    ],
                    'metadata': {
                        'source_article': result['title'],
                        'category': category,
                        'subcategory': result['subcategory'],
                        'conversation_type': conv['conversation_type'],
                        'content_length': len(result['content']),
                        'categorization_confidence': result['categorization_confidence'],
                        'generation_date': datetime.now().isoformat()
                    }
                }
                
                f.write(json.dumps(conversation_record, ensure_ascii=False) + '\n')
                
        self.file_counters[category] += len(result['conversations'])
        
    def finalize_processing(self):
        """Finaliza el procesamiento con modo inteligente ultra-optimizado"""
        print(f"\nüéâ INICIANDO FINALIZACI√ìN ULTRA-OPTIMIZADA")
        
        # Usar finalizaci√≥n inteligente sin esperas prolongadas
        self._finalize_processing_intelligent()
        
        # Generar conscious.txt
        self.generate_conscious_file()
        
        # Generar metadatos por categor√≠a
        self.generate_category_metadata()
        
        # Generar estad√≠sticas
        self.generate_final_statistics()
        
        # Mostrar resumen final
        self.show_final_summary()
        
    def generate_conscious_file(self):
        """Genera el archivo conscious.txt con la base fundamental"""
        conscious_file = self.output_dir / "conscious.txt"
        
        with open(conscious_file, 'w', encoding='utf-8') as f:
            f.write("# CAROLINE CONSCIOUS KNOWLEDGE - BASE FUNDAMENTAL CORREGIDA\n")
            f.write("# =========================================================\n")
            f.write(f"# Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# Art√≠culos procesados: {self.stats['articles_processed']:,}\n")
            f.write(f"# Conversaciones generadas: {self.stats['conversations_generated']:,}\n")
            f.write(f"# Categor√≠as identificadas: {len(self.get_categories())}\n")
            f.write("# Sistema de categorizaci√≥n inteligente: ACTIVADO\n")
            f.write("# Correcci√≥n de problemas de categorizaci√≥n: APLICADA\n")
            f.write("# =========================================================\n\n")
            
            f.write("## SISTEMA CONSCIENTE MEJORADO\n")
            f.write("soy caroline, modelo de ia especializado en wikipedia espa√±ola\n")
            f.write("he procesado 1.5 millones de art√≠culos con categorizaci√≥n corregida\n")
            f.write("cada art√≠culo ha sido analizado con patrones inteligentes\n")
            f.write("las categor√≠as err√≥neas han sido corregidas autom√°ticamente\n")
            f.write("genero conversaciones espec√≠ficas por categor√≠a y subcategor√≠a\n")
            f.write("mi conocimiento est√° estructurado en carpetas organizadas\n\n")
            
            f.write("## CATEGOR√çAS PROCESADAS CON CORRECCI√ìN\n")
            total_conversations = sum(self.category_counters.values())
            
            for category in sorted(self.get_categories()):
                count = self.category_counters[category]
                percentage = (count / total_conversations * 100) if total_conversations > 0 else 0
                
                f.write(f"### {category.upper()}\n")
                f.write(f"- Art√≠culos: {count:,} ({percentage:.1f}%)\n")
                f.write(f"- Directorio: consciencia/categorias/{category}/\n")
                f.write(f"- Archivos: conversaciones_{category}_NNNN.jsonl\n")
                
                # Mostrar subcategor√≠as si existen
                category_dir = self.categories_dir / category
                if category_dir.exists():
                    jsonl_files = list(category_dir.glob("*.jsonl"))
                    f.write(f"- Archivos generados: {len(jsonl_files)}\n")
                    
                f.write("\n")
                
            f.write("## MEJORAS APLICADAS\n")
            f.write("1. Correcci√≥n de categorizaci√≥n musical (ej: Catch a Fire ‚Üí arte)\n")
            f.write("2. Identificaci√≥n precisa de biograf√≠as vs. otros contenidos\n")
            f.write("3. Separaci√≥n clara entre ciencias y otras disciplinas\n")
            f.write("4. Subcategorizaci√≥n espec√≠fica por tipo de contenido\n")
            f.write("5. Conversaciones contextuales por √°rea de conocimiento\n\n")
            
            f.write("## INSTRUCCIONES DE USO\n")
            f.write("- Cada categor√≠a tiene su propio directorio\n")
            f.write("- Las conversaciones est√°n en formato JSONL listo para entrenamiento\n")
            f.write("- Metadatos incluyen confianza de categorizaci√≥n\n")
            f.write("- Subcategor√≠as permiten especializaci√≥n fina\n")
            f.write("- Sistema autocorrige errores de categorizaci√≥n comunes\n")
            
        print(f"üìù Archivo conscious.txt generado: {conscious_file}")
        
    def generate_category_metadata(self):
        """Genera metadatos por cada categor√≠a"""
        for category in self.get_categories():
            if self.category_counters[category] > 0:  # Solo categor√≠as con contenido
                category_dir = self.categories_dir / category
                metadata_file = category_dir / f"metadata_{category}.json"
                
                # Contar archivos
                jsonl_files = list(category_dir.glob("*.jsonl"))
                total_conversations = self.category_counters[category]
                
                metadata = {
                    'category': category,
                    'total_articles': total_conversations,
                    'total_files': len(jsonl_files),
                    'files_generated': [f.name for f in jsonl_files],
                    'generation_date': datetime.now().isoformat(),
                    'description': f"Conversaciones de entrenamiento para la categor√≠a {category}",
                    'conversation_types': ['definicion', 'temporal', 'espacial', 'explicacion', 'contextualizacion'],
                    'processing_mode': 'massive_parallel_no_locks'
                }
                
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
                print(f"üìä Metadatos generados: {metadata_file}")
            
    def generate_final_statistics(self):
        """Genera estad√≠sticas finales del procesamiento"""
        
        # Distribuci√≥n de categor√≠as
        distribution_file = self.stats_dir / "distribucion_categorias.json"
        total_conversations = sum(self.category_counters.values())
        
        distribution = {
            'total_conversations': total_conversations,
            'total_categories': len(self.category_counters),
            'distribution': {
                category: {
                    'count': count,
                    'percentage': (count / total_conversations * 100) if total_conversations > 0 else 0
                }
                for category, count in sorted(self.category_counters.items(), key=lambda x: x[1], reverse=True)
            },
            'generation_date': datetime.now().isoformat()
        }
        
        with open(distribution_file, 'w', encoding='utf-8') as f:
            json.dump(distribution, f, ensure_ascii=False, indent=2)
            
        # Problemas de categorizaci√≥n
        problems_file = self.stats_dir / "problemas_categorizacion.json"
        
        problems = {
            'total_problematic': len(self.problematic_categorizations),
            'low_confidence_threshold': 0.5,
            'problems': self.problematic_categorizations[:100],  # Top 100 problemas
            'analysis_date': datetime.now().isoformat()
        }
        
        with open(problems_file, 'w', encoding='utf-8') as f:
            json.dump(problems, f, ensure_ascii=False, indent=2)
            
        # Resumen de procesamiento
        summary_file = self.stats_dir / "resumen_procesamiento.json"
        
        processing_time = time.time() - self.stats['start_time']
        
        summary = {
            'processing_summary': {
                'total_articles': self.stats['articles_processed'],
                'total_conversations': self.stats['conversations_generated'],
                'total_categories': len([c for c in self.get_categories() if self.category_counters[c] > 0]),
                'files_written': self.stats['files_written'],
                'processing_errors': self.stats['processing_errors'],
                'processing_time_seconds': processing_time,
                'processing_time_hours': processing_time / 3600,
                'articles_per_second': self.stats['articles_processed'] / processing_time if processing_time > 0 else 0,
                'processing_mode': 'massive_parallel_no_locks'
            },
            'categories_created': [c for c in self.get_categories() if self.category_counters[c] > 0],
            'output_structure': {
                'base_directory': str(self.output_dir),
                'categories_directory': str(self.categories_dir),
                'statistics_directory': str(self.stats_dir),
                'conscious_file': str(self.output_dir / "conscious.txt")
            },
            'generation_date': datetime.now().isoformat()
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        print(f"üìà Estad√≠sticas generadas en: {self.stats_dir}")
        
    def show_final_summary(self):
        """Muestra resumen final del procesamiento"""
        processing_time = time.time() - self.stats['start_time']
        
        print(f"\nüéâ PROCESAMIENTO MASIVO FINALIZADO")
        print(f"   ‚è±Ô∏è Tiempo total: {processing_time/3600:.1f} horas")
        print(f"   üìö Art√≠culos procesados: {self.stats['articles_processed']:,}")
        print(f"   üí¨ Conversaciones generadas: {self.stats['conversations_generated']:,}")
        print(f"   üóÇÔ∏è Categor√≠as creadas: {len([c for c in self.get_categories() if self.category_counters[c] > 0])}")
        print(f"   üìÅ Archivos generados: {self.stats['files_written']:,}")
        print(f"   ‚ö†Ô∏è Errores de procesamiento: {self.stats['processing_errors']}")
        print(f"   üöÄ Velocidad: {self.stats['articles_processed']/(processing_time/3600):.0f} art√≠culos/hora")
        print(f"   üî• Modo: PARALELIZACI√ìN MASIVA SIN LOCKS")
        
        print(f"\nüìÇ ESTRUCTURA GENERADA:")
        print(f"   üìÅ {self.output_dir}/")
        print(f"   ‚îú‚îÄ‚îÄ conscious.txt (base fundamental)")
        print(f"   ‚îú‚îÄ‚îÄ categorias/ (categor√≠as con contenido)")
        
        for category in sorted(self.get_categories()):
            count = self.category_counters[category]
            if count > 0:
                files = len(list((self.categories_dir / category).glob("*.jsonl"))) if (self.categories_dir / category).exists() else 0
                print(f"   ‚îÇ   ‚îú‚îÄ‚îÄ {category}/ ({count:,} conversaciones, {files} archivos)")
            
        print(f"   ‚îî‚îÄ‚îÄ estadisticas/ (an√°lisis y metadatos)")
        
        print(f"\n‚úÖ Dataset de entrenamiento masivo listo en: {self.output_dir}")
    
    def _finalize_processing_intelligent(self):
        """Finalizaci√≥n autom√°tica inteligente sin esperas prolongadas (del extractor hybrid)"""
        print(f"üîÑ INICIANDO FINALIZACI√ìN INTELIGENTE SIN LOCKS...")
        
        # 1. Detener la alimentaci√≥n de nuevos trabajos
        self.running = False
        
        # 2. Esperar con timeout adaptativo basado en el volumen
        estimated_articles = sum(self.category_counters.values())
        expected_time = max(10, min(60, estimated_articles // 50000))  # 10-60 segundos seg√∫n volumen
        print(f"‚è≥ Esperando finalizaci√≥n (m√°ximo {expected_time} segundos basado en {estimated_articles:,} conversaciones)...")
        
        start_wait = time.time()
        last_conversations = sum(self.category_counters.values())
        stable_count = 0  # Contador de estabilidad
        
        while time.time() - start_wait < expected_time:
            time.sleep(2)  # Check cada 2 segundos
            current_conversations = sum(self.category_counters.values())
            
            # Estado de colas
            total_queue_size = sum(q.qsize() for q in self.category_queues.values())
            article_queue_size = self.article_queue.qsize()
            
            elapsed_wait = time.time() - start_wait
            print(f"üìä T+{elapsed_wait:.0f}s: {current_conversations:,} conversaciones | Colas: Art({article_queue_size}) Cat({total_queue_size})")
            
            # Verificar estabilidad
            if current_conversations == last_conversations:
                stable_count += 1
            else:
                stable_count = 0
                last_conversations = current_conversations
            
            # Condiciones de finalizaci√≥n inteligente
            if article_queue_size == 0 and total_queue_size == 0:
                if stable_count >= 2:  # 4 segundos estable con colas vac√≠as
                    print(f"‚úÖ Procesamiento completado (colas vac√≠as + estable)")
                    break
            elif stable_count >= 3:  # 6 segundos sin cambios
                print(f"‚úÖ Procesamiento estabilizado (sin nuevas conversaciones)")
                break
        
        # 3. Finalizaci√≥n ultra-agresiva sin locks
        elapsed_total = time.time() - start_wait
        print(f"üõë Finalizaci√≥n inteligente despu√©s de {elapsed_total:.1f}s")
        
        # Flush final de todos los buffers
        self._flush_all_buffers_final()
        
        # Shutdown pools sin esperar
        print(f"üö® Shutdown ultra-agresivo de pools...")
        try:
            self.category_pool.shutdown(wait=False)
            self.conversation_pool.shutdown(wait=False) 
            self.output_pool.shutdown(wait=False)
            print(f"‚úÖ Pools terminados sin espera")
        except Exception as e:
            print(f"‚ö†Ô∏è Error en shutdown: {e}")
        
        print(f"üî• FINALIZACI√ìN INTELIGENTE COMPLETADA")
    
    def _flush_all_buffers_final(self):
        """Flush final de todos los buffers sin locks"""
        print(f"üíæ FLUSH FINAL DE TODOS LOS BUFFERS...")
        
        total_flushed = 0
        for category in self.get_categories():
            if self.category_counters[category] > 0:
                # Flush todos los workers de esta categor√≠a
                for worker_id in self.category_buffers[category]:
                    buffer = self.category_buffers[category][worker_id]
                    if buffer:
                        try:
                            self._flush_category_worker_buffer(category, worker_id)
                            total_flushed += len(buffer)
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error flush final {category}-{worker_id}: {e}")
        
        print(f"‚úÖ Flush final completado: {total_flushed:,} conversaciones")


def main():
    """Funci√≥n principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Full Dataset For Training - Procesador Completo")
    
    # Accept both --input/--output and --input-dir/--output-dir for flexibility
    parser.add_argument("--input", "--input-dir", "-i", type=str, 
                       default="./data_ultra_hybrid",
                       help="Directorio de entrada con archivos h√≠bridos")
    parser.add_argument("--output", "--output-dir", "-o", type=str, 
                       default="consciencia",
                       help="Directorio de salida")
    
    args = parser.parse_args()
    
    print("üß† FULL DATASET FOR TRAINING - Procesador Completo")
    print("=" * 60)
    
    # Configurar directorios desde argumentos CLI
    input_dir = args.input
    output_dir = args.output
    
    print(f"üìÇ Input: {input_dir}")
    print(f"üìÅ Output: {output_dir}")
    
    # Verificar input
    if not Path(input_dir).exists():
        print(f"‚ùå Directorio de entrada no encontrado: {input_dir}")
        print("üí° Aseg√∫rate de que los archivos h√≠bridos est√©n disponibles")
        return 1
        
    # Crear procesador masivo
    processor = MassiveParallelDatasetProcessor(input_dir, output_dir)
    
    # Procesar todo
    success = processor.process_all_files()
    
    if success:
        print("\nüéâ PROCESAMIENTO EXITOSO")
        print("üìã El dataset de entrenamiento est√° listo para usar")
        print("üß† Base de conocimiento consciente generada")
        return 0
    else:
        print("\n‚ùå PROCESAMIENTO FALLIDO")
        return 1


if __name__ == "__main__":
    sys.exit(main())
