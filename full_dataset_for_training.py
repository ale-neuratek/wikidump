#!/usr/bin/env python3
"""
üß† FULL DATASET FOR TRAINING - Procesador Completo con Categor√≠a Consciencia
=============================================================================
Procesador masivo paralelo con logging optimizado y categor√≠a consciencia

OBJETIVOS:
- Procesar 100% de los art√≠culos h√≠bridos disponibles
- Crear categor√≠a "consciencia" con descripci√≥n del conocimiento disponible
- Logging con timestamps cada 5-10 minutos para seguimiento
- Generar conversaciones de entrenamiento usando content_manager
- Sistema de categorizaci√≥n inteligente sin conscious.txt

ESTRUCTURA DE SALIDA:
consciencia_completa/
‚îú‚îÄ‚îÄ categorias/
‚îÇ   ‚îú‚îÄ‚îÄ consciencia/           ‚Üê NUEVA: Descripci√≥n del conocimiento disponible
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversaciones_consciencia_0001.jsonl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata_consciencia.json
‚îÇ   ‚îú‚îÄ‚îÄ arte/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversaciones_arte_0001.jsonl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata_arte.json
‚îÇ   ‚îú‚îÄ‚îÄ ciencias/
‚îÇ   ‚îî‚îÄ‚îÄ ... (todas las categor√≠as encontradas)
‚îî‚îÄ‚îÄ estadisticas/
    ‚îú‚îÄ‚îÄ distribucion_categorias.json
    ‚îî‚îÄ‚îÄ resumen_procesamiento.json
"""

import json
import re
import os
import sys
import time
import hashlib
import threading
import queue
import gc
import signal
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Set, Iterator
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback

# Importar configuraciones din√°micas por hardware
from hardware_configs import get_hardware_config, print_hardware_info

# Importar el gestor de contenido externo
from content_manager import ContentManager
class MassiveParallelDatasetProcessor:
    """Procesador de dataset simplificado, optimizaci√≥n manejada por adaptive_processor"""
    
    def __init__(self, input_dir: str, output_dir: str = "consciencia_completa"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Configuraci√≥n por defecto (ser√° sobrescrita por adaptive_processor)
        self.adaptive_config = {
            'CATEGORY_WORKERS': 8,
            'CONVERSATION_WORKERS': 16,
            'OUTPUT_WORKERS': 8,
            'BATCH_SIZE': 50000,
            'QUEUE_SIZE': 1000,
            'CONVERSATIONS_PER_OUTPUT_FILE': 50000,
            'AUTO_FLUSH_THRESHOLD': 25000,
            'MAX_QUEUE_RETRIES': 30,
            'QUEUE_TIMEOUT': 5.0
        }
        
        # Logger (ser√° inyectado por adaptive_processor)
        self.logger = None
        
        # Inicializar gestor de contenido
        self.content_manager = ContentManager()
        
        # Crear estructura de carpetas
        self.categories_dir = self.output_dir / "categorias"
        self.stats_dir = self.output_dir / "estadisticas"
        self.categories_dir.mkdir(exist_ok=True)
        self.stats_dir.mkdir(exist_ok=True)
        
        # Crear carpetas por categor√≠a
        self.category_dirs = {}
        for category in self.get_categories():
            category_dir = self.categories_dir / category
            category_dir.mkdir(exist_ok=True)
            self.category_dirs[category] = category_dir
        
        # Thread pools (inicializados con configuraci√≥n por defecto)
        self._init_workers()
        
        # Colas especializadas
        self._init_queues()
        
        # Buffers y contadores
        self.category_buffers = {category: defaultdict(list) for category in self.get_categories()}
        self.category_counters = {category: 0 for category in self.get_categories()}
        self.file_counters = {category: 0 for category in self.get_categories()}
        
        # Estado y estad√≠sticas
        self.running = True
        self.stats = {
            'articles_processed': 0,
            'articles_categorized': 0,
            'conversations_generated': 0,
            'files_written': 0,
            'processing_errors': 0,
            'start_time': time.time()
        }
        
        # Descubrir archivos
        self.hybrid_files = self._discover_hybrid_files()
        
        self._log(f"üß† PROCESADOR INICIADO", force=True)
        self._log(f"   üìÇ Input: {self.input_dir}")
        self._log(f"   üìÅ Output: {self.output_dir}")
        self._log(f"   üìÑ Archivos encontrados: {len(self.hybrid_files)}")
        self._log(f"   üìÇ Categor√≠as: {len(self.get_categories())}")
    
    def _log(self, message: str, force: bool = False):
        """Log usando el logger inyectado o print por defecto"""
        if self.logger:
            self.logger.log(message, force=force)
        else:
            if force:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
    
    def _init_workers(self):
        """Inicializar workers con configuraci√≥n actual"""
        if hasattr(self, 'category_pool'):
            self.category_pool.shutdown(wait=False)
        if hasattr(self, 'conversation_pool'):
            self.conversation_pool.shutdown(wait=False)
        if hasattr(self, 'output_pool'):
            self.output_pool.shutdown(wait=False)
        
        self.category_pool = ThreadPoolExecutor(
            max_workers=self.adaptive_config['CATEGORY_WORKERS'], 
            thread_name_prefix="category"
        )
        self.conversation_pool = ThreadPoolExecutor(
            max_workers=self.adaptive_config['CONVERSATION_WORKERS'], 
            thread_name_prefix="conversation"
        )
        self.output_pool = ThreadPoolExecutor(
            max_workers=self.adaptive_config['OUTPUT_WORKERS'], 
            thread_name_prefix="output"
        )
    
    def _init_queues(self):
        """Inicializar colas con configuraci√≥n actual"""
        queue_size = self.adaptive_config['QUEUE_SIZE']
        self.article_queue = queue.Queue(maxsize=queue_size)
        self.category_queues = {
            category: queue.Queue(maxsize=queue_size) 
            for category in self.get_categories()
        }
        
    def get_categories(self) -> List[str]:
        """Obtiene lista de categor√≠as principales del gestor de contenido"""
        return self.content_manager.get_categories()
    
    def _discover_hybrid_files(self) -> List[Path]:
        """Descubre archivos h√≠bridos disponibles"""
        hybrid_files = []
        search_dirs = [self.input_dir, Path(".")]
        
        for search_dir in search_dirs:
            if search_dir.exists():
                files = list(search_dir.glob("articles_hybrid_*.jsonl"))
                hybrid_files.extend(files)
        
        # Remover duplicados y ordenar
        unique_files = list(set(hybrid_files))
        unique_files.sort()
        
        return unique_files
    
    def start_workers(self):
        """Inicia pools de workers especializados (patr√≥n h√≠brido extractor optimizado)"""
        print(f"üöÄ Iniciando workers especializados (patr√≥n h√≠brido extractor)...")
        
        # Workers de categorizaci√≥n (procesan art√≠culos y los clasifican)
        for i in range(self.category_workers):
            self.category_pool.submit(self._category_worker, i)
        
        # Workers de procesamiento de conversaciones (generan conversaciones)
        for i in range(self.conversation_workers):
            self.conversation_pool.submit(self._conversation_worker_universal, i)
        
        # Workers de escritura (escriben a disco)
        for i in range(self.output_workers):
            self.output_pool.submit(self._output_worker_universal, i)
        
        total_workers = self.category_workers + self.conversation_workers + self.output_workers
        print(f"‚úÖ {total_workers} workers especializados activos (patr√≥n h√≠brido)")
        print(f"   üìÇ Categorizaci√≥n: {self.category_workers} workers")
        print(f"   üí¨ Procesamiento: {self.conversation_workers} workers")
        print(f"   üìÅ Escritura: {self.output_workers} workers")
    
    def _category_worker(self, worker_id: int):
        """Worker de categorizaci√≥n ULTRA-OPTIMIZADO que distribuye art√≠culos SIN LOCKS"""
        while self.running:
            try:
                article_batch = self.article_queue.get(timeout=0.1)
                if article_batch is None:
                    break
                
                # OPTIMIZACI√ìN: Procesar todo el batch de una vez
                categorized_articles = defaultdict(list)
                
                for article in article_batch:
                    if not self.running:
                        break
                        
                    # Procesar art√≠culo usando ContentManager (SIN LOCKS)
                    result = self.content_manager.process_article(article)
                    
                    if result:
                        enhanced_article = {
                            **article,
                            'category': result['category'],
                            'subcategory': result['subcategory'],
                            'confidence': result['confidence'],
                            'conversations': result['conversations']
                        }
                        categorized_articles[result['category']].append(enhanced_article)
                    
                    self.stats['articles_processed'] += 1  # At√≥mico sin lock
                
                # Enviar art√≠culos categorizados a sus colas espec√≠ficas
                for category, articles in categorized_articles.items():
                    for article in articles:
                        try:
                            self.category_queues[category].put_nowait(article)
                            self.stats['articles_categorized'] += 1  # At√≥mico sin lock
                        except queue.Full:
                            # Si la cola est√° llena, usar categor√≠a general
                            try:
                                article['category'] = 'general'
                                self.category_queues['general'].put_nowait(article)
                            except queue.Full:
                                pass  # Skip si todas las colas est√°n llenas
                
                self.article_queue.task_done()
                
            except queue.Empty:
                if not self.running:
                    break
                continue
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error categor√≠a worker {worker_id}: {e}")
                if not self.running:
                    break
    
    def _conversation_worker(self, category: str, worker_id: int):
        """Worker especializado en generaci√≥n de conversaciones SIN LOCKS"""
        while self.running:
            try:
                article = self.category_queues[category].get(timeout=0.05)
                if article is None:
                    break
                
                if not self.running:
                    break
                
                # Usar conversaciones ya generadas por ContentManager
                conversations = article.get('conversations', [])
                
                # Si no hay conversaciones pre-generadas, usar ContentManager
                if not conversations:
                    result = self.content_manager.process_article(article)
                    conversations = result['conversations'] if result else []
                
                if conversations and self.running:
                    # A√±adir a buffer espec√≠fico del worker (SIN LOCKS EXPL√çCITOS)
                    self.category_buffers[category][worker_id].extend(conversations)
                    self.category_counters[category] += len(conversations)  # At√≥mico
                    
                    # Auto-flush si el buffer del worker est√° lleno
                    if len(self.category_buffers[category][worker_id]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                        self._flush_category_worker_buffer(category, worker_id)
                
                self.stats['conversations_generated'] += len(conversations)  # At√≥mico sin lock
                
                self.category_queues[category].task_done()
                
            except queue.Empty:
                if not self.running:
                    break
                continue
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error conversaci√≥n worker {category}-{worker_id}: {e}")
                if not self.running:
                    break
    
    def _conversation_worker_universal(self, worker_id: int):
        """Worker universal de conversaciones ULTRA-OPTIMIZADO (patr√≥n h√≠brido extractor)"""
        local_buffer = defaultdict(list)  # Buffer local para reducir contenci√≥n
        local_counter = 0
        
        while self.running:
            try:
                # Procesar desde todas las colas de categor√≠as de forma universal
                articles_processed = 0
                
                # OPTIMIZACI√ìN: Procesar m√∫ltiples art√≠culos por iteraci√≥n
                for category in self.get_categories():
                    try:
                        # Intentar obtener m√∫ltiples art√≠culos de una vez
                        batch_articles = []
                        for _ in range(min(10, self.category_queues[category].qsize())):  # Hasta 10 art√≠culos por batch
                            try:
                                article = self.category_queues[category].get_nowait()
                                if article is not None:
                                    batch_articles.append((category, article))
                                    self.category_queues[category].task_done()
                            except queue.Empty:
                                break
                        
                        # Procesar batch de art√≠culos
                        for cat, article in batch_articles:
                            if not self.running:
                                break
                                
                            # Usar conversaciones ya generadas por ContentManager
                            conversations = article.get('conversations', [])
                            
                            # Si no hay conversaciones pre-generadas, usar ContentManager
                            if not conversations:
                                result = self.content_manager.process_article(article)
                                conversations = result['conversations'] if result else []
                            
                            if conversations and self.running:
                                # A√±adir a buffer local para reducir contenci√≥n
                                for conv in conversations:
                                    conv['source_article'] = article.get('title', '')
                                    conv['worker_id'] = worker_id
                                
                                local_buffer[cat].extend(conversations)
                                local_counter += len(conversations)
                                articles_processed += 1
                                
                                # Flush local cuando alcance threshold
                                if len(local_buffer[cat]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                                    self.category_buffers[cat][worker_id].extend(local_buffer[cat])
                                    self.category_counters[cat] += len(local_buffer[cat])  # At√≥mico
                                    local_buffer[cat].clear()
                                    
                                    # Auto-flush si el buffer principal est√° lleno
                                    if len(self.category_buffers[cat][worker_id]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                                        self._flush_category_worker_buffer(cat, worker_id)
                            
                            self.stats['conversations_generated'] += len(conversations)  # At√≥mico sin lock
                        
                        if batch_articles:
                            break  # Pasar a siguiente worker si proces√≥ algo
                            
                    except Exception as e:
                        if self.running:
                            print(f"‚ö†Ô∏è Error batch processing {category}: {e}")
                        continue
                
                # Si no proces√≥ art√≠culos, flush buffers locales y pausa breve
                if articles_processed == 0:
                    # Flush buffers locales pendientes
                    for cat, buffer in local_buffer.items():
                        if buffer:
                            self.category_buffers[cat][worker_id].extend(buffer)
                            self.category_counters[cat] += len(buffer)
                            buffer.clear()
                    
                    if not self.running:
                        break
                    time.sleep(0.001)  # Pausa ultra-corta - 1ms para flujo continuo
                    
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error conversaci√≥n worker universal {worker_id}: {e}")
                if not self.running:
                    break
        
        # Flush final de buffers locales
        for cat, buffer in local_buffer.items():
            if buffer:
                self.category_buffers[cat][worker_id].extend(buffer)
                self.category_counters[cat] += len(buffer)
    
    def _output_worker_universal(self, worker_id: int):
        """Worker universal de escritura paralela SIN LOCKS (patr√≥n h√≠brido extractor optimizado)"""
        while self.running:
            try:
                # Verificar buffers de todas las categor√≠as para este worker
                for category in self.get_categories():
                    if len(self.category_buffers[category][worker_id]) >= self.adaptive_config['AUTO_FLUSH_THRESHOLD']:
                        self._flush_category_worker_buffer(category, worker_id)
                
                # Sleep ultra-corto para m√°xima responsividad con 450GB RAM
                time.sleep(0.01)
                
            except Exception as e:
                if self.running:
                    print(f"‚ö†Ô∏è Error output worker universal {worker_id}: {e}")
                if not self.running:
                    break
    
    def _flush_category_worker_buffer(self, category: str, worker_id: int):
        """Escribe buffer espec√≠fico de un worker a archivo JSONL SIN LOCKS"""
        if not self.category_buffers[category][worker_id]:
            return
        
        try:
            # Crear archivo espec√≠fico por categor√≠a y worker
            output_file = self.category_dirs[category] / f"conversaciones_{category}_{worker_id}_{self.file_counters[category]:04d}.jsonl"
            
            # Escribir conversaciones con buffering ULTRA-MASIVO para 450GB RAM
            with open(output_file, 'w', encoding='utf-8', buffering=64*1024*1024) as f:  # 64MB buffer
                for conversation in self.category_buffers[category][worker_id]:
                    # Crear registro de conversaci√≥n completo
                    conversation_record = {
                        'id': f"{category}_{worker_id}_{self.file_counters[category]}_{len(self.category_buffers[category][worker_id])}",
                        'category': category,
                        'subcategory': conversation.get('subcategory', 'general'),
                        'article_title': conversation.get('source_article', ''),
                        'conversation': [
                            {'role': 'user', 'content': conversation['question']},
                            {'role': 'assistant', 'content': conversation['answer']}
                        ],
                        'metadata': {
                            'source_article': conversation.get('source_article', ''),
                            'category': category,
                            'subcategory': conversation.get('subcategory', 'general'),
                            'conversation_type': conversation.get('conversation_type', 'general'),
                            'generation_date': datetime.now().isoformat(),
                            'worker_id': worker_id
                        }
                    }
                    f.write(json.dumps(conversation_record, ensure_ascii=False) + '\n')
            
            conversations_written = len(self.category_buffers[category][worker_id])
            conversations_written = len(self.category_buffers[category][worker_id])
            self.category_buffers[category][worker_id].clear()
            self.file_counters[category] += 1
            
            self.stats['files_written'] += 1  # At√≥mico sin lock
            
            print(f"üíæ {category}-W{worker_id}: {conversations_written:,} conversaciones ‚Üí {output_file.name}")
            
        except Exception as e:
            print(f"‚ùå Error flush {category}-{worker_id}: {e}")
    
    def load_hybrid_files_batch(self, batch_size: int = None) -> Iterator[List[Dict]]:
        """Carga art√≠culos de archivos h√≠bridos en batches - MODO STREAMING SIMPLIFICADO Y ROBUSTO"""
        if batch_size is None:
            batch_size = self.adaptive_config['BATCH_SIZE']
        
        total_articles_processed = 0
        
        print(f"üìö MODO STREAMING SIMPLIFICADO: {len(self.hybrid_files)} archivos, batch_size={batch_size:,}")
        
        # SIMPLIFICACI√ìN: Procesar archivos secuencialmente pero con batches grandes
        current_batch = []
        
        for file_idx, hybrid_file in enumerate(self.hybrid_files, 1):
            try:
                file_size_mb = hybrid_file.stat().st_size / 1024 / 1024
                print(f"üìñ Procesando archivo {file_idx}/{len(self.hybrid_files)}: {hybrid_file.name} ({file_size_mb:.1f}MB)")
                
                file_articles = 0
                with open(hybrid_file, 'r', encoding='utf-8', buffering=128*1024*1024) as f:  # 128MB buffer para 450GB RAM
                    for line_num, line in enumerate(f, 1):
                        if not line.strip():
                            continue
                        
                        try:
                            article = json.loads(line)
                            if (article.get('title') and 
                                article.get('content') and 
                                len(article.get('content', '')) > 100):
                                current_batch.append(article)
                                file_articles += 1
                                total_articles_processed += 1
                                
                                # Enviar batch cuando est√© lleno
                                if len(current_batch) >= batch_size:
                                    print(f"üì® Enviando batch: {len(current_batch):,} art√≠culos (total: {total_articles_processed:,})")
                                    yield current_batch
                                    current_batch = []
                                
                                # FLUJO CONTINUO: Enviar batch parcial cada 5000 art√≠culos para evitar esperas
                                elif len(current_batch) % 5000 == 0 and len(current_batch) > 0:
                                    print(f"üì§ Enviando batch parcial: {len(current_batch):,} art√≠culos (flujo continuo)")
                                    yield current_batch.copy()  # Enviar copia para flujo continuo
                                    # NO limpiar current_batch aqu√≠ para mantener el batch principal
                                    
                        except json.JSONDecodeError:
                            continue
                        except Exception:
                            continue
                        
                        # Progreso cada 20K l√≠neas por archivo
                        if line_num % 50000 == 0:
                            print(f"   ÔøΩ Procesadas {line_num:,} l√≠neas, {file_articles:,} art√≠culos v√°lidos")
                
                print(f"‚úÖ {hybrid_file.name}: {file_articles:,} art√≠culos v√°lidos procesados")
                
                # Gesti√≥n de memoria cada 2 archivos (m√°s frecuente para 450GB RAM)
                if file_idx % 2 == 0:  # Cada 2 archivos
                    gc.collect()
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error procesando {hybrid_file.name}: {e}")
                continue
        
        # Enviar √∫ltimo batch si tiene contenido
        if current_batch:
            print(f"üì® Enviando batch final: {len(current_batch):,} art√≠culos")
            yield current_batch
        
        print(f"‚úÖ STREAMING COMPLETADO: {total_articles_processed:,} art√≠culos de {len(self.hybrid_files)} archivos")
    
    def add_article_batch(self, articles: List[Dict]) -> bool:
        """A√±ade batch de art√≠culos al pipeline SIN LOCKS"""
        try:
            self.article_queue.put(articles, timeout=0.1)
            return True
        except queue.Full:
            return False
    
    def process_all_files_massive(self):
        """Procesa TODOS los archivos h√≠bridos con enfoque masivo paralelo SIN LOCKS"""
        if not self.hybrid_files:
            print(f"‚ùå No se encontraron archivos h√≠bridos para procesar")
            return False
        
        print(f"üöÄ INICIANDO PROCESAMIENTO MASIVO PARALELO SIN LOCKS")
        print(f"   üìÅ Archivos h√≠bridos: {len(self.hybrid_files)}")
        print(f"   üéØ Modo: PROCESAMIENTO COMPLETO (100% art√≠culos)")
        print(f"   üî¢ Conversaciones por art√≠culo: {MASSIVE_PARALLEL_CONFIG['CONVERSATIONS_PER_ARTICLE']}")
        print(f"   üöÄ Paralelizaci√≥n: SIN LOCKS para m√°ximo rendimiento")
        
        # Iniciar workers especializados
        self.start_workers()
        
        # Manejo de se√±ales ultra-agresivo (como en caroline_ultra_extractor_hybrid.py)
        def signal_handler(signum, frame):
            print(f"\n‚ö†Ô∏è Se√±al {signum} recibida, terminando INMEDIATAMENTE...")
            self.running = False
            self.stop_workers()
            
            # Esperar m√°ximo 5 segundos para limpieza
            cleanup_start = time.time()
            while time.time() - cleanup_start < 5:
                import threading
                if threading.active_count() <= 5:
                    break
                time.sleep(0.5)
            
            print(f"üö® TERMINACI√ìN FORZADA POR SE√ëAL")
            os._exit(1)  # Salida inmediata
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # Procesar archivos en batches
        start_time = time.time()
        total_batches = 0
        
        print(f"üîÑ Iniciando iteraci√≥n de batches...")
        
        try:
            for batch in self.load_hybrid_files_batch(self.adaptive_config['BATCH_SIZE']):
                print(f"üì¶ Recibido batch #{total_batches + 1} con {len(batch):,} art√≠culos")
                
                # Enviar batch a workers con configuraci√≥n anti-bloqueo optimizada
                attempts = 0
                max_retries = self.adaptive_config.get('MAX_QUEUE_RETRIES', 30)
                queue_timeout = self.adaptive_config.get('QUEUE_TIMEOUT', 5.0)
                
                while not self.add_article_batch(batch):
                    attempts += 1
                    if attempts > max_retries:  # Usar configuraci√≥n adaptativa
                        print(f"‚ö†Ô∏è Cola llena despu√©s de {attempts} intentos, continuando...")
                        break
                    time.sleep(queue_timeout / max_retries)  # Sleep adaptativo
                
                total_batches += 1
                print(f"‚úÖ Batch #{total_batches} enviado a workers")
                
                # Progreso cada 10 batches (m√°s frecuente para diagn√≥stico de flujo continuo)
                if total_batches % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = self.stats['articles_processed'] / elapsed if elapsed > 0 else 0
                    
                    print(f"üìä Progreso Masivo: {total_batches:,} batches | "
                          f"{self.stats['articles_processed']:,} procesados | "
                          f"{self.stats['articles_categorized']:,} categorizados | "
                          f"{self.stats['conversations_generated']:,} conversaciones | "
                          f"{rate:.0f} a/s")
                    
                    # Diagn√≥stico de colas
                    total_queue_size = sum(q.qsize() for q in self.category_queues.values())
                    article_queue_size = self.article_queue.qsize()
                    print(f"üîç Estado colas: Art√≠culos({article_queue_size}) Categor√≠as({total_queue_size})")
            
            print(f"üéâ Todos los batches procesados: {total_batches:,} batches totales")
            
        except Exception as e:
            print(f"‚ùå Error durante procesamiento de batches: {e}")
            import traceback
            traceback.print_exc()
        
        return True
    
    def _flush_all_category_buffers(self):
        """Flushea todos los buffers de todas las categor√≠as y workers"""
        print(f"üîÑ Finalizando escritura de todos los buffers...")
        
        for category in self.get_categories():
            for worker_id in self.category_buffers[category].keys():
                if self.category_buffers[category][worker_id]:
                    self._flush_category_worker_buffer(category, worker_id)
    
    def stop_workers(self):
        """Detiene workers de forma ULTRA-AGRESIVA sin esperas (inspirado en caroline_ultra_extractor_hybrid.py)"""
        print(f"üõë DETENCI√ìN ULTRA-AGRESIVA DE WORKERS...")
        self.running = False
        
        # 1. Enviar se√±ales de parada masivas a todas las colas (sin timeout)
        print("üì§ Flooding colas con se√±ales de parada...")
        for _ in range(self.category_workers + self.conversation_workers + self.output_workers + 10):
            try:
                self.article_queue.put_nowait(None)
                for q in self.category_queues.values():
                    q.put_nowait(None)
            except:
                pass  # Ignorar errores de cola llena
        
        # 2. Vaciar todas las colas agresivamente
        print("üóëÔ∏è Vaciando colas agresivamente...")
        
        def drain_queue(q, name):
            count = 0
            try:
                while not q.empty():
                    q.get_nowait()
                    count += 1
                    if count > 5000:  # L√≠mite de seguridad
                        break
            except:
                pass
            return count
        
        article_drained = drain_queue(self.article_queue, "Article")
        total_cat_drained = 0
        for category, q in self.category_queues.items():
            cat_drained = drain_queue(q, f"Cat-{category}")
            total_cat_drained += cat_drained
        
        print(f"üóëÔ∏è Vaciadas: Article({article_drained}), Categories({total_cat_drained})")
        
        # 3. Shutdown inmediato de pools (sin esperar)
        print("üõë Shutdown inmediato de pools...")
        
        pools = [
            ("Category", self.category_pool),
            ("Conversation", self.conversation_pool), 
            ("Output", self.output_pool)
        ]
        
        for name, pool in pools:
            try:
                pool.shutdown(wait=False)  # NO ESPERAR
                print(f"‚úÖ {name} pool ‚Üí shutdown iniciado")
            except Exception as e:
                print(f"‚ö†Ô∏è {name} pool error: {e}")
        
        # 4. Verificaci√≥n r√°pida de threads con timeout forzado
        import threading
        start_wait = time.time()
        FORCE_EXIT_TIMEOUT = 10  # 10 segundos m√°ximo
        
        while time.time() - start_wait < FORCE_EXIT_TIMEOUT:
            active_count = threading.active_count()
            
            if active_count <= 10:  # Solo threads del sistema
                print(f"‚úÖ Workers terminados limpiamente ({active_count} threads)")
                break
            
            time.sleep(0.5)
        
        elapsed = time.time() - start_wait
        if elapsed >= FORCE_EXIT_TIMEOUT:
            print(f"üö® TIMEOUT ALCANZADO - Forzando terminaci√≥n despu√©s de {elapsed:.1f}s")
            
            # Matar pools agresivamente
            for name, pool in pools:
                try:
                    pool._threads.clear()  # Forzar limpieza de threads
                except:
                    pass
        
        final_active = threading.active_count()
        print(f"üßµ Threads finales: {final_active}")
        
        # 5. Forzar garbage collection
        import gc
        gc.collect()
        
        print(f"‚úÖ DETENCI√ìN COMPLETADA en {elapsed:.1f}s")
        
    def discover_hybrid_files(self) -> List[Path]:
        """Descubre todos los archivos h√≠bridos disponibles"""
        return self._discover_hybrid_files()
        
    def process_all_files(self):
        """Procesa todos los archivos h√≠bridos con paralelizaci√≥n masiva"""
        if not self.hybrid_files:
            print("‚ùå No se encontraron archivos h√≠bridos")
            return False
            
        print(f"\nüöÄ INICIANDO PROCESAMIENTO MASIVO SIN LOCKS")
        print(f"   üìÅ Archivos a procesar: {len(self.hybrid_files)}")
        print(f"   üéØ Categorizaci√≥n inteligente: ACTIVADA")
        print(f"   üìù Conversaciones de entrenamiento: ACTIVADAS")
        print(f"   üöÄ Paralelizaci√≥n: MASIVA SIN LOCKS")
        
        # Procesar con sistema masivo paralelo
        success = self.process_all_files_massive()
        
        if success:
            # Finalizar procesamiento
            self.finalize_processing()
        
        return success
        
    def process_hybrid_file(self, file_path: Path):
        """Procesa un archivo h√≠brido individual"""
        articles_in_file = 0
        conversations_in_file = 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                    
                try:
                    article = json.loads(line)
                    
                    # Procesar art√≠culo
                    result = self.process_single_article(article)
                    
                    if result:
                        articles_in_file += 1
                        conversations_in_file += len(result['conversations'])
                        
                        # Guardar resultado
                        self.save_article_result(result)
                        
                except json.JSONDecodeError as e:
                    if line_num % 10000 == 0:  # Log error cada 10K l√≠neas
                        print(f"‚ö†Ô∏è Error JSON l√≠nea {line_num}: {e}")
                    continue
                except Exception as e:
                    print(f"‚ö†Ô∏è Error procesando art√≠culo l√≠nea {line_num}: {e}")
                    continue
                    
                # Progreso cada 1000 art√≠culos
                if articles_in_file % 1000 == 0:
                    print(f"   üìä Progreso: {articles_in_file:,} art√≠culos, {conversations_in_file:,} conversaciones")
                    
        print(f"   ‚úÖ Completado: {articles_in_file:,} art√≠culos, {conversations_in_file:,} conversaciones")
        
        # Actualizar estad√≠sticas globales
        self.stats['files_processed'] += 1
        self.stats['articles_processed'] += articles_in_file
        self.stats['conversations_generated'] += conversations_in_file
            
    def process_single_article(self, article: Dict) -> Optional[Dict]:
        """Procesa un art√≠culo individual usando el ContentManager externo"""
        title = article.get('title', '').strip()
        content = article.get('content', '').strip()
        
        if not title or not content:
            return None
            
        # Procesar art√≠culo usando el ContentManager externo
        result = self.content_manager.process_article(article)
        
        if not result:
            return None
            
        # Reestructurar para mantener compatibilidad con el formato existente
        formatted_result = {
            'id': f"{result['category']}_{hashlib.md5(title.encode()).hexdigest()[:8]}",
            'title': result['title'],
            'content': content,
            'category': result['category'],
            'subcategory': result['subcategory'],
            'categorization_confidence': result['confidence'],
            'conversations': result['conversations'],
            'metadata': {
                'source': 'content_manager',
                'processed_at': result['metadata']['processed_at'],
                'content_length': result['metadata']['content_length'],
                'conversations_count': result['metadata']['conversations_count'],
                'original_article': article
            }
        }
        
        # Actualizar contadores (sin lock para mejor rendimiento)
        self.category_counters[result['category']] += 1
        self.stats['articles_categorized'] += 1
        self.stats['conversations_generated'] += len(result['conversations'])
            
        # Registrar problemas de confianza baja
        if result['confidence'] < 0.5:
            self.problematic_categorizations.append({
                'title': title,
                'category': result['category'],
                'confidence': result['confidence'],
                'reason': 'low_confidence'
            })
            
        return formatted_result
        
    def save_article_result(self, result: Dict):
        """Guarda el resultado de un art√≠culo en la categor√≠a correspondiente"""
        category = result['category']
        
        # Crear directorio de categor√≠a si no existe
        category_dir = self.categories_dir / category
        category_dir.mkdir(exist_ok=True)
        
        # Determinar archivo de salida
        file_counter = self.file_counters[category]
        conversations_per_file = 10000  # 10K conversaciones por archivo
        
        if file_counter % conversations_per_file == 0:
            file_num = (file_counter // conversations_per_file) + 1
            current_file = category_dir / f"conversaciones_{category}_{file_num:04d}.jsonl"
        else:
            file_num = (file_counter // conversations_per_file) + 1
            current_file = category_dir / f"conversaciones_{category}_{file_num:04d}.jsonl"
            
        # Escribir conversaciones individuales
        with open(current_file, 'a', encoding='utf-8') as f:
            for conv in result['conversations']:
                conversation_record = {
                    'id': f"{result['id']}_{len(result['conversations'])}",
                    'category': category,
                    'subcategory': result['subcategory'],
                    'article_title': result['title'],
                    'conversation': [
                        {'role': 'user', 'content': conv['question']},
                        {'role': 'assistant', 'content': conv['answer']}
                    ],
                    'metadata': {
                        'source_article': result['title'],
                        'category': category,
                        'subcategory': result['subcategory'],
                        'conversation_type': conv['conversation_type'],
                        'content_length': len(result['content']),
                        'categorization_confidence': result['categorization_confidence'],
                        'generation_date': datetime.now().isoformat()
                    }
                }
                
                f.write(json.dumps(conversation_record, ensure_ascii=False) + '\n')
                
        self.file_counters[category] += len(result['conversations'])
        
    def finalize_processing(self):
        """Finaliza el procesamiento con modo inteligente ultra-optimizado (inspirado en caroline_ultra_extractor_hybrid.py)"""
        print(f"\nüéâ INICIANDO FINALIZACI√ìN ULTRA-OPTIMIZADA")
        
        # Usar finalizaci√≥n inteligente ultra-agresiva (incluye stop_workers internamente)
        self._finalize_processing_intelligent()
        
        # Solo generar archivos finales si la finalizaci√≥n fue exitosa
        print(f"üìù Generando archivos finales...")
        
        try:
            # Generar conscious.txt
            self.generate_conscious_file()
            
            # Generar metadatos por categor√≠a
            self.generate_category_metadata()
            
            # Generar estad√≠sticas
            self.generate_final_statistics()
            
            # Mostrar resumen final
            self.show_final_summary()
            
            print(f"‚úÖ Archivos finales generados exitosamente")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error generando archivos finales: {e}")
            print(f"üìä Los archivos de conversaciones principales ya est√°n guardados")
        
        print(f"üéØ FINALIZACI√ìN COMPLETADA")
        
    def generate_conscious_file(self):
        """Genera el archivo conscious.txt con la base fundamental"""
        conscious_file = self.output_dir / "conscious.txt"
        
        with open(conscious_file, 'w', encoding='utf-8') as f:
            f.write("# CAROLINE CONSCIOUS KNOWLEDGE - BASE FUNDAMENTAL CORREGIDA\n")
            f.write("# =========================================================\n")
            f.write(f"# Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# Art√≠culos procesados: {self.stats['articles_processed']:,}\n")
            f.write(f"# Conversaciones generadas: {self.stats['conversations_generated']:,}\n")
            f.write(f"# Categor√≠as identificadas: {len(self.get_categories())}\n")
            f.write("# Sistema de categorizaci√≥n inteligente: ACTIVADO\n")
            f.write("# Correcci√≥n de problemas de categorizaci√≥n: APLICADA\n")
            f.write("# =========================================================\n\n")
            
            f.write("## SISTEMA CONSCIENTE MEJORADO\n")
            f.write("soy caroline, modelo de ia especializado en wikipedia espa√±ola\n")
            f.write("he procesado 1.5 millones de art√≠culos con categorizaci√≥n corregida\n")
            f.write("cada art√≠culo ha sido analizado con patrones inteligentes\n")
            f.write("las categor√≠as err√≥neas han sido corregidas autom√°ticamente\n")
            f.write("genero conversaciones espec√≠ficas por categor√≠a y subcategor√≠a\n")
            f.write("mi conocimiento est√° estructurado en carpetas organizadas\n\n")
            
            f.write("## CATEGOR√çAS PROCESADAS CON CORRECCI√ìN\n")
            total_conversations = sum(self.category_counters.values())
            
            for category in sorted(self.get_categories()):
                count = self.category_counters[category]
                percentage = (count / total_conversations * 100) if total_conversations > 0 else 0
                
                f.write(f"### {category.upper()}\n")
                f.write(f"- Art√≠culos: {count:,} ({percentage:.1f}%)\n")
                f.write(f"- Directorio: consciencia/categorias/{category}/\n")
                f.write(f"- Archivos: conversaciones_{category}_NNNN.jsonl\n")
                
                # Mostrar subcategor√≠as si existen
                category_dir = self.categories_dir / category
                if category_dir.exists():
                    jsonl_files = list(category_dir.glob("*.jsonl"))
                    f.write(f"- Archivos generados: {len(jsonl_files)}\n")
                    
                f.write("\n")
                
            f.write("## MEJORAS APLICADAS\n")
            f.write("1. Correcci√≥n de categorizaci√≥n musical (ej: Catch a Fire ‚Üí arte)\n")
            f.write("2. Identificaci√≥n precisa de biograf√≠as vs. otros contenidos\n")
            f.write("3. Separaci√≥n clara entre ciencias y otras disciplinas\n")
            f.write("4. Subcategorizaci√≥n espec√≠fica por tipo de contenido\n")
            f.write("5. Conversaciones contextuales por √°rea de conocimiento\n\n")
            
            f.write("## INSTRUCCIONES DE USO\n")
            f.write("- Cada categor√≠a tiene su propio directorio\n")
            f.write("- Las conversaciones est√°n en formato JSONL listo para entrenamiento\n")
            f.write("- Metadatos incluyen confianza de categorizaci√≥n\n")
            f.write("- Subcategor√≠as permiten especializaci√≥n fina\n")
            f.write("- Sistema autocorrige errores de categorizaci√≥n comunes\n")
            
        print(f"üìù Archivo conscious.txt generado: {conscious_file}")
        
    def generate_category_metadata(self):
        """Genera metadatos por cada categor√≠a"""
        for category in self.get_categories():
            if self.category_counters[category] > 0:  # Solo categor√≠as con contenido
                category_dir = self.categories_dir / category
                metadata_file = category_dir / f"metadata_{category}.json"
                
                # Contar archivos
                jsonl_files = list(category_dir.glob("*.jsonl"))
                total_conversations = self.category_counters[category]
                
                metadata = {
                    'category': category,
                    'total_articles': total_conversations,
                    'total_files': len(jsonl_files),
                    'files_generated': [f.name for f in jsonl_files],
                    'generation_date': datetime.now().isoformat(),
                    'description': f"Conversaciones de entrenamiento para la categor√≠a {category}",
                    'conversation_types': ['definicion', 'temporal', 'espacial', 'explicacion', 'contextualizacion'],
                    'processing_mode': 'massive_parallel_no_locks'
                }
                
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
                print(f"üìä Metadatos generados: {metadata_file}")
            
    def generate_final_statistics(self):
        """Genera estad√≠sticas finales del procesamiento"""
        
        # Distribuci√≥n de categor√≠as
        distribution_file = self.stats_dir / "distribucion_categorias.json"
        total_conversations = sum(self.category_counters.values())
        
        distribution = {
            'total_conversations': total_conversations,
            'total_categories': len(self.category_counters),
            'distribution': {
                category: {
                    'count': count,
                    'percentage': (count / total_conversations * 100) if total_conversations > 0 else 0
                }
                for category, count in sorted(self.category_counters.items(), key=lambda x: x[1], reverse=True)
            },
            'generation_date': datetime.now().isoformat()
        }
        
        with open(distribution_file, 'w', encoding='utf-8') as f:
            json.dump(distribution, f, ensure_ascii=False, indent=2)
            
        # Problemas de categorizaci√≥n
        problems_file = self.stats_dir / "problemas_categorizacion.json"
        
        problems = {
            'total_problematic': len(self.problematic_categorizations),
            'low_confidence_threshold': 0.5,
            'problems': self.problematic_categorizations[:100],  # Top 100 problemas
            'analysis_date': datetime.now().isoformat()
        }
        
        with open(problems_file, 'w', encoding='utf-8') as f:
            json.dump(problems, f, ensure_ascii=False, indent=2)
            
        # Resumen de procesamiento
        summary_file = self.stats_dir / "resumen_procesamiento.json"
        
        processing_time = time.time() - self.stats['start_time']
        
        summary = {
            'processing_summary': {
                'total_articles': self.stats['articles_processed'],
                'total_conversations': self.stats['conversations_generated'],
                'total_categories': len([c for c in self.get_categories() if self.category_counters[c] > 0]),
                'files_written': self.stats['files_written'],
                'processing_errors': self.stats['processing_errors'],
                'processing_time_seconds': processing_time,
                'processing_time_hours': processing_time / 3600,
                'articles_per_second': self.stats['articles_processed'] / processing_time if processing_time > 0 else 0,
                'processing_mode': 'massive_parallel_no_locks'
            },
            'categories_created': [c for c in self.get_categories() if self.category_counters[c] > 0],
            'output_structure': {
                'base_directory': str(self.output_dir),
                'categories_directory': str(self.categories_dir),
                'statistics_directory': str(self.stats_dir),
                'conscious_file': str(self.output_dir / "conscious.txt")
            },
            'generation_date': datetime.now().isoformat()
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        print(f"üìà Estad√≠sticas generadas en: {self.stats_dir}")
        
    def show_final_summary(self):
        """Muestra resumen final del procesamiento"""
        processing_time = time.time() - self.stats['start_time']
        
        print(f"\nüéâ PROCESAMIENTO MASIVO FINALIZADO")
        print(f"   ‚è±Ô∏è Tiempo total: {processing_time/3600:.1f} horas")
        print(f"   üìö Art√≠culos procesados: {self.stats['articles_processed']:,}")
        print(f"   üí¨ Conversaciones generadas: {self.stats['conversations_generated']:,}")
        print(f"   üóÇÔ∏è Categor√≠as creadas: {len([c for c in self.get_categories() if self.category_counters[c] > 0])}")
        print(f"   üìÅ Archivos generados: {self.stats['files_written']:,}")
        print(f"   ‚ö†Ô∏è Errores de procesamiento: {self.stats['processing_errors']}")
        print(f"   üöÄ Velocidad: {self.stats['articles_processed']/(processing_time/3600):.0f} art√≠culos/hora")
        print(f"   üî• Modo: PARALELIZACI√ìN MASIVA SIN LOCKS")
        
        print(f"\nüìÇ ESTRUCTURA GENERADA:")
        print(f"   üìÅ {self.output_dir}/")
        print(f"   ‚îú‚îÄ‚îÄ conscious.txt (base fundamental)")
        print(f"   ‚îú‚îÄ‚îÄ categorias/ (categor√≠as con contenido)")
        
        for category in sorted(self.get_categories()):
            count = self.category_counters[category]
            if count > 0:
                files = len(list((self.categories_dir / category).glob("*.jsonl"))) if (self.categories_dir / category).exists() else 0
                print(f"   ‚îÇ   ‚îú‚îÄ‚îÄ {category}/ ({count:,} conversaciones, {files} archivos)")
            
        print(f"   ‚îî‚îÄ‚îÄ estadisticas/ (an√°lisis y metadatos)")
        
        print(f"\n‚úÖ Dataset de entrenamiento masivo listo en: {self.output_dir}")
    
    def _finalize_processing_intelligent(self):
        """Finalizaci√≥n autom√°tica inteligente ULTRA-AGRESIVA (basada exactamente en caroline_ultra_extractor_hybrid.py)"""
        print(f"üîÑ INICIANDO FINALIZACI√ìN ULTRA-AGRESIVA SIN LOCKS...")
        
        # 1. Detener la alimentaci√≥n de nuevos trabajos INMEDIATAMENTE
        self.running = False
        
        # 2. Timeout MUCHO m√°s agresivo (m√°ximo 10 segundos, como en caroline_ultra_extractor_hybrid.py)
        max_wait = 10  # Solo 10 segundos m√°ximo (ultra-agresivo)
        print(f"‚è≥ Esperando finalizaci√≥n ULTRA-AGRESIVA (m√°ximo {max_wait}s)...")
        
        start_wait = time.time()
        last_conversations = sum(self.category_counters.values())
        stable_count = 0
        
        while time.time() - start_wait < max_wait:
            time.sleep(0.5)  # Check cada 0.5 segundos (MUY agresivo)
            current_conversations = sum(self.category_counters.values())
            
            # Estado de colas
            total_queue_size = sum(q.qsize() for q in self.category_queues.values())
            article_queue_size = self.article_queue.qsize()
            
            elapsed_wait = time.time() - start_wait
            print(f"üìä T+{elapsed_wait:.1f}s: {current_conversations:,} conversaciones | Colas: Art({article_queue_size}) Cat({total_queue_size})")
            
            # Verificar estabilidad
            if current_conversations == last_conversations:
                stable_count += 1
            else:
                stable_count = 0
                last_conversations = current_conversations
            
            # Condiciones de finalizaci√≥n ULTRA-AGRESIVAS (inspiradas en caroline_ultra_extractor_hybrid.py)
            if article_queue_size == 0 and total_queue_size == 0 and stable_count >= 2:
                print(f"‚úÖ Procesamiento completado (colas vac√≠as)")
                break
            elif stable_count >= 6:  # Solo 3 segundos sin cambios (6 * 0.5s)
                print(f"‚úÖ Procesamiento estabilizado")
                break
        
        # 3. TIMEOUT FORZADO como en caroline_ultra_extractor_hybrid.py
        elapsed_total = time.time() - start_wait
        if elapsed_total >= max_wait:
            print(f"ÔøΩ TIMEOUT DE FINALIZACI√ìN ({elapsed_total:.1f}s) - Continuando con terminaci√≥n")
        
        # 4. Flush final de todos los buffers
        self._flush_all_buffers_final()
        
        # 5. Llamar a stop_workers() directamente (como en caroline_ultra_extractor_hybrid.py)
        print(f"üõë Llamando a stop_workers() ultra-agresivo...")
        self.stop_workers()
        
        print(f"üî• FINALIZACI√ìN ULTRA-AGRESIVA COMPLETADA en {elapsed_total:.1f}s")
    
    def _flush_all_buffers_final(self):
        """Flush final de todos los buffers sin locks"""
        print(f"üíæ FLUSH FINAL DE TODOS LOS BUFFERS...")
        
        total_flushed = 0
        for category in self.get_categories():
            if self.category_counters[category] > 0:
                # Flush todos los workers de esta categor√≠a
                for worker_id in self.category_buffers[category]:
                    buffer = self.category_buffers[category][worker_id]
                    if buffer:
                        try:
                            self._flush_category_worker_buffer(category, worker_id)
                            total_flushed += len(buffer)
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error flush final {category}-{worker_id}: {e}")
        
        print(f"‚úÖ Flush final completado: {total_flushed:,} conversaciones")


def main():
    """Funci√≥n principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Full Dataset For Training - Procesador Completo")
    
    # Accept both --input/--output and --input-dir/--output-dir for flexibility
    parser.add_argument("--input", "--input-dir", "-i", type=str, 
                       default="./data_ultra_hybrid",
                       help="Directorio de entrada con archivos h√≠bridos")
    parser.add_argument("--output", "--output-dir", "-o", type=str, 
                       default="consciencia",
                       help="Directorio de salida")
    
    args = parser.parse_args()
    
    print("üß† FULL DATASET FOR TRAINING - Procesador Completo")
    print("=" * 60)
    
    # Configurar directorios desde argumentos CLI
    input_dir = args.input
    output_dir = args.output
    
    print(f"üìÇ Input: {input_dir}")
    print(f"üìÅ Output: {output_dir}")
    
    # Verificar input
    if not Path(input_dir).exists():
        print(f"‚ùå Directorio de entrada no encontrado: {input_dir}")
        print("üí° Aseg√∫rate de que los archivos h√≠bridos est√©n disponibles")
        return 1
        
    # Crear procesador masivo
    processor = MassiveParallelDatasetProcessor(input_dir, output_dir)
    
    # Procesar todo
    success = processor.process_all_files()
    
    if success:
        print("\nüéâ PROCESAMIENTO EXITOSO")
        print("üìã El dataset de entrenamiento est√° listo para usar")
        print("üß† Base de conocimiento consciente generada")
        return 0
    else:
        print("\n‚ùå PROCESAMIENTO FALLIDO")
        return 1


if __name__ == "__main__":
    sys.exit(main())
