#!/usr/bin/env python3
"""
üöÄ Caroline Ultra Extractor - VERSI√ìN H√çBRIDA ULTRA-OPTIMIZADA
Combina SAX parser confiable con optimizaciones masivas de rendimiento
"""

import xml.sax
import re
import time
import os
import gc
import psutil
import hashlib
import sys
import argparse
import json
import threading
import queue
import mmap
import io
from datetime import datetime
from pathlib import Path
from typing import List, Set, Dict, Optional, Tuple, Iterator
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal

# Patrones precompilados una sola vez (optimizaci√≥n cr√≠tica)
PRECOMPILED_PATTERNS = {
    'cleanup': re.compile(r'\{\{[^}]*\}\}|<ref[^>]*>.*?</ref>|<[^>]*>', re.DOTALL),
    'links': re.compile(r'\[\[([^|\]]*\|)?([^\]]*)\]\]'),
    'whitespace': re.compile(r'\s+'),
    'spanish': re.compile(r'[a-z√°√©√≠√≥√∫√±√º√ß]', re.IGNORECASE)
}

# Configuraci√≥n ultra-optimizada para GH200 con 500GB RAM (mejorada con patterns lockless)
ULTRA_CONFIG = {
    'MAX_WORKERS': min(256, os.cpu_count() * 4),  # Incrementado para m√°ximo rendimiento
    'BATCH_SIZE': 100000,  # Batches ultra-masivos para aprovechar RAM
    'QUEUE_SIZE': 1000,  # Colas ultra-masivas para evitar bloqueos
    'MEMORY_BUFFER_GB': 300,  # 300GB de buffer para GH200
    'TARGET_SPEED': 100000,  # 100K p√°ginas/segundo objetivo ultra-agresivo
    'PROCESSING_THREADS': 3,  # Threads especializados por tipo
    'AUTO_FLUSH_THRESHOLD': 200000,  # Flush cada 200K art√≠culos (ultra-masivo)
    'TURBO_MODE': True,
    'ULTRA_AGGRESSIVE_MODE': True,  # Modo ultra-agresivo sin esperas
    'LOCKLESS_STATS': True,  # Estad√≠sticas at√≥micas sin locks
    'STREAMING_BUFFERS': True,  # Buffers en modo streaming continuo
}

class UltraOptimizedProcessor:
    """Procesador ultra-optimizado con specializaci√≥n de workers"""
    
    def __init__(self):
        self.num_workers = ULTRA_CONFIG['MAX_WORKERS']
        self.batch_size = ULTRA_CONFIG['BATCH_SIZE']
        
        # Pools especializados para diferentes etapas
        self.extraction_pool = ThreadPoolExecutor(max_workers=self.num_workers // 3, thread_name_prefix="extract")
        self.processing_pool = ThreadPoolExecutor(max_workers=self.num_workers // 3, thread_name_prefix="process")
        self.output_pool = ThreadPoolExecutor(max_workers=self.num_workers // 3, thread_name_prefix="output")
        
        # Colas de trabajo ultra-eficientes
        self.raw_batch_queue = queue.Queue(maxsize=ULTRA_CONFIG['QUEUE_SIZE'])
        self.processed_queue = queue.Queue(maxsize=ULTRA_CONFIG['QUEUE_SIZE'])
        self.output_queue = queue.Queue(maxsize=ULTRA_CONFIG['QUEUE_SIZE'])
        
        # Estado y estad√≠sticas
        self.running = True
        self.stats = {
            'batches_sent': 0,
            'batches_processed': 0,
            'articles_processed': 0,
            'batches_written': 0
        }
        
        # Output management (mejorado con counter at√≥mico lockless)
        self.output_dir = Path("data_ultra_hybrid")
        self.output_dir.mkdir(exist_ok=True)
        self.file_counter = 0  # Counter at√≥mico simple (sin lock expl√≠cito)
        
        # Per-worker buffers lockless (inspirado en caroline_ultra_billion_parameters_500m)
        self.worker_buffers = defaultdict(list)  # Buffers por worker ID sin locks
        self.worker_file_counters = defaultdict(int)  # Contadores por worker
        
        print(f"üöÄ ULTRA-OPTIMIZED PROCESSOR:")
        print(f"   üîÑ Workers: Extract({self.num_workers//3}), Process({self.num_workers//3}), Output({self.num_workers//3})")
        print(f"   üì¶ Batch size: {self.batch_size:,}")
        print(f"   üíæ Memory buffer: {ULTRA_CONFIG['MEMORY_BUFFER_GB']}GB")
    
    def start_workers(self):
        """Inicia todos los pools de workers especializados"""
        print(f"üöÄ Iniciando workers especializados...")
        
        # Workers de extracci√≥n (reciben batches del SAX parser)
        for i in range(self.num_workers // 3):
            self.extraction_pool.submit(self._extraction_worker, i)
        
        # Workers de procesamiento (limpian y validan art√≠culos)
        for i in range(self.num_workers // 3):
            self.processing_pool.submit(self._processing_worker, i)
        
        # Workers de salida (escriben a disco)
        for i in range(self.num_workers // 3):
            self.output_pool.submit(self._output_worker, i)
        
        print(f"‚úÖ {self.num_workers} workers especializados activos")
    
    def _extraction_worker(self, worker_id: int):
        """Worker especializado en extracci√≥n r√°pida de datos b√°sicos"""
        while self.running:
            try:
                raw_batch = self.raw_batch_queue.get(timeout=1)
                if raw_batch is None:
                    break
                
                # Extracci√≥n ultra-r√°pida (solo filtros b√°sicos)
                extracted = []
                for title, text in raw_batch:
                    # Filtros ultra-r√°pidos sin regex
                    if (len(text) > 200 and 
                        ':' not in title and 
                        'wikipedia:' not in title.lower() and
                        'plantilla:' not in title.lower()):
                        extracted.append((title, text))
                
                if extracted:
                    self.processed_queue.put(extracted)
                    self.stats['batches_sent'] += 1
                
                self.raw_batch_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error extractor {worker_id}: {e}")
    
    def _processing_worker(self, worker_id: int):
        """Worker especializado en procesamiento intensivo con regex"""
        # Patrones locales para thread-safety
        patterns = PRECOMPILED_PATTERNS
        
        while self.running:
            try:
                batch = self.processed_queue.get(timeout=1)
                if batch is None:
                    break
                
                # Procesamiento intensivo ultra-optimizado
                processed_articles = []
                
                for title, text in batch:
                    try:
                        # Limpieza ultra-r√°pida en un solo paso
                        cleaned = patterns['cleanup'].sub('', text)
                        cleaned = patterns['links'].sub(r'\2', cleaned)
                        cleaned = patterns['whitespace'].sub(' ', cleaned).strip()
                        
                        if len(cleaned) < 100:
                            continue
                        
                        # Verificaci√≥n de idioma ultra-r√°pida
                        sample = cleaned[:400]
                        spanish_count = patterns['spanish'].findall(sample)
                        if len(spanish_count) / len(sample) < 0.25:
                            continue
                        
                        # Crear art√≠culo optimizado
                        article = {
                            'title': title.strip(),
                            'content': cleaned,
                            'length': len(cleaned),
                            'worker_id': worker_id,
                            'hash': hashlib.sha256(f"{title}{cleaned[:30]}".encode()).hexdigest()[:8]
                        }
                        
                        processed_articles.append(article)
                        
                    except Exception:
                        continue  # Skip art√≠culos problem√°ticos sin logging
                
                if processed_articles:
                    self.output_queue.put(processed_articles)
                    self.stats['articles_processed'] += len(processed_articles)
                    self.stats['batches_processed'] += 1
                
                self.processed_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error processor {worker_id}: {e}")
    
    def _output_worker(self, worker_id: int):
        """Worker especializado en escritura ultra-r√°pida a disco con timeout estricto"""
        write_buffer = []
        last_flush_time = time.time()
        start_time = time.time()  # Para timeout absoluto
        MAX_WORKER_TIME = 3600  # 1 hora m√°ximo por worker
        
        while self.running and (time.time() - start_time < MAX_WORKER_TIME):
            try:
                articles = self.output_queue.get(timeout=0.5)  # Timeout m√°s corto
                if articles is None:
                    print(f"üíæ Worker-{worker_id}: Se√±al de parada recibida")
                    break
                
                write_buffer.extend(articles)
                
                # Escribir cuando el buffer est√© lleno O haya pasado tiempo
                current_time = time.time()
                should_flush = (len(write_buffer) >= 200000 or  # 200K art√≠culos por archivo (ultra-masivo)
                               (write_buffer and current_time - last_flush_time > 15))  # O cada 15 segundos (m√°s agresivo)
                
                if should_flush:
                    self._write_buffer_ultra_fast(write_buffer, worker_id)
                    write_buffer.clear()
                    last_flush_time = current_time
                
                self.output_queue.task_done()
                
            except queue.Empty:
                # Escribir buffer pendiente en timeout si hay datos
                current_time = time.time()
                if write_buffer and (current_time - last_flush_time > 10):  # Flush cada 10s en timeout
                    self._write_buffer_ultra_fast(write_buffer, worker_id)
                    write_buffer.clear()
                    last_flush_time = current_time
                
                # Verificar si debemos terminar por falta de trabajo
                if not self.running:
                    break
                    
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error output {worker_id}: {e}")
                # En caso de error, intentar escribir buffer y salir
                if write_buffer:
                    try:
                        self._write_buffer_ultra_fast(write_buffer, worker_id)
                    except:
                        pass
                break
        
        # Escribir buffer final SIEMPRE
        if write_buffer:
            print(f"üíæ Worker-{worker_id}: Escribiendo buffer final ({len(write_buffer)} art√≠culos)")
            try:
                self._write_buffer_ultra_fast(write_buffer, worker_id)
            except Exception as e:
                print(f"‚ùå Worker-{worker_id}: Error en buffer final: {e}")
        
        elapsed = time.time() - start_time
        print(f"‚úÖ Worker-{worker_id}: Terminado despu√©s de {elapsed:.1f}s")
    
    def _write_buffer_ultra_fast(self, articles: List[Dict], worker_id: int):
        """Escritura ultra-r√°pida a disco con buffers grandes LOCKLESS (mejorado con patterns del billion_parameters)"""
        try:
            # Counter at√≥mico sin lock expl√≠cito (per-worker)
            current_file_num = self.worker_file_counters[worker_id]
            self.worker_file_counters[worker_id] += 1
            
            output_file = self.output_dir / f"articles_hybrid_{worker_id}_{current_file_num:04d}.jsonl"
            
            # Buffer ultra-masivo de escritura (8MB inspirado en billion_parameters)
            with open(output_file, 'w', encoding='utf-8', buffering=8*1024*1024) as f:  # 8MB buffer
                for article in articles:
                    json.dump(article, f, ensure_ascii=False, separators=(',', ':'))
                    f.write('\n')
            
            # Stats at√≥micos sin lock
            self.stats['batches_written'] += 1
            
            print(f"üíæ Worker-{worker_id}: {len(articles):,} art√≠culos ‚Üí {output_file.name}")
            
        except Exception as e:
            print(f"‚ùå Error escritura lockless: {e}")
    
    def add_batch(self, batch: List[Tuple[str, str]]):
        """A√±ade batch al pipeline (thread-safe) con retry agresivo"""
        for attempt in range(5):  # 5 intentos
            try:
                timeout = 0.1 + (attempt * 0.05)  # Timeout incremental
                self.raw_batch_queue.put(batch, timeout=timeout)
                return True
            except queue.Full:
                if attempt == 4:  # √öltimo intento
                    print(f"‚ö†Ô∏è Cola saturada despu√©s de 5 intentos, skipping batch de {len(batch)} p√°ginas")
                    return False
                time.sleep(0.01)  # Breve pausa antes del retry
        return False
    
    def stop_workers(self):
        """Detiene workers de forma ULTRA-AGRESIVA sin esperas"""
        print(f"ÔøΩ DETENCI√ìN ULTRA-AGRESIVA DE WORKERS...")
        self.running = False
        
        # 1. Enviar se√±ales de parada masivas (sin timeout)
        print("üì§ Flooding colas con se√±ales de parada...")
        for _ in range(self.num_workers * 3):  # Triple de se√±ales
            try:
                self.raw_batch_queue.put_nowait(None)
                self.processed_queue.put_nowait(None)
                self.output_queue.put_nowait(None)
            except:
                pass  # Ignorar errores de cola llena
        
        # 2. Vaciar todas las colas agresivamente
        print("üóëÔ∏è Vaciando colas agresivamente...")
        
        def drain_queue(q, name):
            count = 0
            try:
                while not q.empty():
                    q.get_nowait()
                    count += 1
                    if count > 1000:  # L√≠mite de seguridad
                        break
            except:
                pass
            return count
        
        raw_drained = drain_queue(self.raw_batch_queue, "Raw")
        proc_drained = drain_queue(self.processed_queue, "Processed")
        out_drained = drain_queue(self.output_queue, "Output")
        
        print(f"üóëÔ∏è Vaciadas: Raw({raw_drained}), Proc({proc_drained}), Out({out_drained})")
        
        # 3. Shutdown inmediato de pools (sin esperar)
        print("üõë Shutdown inmediato de pools...")
        
        pools = [
            ("Extraction", self.extraction_pool),
            ("Processing", self.processing_pool), 
            ("Output", self.output_pool)
        ]
        
        for name, pool in pools:
            try:
                pool.shutdown(wait=False)  # NO ESPERAR
                print(f"‚úÖ {name} pool ‚Üí shutdown iniciado")
            except Exception as e:
                print(f"‚ö†Ô∏è {name} pool error: {e}")
        
        # 4. Verificaci√≥n r√°pida de threads
        import threading
        active_count = threading.active_count()
        print(f"üßµ Threads activos: {active_count}")
        
        # 5. Forzar garbage collection
        import gc
        gc.collect()
        
        print(f"‚úÖ DETENCI√ìN ULTRA-AGRESIVA COMPLETADA")
        print(f"üö® NOTA: Algunos threads pueden seguir activos pero el proceso principal continuar√°")
    
    def get_stats(self):
        """Obtiene estad√≠sticas actuales"""
        return self.stats.copy()

class UltraFastXMLHandler(xml.sax.ContentHandler):
    """Handler SAX ultra-optimizado con env√≠o masivo a workers y finalizaci√≥n autom√°tica"""
    
    def __init__(self, processor: UltraOptimizedProcessor):
        super().__init__()
        self.processor = processor
        
        # Estado del parser
        self.current_element = ""
        self.current_page = {}
        self.in_page = False
        self.content_buffer = ""
        
        # Batch management ultra-eficiente
        self.page_batch = []
        self.batch_size = ULTRA_CONFIG['BATCH_SIZE']
        
        # Contadores y estado
        self.total_pages_seen = 0
        self.total_batches_sent = 0
        self.start_time = time.time()
        self.last_page_time = time.time()  # Para detectar finalizaci√≥n
        self.xml_finished = False  # Flag de finalizaci√≥n del XML
        
        print(f"üìñ ULTRA-FAST XML HANDLER:")
        print(f"   üì¶ Batch size: {self.batch_size:,}")
        print(f"   üöÄ Turbo mode: ‚úÖ")
    
    def startElement(self, name, attrs):
        self.current_element = name
        if name == 'page':
            self.in_page = True
            self.current_page = {}
        elif name == 'text':
            self.content_buffer = ""
    
    def endDocument(self):
        """Se llama cuando el XML ha terminado de procesarse"""
        print(f"üèÅ XML COMPLETADO - Iniciando finalizaci√≥n autom√°tica...")
        self.xml_finished = True
        
        # Enviar √∫ltimo batch inmediatamente
        if self.page_batch:
            print(f"üì¶ Enviando √∫ltimo batch: {len(self.page_batch)} p√°ginas")
            self._send_batch_to_workers()
        
        print(f"‚úÖ XML procesado completamente")
    
    def endElement(self, name):
        if name == 'page' and self.in_page:
            self._process_page()
            self.in_page = False
            self.current_page = {}
        elif name in ['title', 'text'] and self.in_page:
            self.current_page[name] = self.content_buffer
            self.content_buffer = ""
        self.current_element = ""
    
    def characters(self, content):
        if self.current_element in ['title', 'text']:
            self.content_buffer += content
    
    def _process_page(self):
        """Procesa p√°gina de forma ultra-eficiente"""
        self.total_pages_seen += 1
        self.last_page_time = time.time()  # Actualizar tiempo de √∫ltima p√°gina
        
        title = self.current_page.get('title', '').strip()
        text = self.current_page.get('text', '').strip()
        
        # Filtro ultra-r√°pido sin regex
        if title and text and len(text) > 150:
            self.page_batch.append((title, text))
        
        # Enviar batch cuando est√© lleno
        if len(self.page_batch) >= self.batch_size:
            self._send_batch_to_workers()
        
        # Progreso cada 200K p√°ginas
        if self.total_pages_seen % 200000 == 0:
            self._print_ultra_progress()
    
    def _send_batch_to_workers(self):
        """Env√≠a batch a workers de forma persistente"""
        if not self.page_batch:
            return
        
        # Env√≠o persistente con retry
        max_attempts = 10
        for attempt in range(max_attempts):
            if self.processor.add_batch(self.page_batch.copy()):
                self.total_batches_sent += 1
                self.page_batch.clear()
                return
            
            # Si falla, esperar progresivamente m√°s tiempo
            wait_time = 0.01 * (2 ** attempt)  # Backoff exponencial
            time.sleep(min(wait_time, 1.0))  # M√°ximo 1 segundo
            
            # En el intento 5, reducir batch size
            if attempt == 5 and len(self.page_batch) > 10000:
                print(f"‚ö†Ô∏è Reduciendo batch size de {len(self.page_batch)} a 10000")
                self.page_batch = self.page_batch[:10000]
        
        # Si despu√©s de todos los intentos no se puede enviar
        print(f"‚ùå No se pudo enviar batch despu√©s de {max_attempts} intentos, descartando {len(self.page_batch)} p√°ginas")
        self.page_batch.clear()
    
    def _print_ultra_progress(self):
        """Progreso ultra-r√°pido con informaci√≥n de colas"""
        elapsed = time.time() - self.start_time
        pages_rate = self.total_pages_seen / elapsed if elapsed > 0 else 0
        
        processor_stats = self.processor.get_stats()
        
        # Informaci√≥n de colas para debugging
        raw_queue_size = self.processor.raw_batch_queue.qsize()
        processed_queue_size = self.processor.processed_queue.qsize()
        output_queue_size = self.processor.output_queue.qsize()
        
        print(f"üöÄ ULTRA-FAST PROGRESS:")
        print(f"   üìñ P√°ginas: {self.total_pages_seen:,} ({pages_rate:.0f} p/s)")
        print(f"   üì¶ Batches enviados: {self.total_batches_sent:,}")
        print(f"   üìö Art√≠culos procesados: {processor_stats['articles_processed']:,}")
        print(f"   üóÇÔ∏è Colas: Raw({raw_queue_size}), Proc({processed_queue_size}), Out({output_queue_size})")
        print(f"   ‚è±Ô∏è Tiempo: {elapsed/60:.1f}min")
        
        # Verificar si alcanzamos el objetivo
        if pages_rate >= ULTRA_CONFIG['TARGET_SPEED']:
            print(f"üéØ ‚úÖ OBJETIVO ALCANZADO: {pages_rate:.0f} >= {ULTRA_CONFIG['TARGET_SPEED']:,} p/s")
        
        # Advertir si las colas est√°n muy llenas
        max_queue_size = ULTRA_CONFIG['QUEUE_SIZE']
        if raw_queue_size > max_queue_size * 0.8:
            print(f"‚ö†Ô∏è Cola raw cerca del l√≠mite: {raw_queue_size}/{max_queue_size}")
        if output_queue_size > max_queue_size * 0.8:
            print(f"‚ö†Ô∏è Cola output cerca del l√≠mite: {output_queue_size}/{max_queue_size}")
    
    def finalize_processing(self):
        """Finaliza el procesamiento con detecci√≥n autom√°tica y timeout inteligente"""
        print(f"üîÑ INICIANDO FINALIZACI√ìN INTELIGENTE...")
        
        # 1. Verificar si XML termin√≥ correctamente
        if self.xml_finished:
            print(f"‚úÖ XML completado correctamente")
        else:
            print(f"‚ö†Ô∏è XML no completado - posible interrupci√≥n")
            
            # Enviar √∫ltimo batch si existe
            if self.page_batch:
                print(f"üì¶ √öltimo batch: {len(self.page_batch)} p√°ginas")
                try:
                    self.processor.raw_batch_queue.put(self.page_batch.copy(), timeout=1.0)
                    print(f"‚úÖ √öltimo batch enviado")
                except:
                    print(f"‚ö†Ô∏è √öltimo batch descartado por timeout")
                self.page_batch.clear()
        
        # 2. Esperar con timeout adaptativo basado en el volumen
        expected_time = max(5, min(30, self.total_batches_sent // 10))  # 5-30 segundos seg√∫n volumen
        print(f"‚è≥ Esperando finalizaci√≥n (m√°ximo {expected_time} segundos basado en volumen)...")
        
        start_wait = time.time()
        last_articles = self.processor.get_stats()['articles_processed']
        stable_count = 0  # Contador de estabilidad
        
        while time.time() - start_wait < expected_time:
            time.sleep(1)
            current_stats = self.processor.get_stats()
            current_articles = current_stats['articles_processed']
            
            # Estado de colas
            raw_size = self.processor.raw_batch_queue.qsize()
            proc_size = self.processor.processed_queue.qsize()
            out_size = self.processor.output_queue.qsize()
            
            elapsed_wait = time.time() - start_wait
            print(f"üìä T+{elapsed_wait:.0f}s: {current_articles:,} art√≠culos | Colas: R({raw_size}) P({proc_size}) O({out_size})")
            
            # Verificar estabilidad
            if current_articles == last_articles:
                stable_count += 1
            else:
                stable_count = 0
                last_articles = current_articles
            
            # Condiciones de finalizaci√≥n
            if raw_size == 0 and proc_size == 0 and out_size == 0:
                if stable_count >= 2:  # 2 segundos estable con colas vac√≠as
                    print(f"‚úÖ Procesamiento completado (colas vac√≠as + estable)")
                    break
            elif stable_count >= 5:  # 5 segundos sin cambios
                print(f"‚úÖ Procesamiento estabilizado (sin nuevos art√≠culos)")
                break
        
        # 3. Finalizaci√≥n
        elapsed_total = time.time() - start_wait
        print(f"üõë Finalizaci√≥n despu√©s de {elapsed_total:.1f}s")
        
        # Estad√≠sticas finales
        final_stats = self.processor.get_stats()
        print(f"üìä RESUMEN FINAL:")
        print(f"   üìñ P√°ginas XML: {self.total_pages_seen:,}")
        print(f"   üì¶ Batches enviados: {self.total_batches_sent:,}")
        print(f"   üìö Art√≠culos procesados: {final_stats['articles_processed']:,}")
        print(f"   üíæ Archivos escritos: {final_stats['batches_written']:,}")
        
        # Forzar detenci√≥n inmediata
        self.processor.running = False
        print(f"üö® Workers marcados para detenci√≥n")

def setup_system_for_ultra_performance():
    """Configura el sistema para m√°ximo rendimiento"""
    print("‚ö° CONFIGURANDO SISTEMA PARA ULTRA-RENDIMIENTO...")
    
    try:
        # L√≠mites del sistema
        import resource
        resource.setrlimit(resource.RLIMIT_NOFILE, (100000, 100000))
        
        # Variables de entorno optimizadas
        os.environ['PYTHONUNBUFFERED'] = '1'
        os.environ['PYTHONDONTWRITEBYTECODE'] = '1'
        os.environ['OMP_NUM_THREADS'] = str(ULTRA_CONFIG['MAX_WORKERS'])
        
        # Configurar afinidad de CPU
        process = psutil.Process()
        available_cpus = list(range(min(ULTRA_CONFIG['MAX_WORKERS'], os.cpu_count())))
        process.cpu_affinity(available_cpus)
        
        # Configurar prioridad alta
        try:
            process.nice(-10)  # Prioridad alta
        except:
            pass
        
        print(f"‚úÖ Sistema configurado:")
        print(f"   üîÑ CPU cores: {len(available_cpus)}")
        print(f"   üìÅ File descriptors: 100,000")
        print(f"   ‚ö° Prioridad: Alta")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Configuraci√≥n parcial: {e}")

def main():
    """Funci√≥n principal h√≠brida ultra-optimizada"""
    parser = argparse.ArgumentParser(description="Caroline Ultra Extractor - Versi√≥n H√≠brida")
    parser.add_argument("--xml", type=str, required=True, help="Archivo XML de Wikipedia")
    parser.add_argument("--output", type=str, default="data_ultra_hybrid", help="Directorio de salida")
    
    args = parser.parse_args()
    
    # Configurar sistema
    setup_system_for_ultra_performance()
    
    # Verificar archivo
    xml_path = Path(args.xml)
    if not xml_path.exists():
        print(f"‚ùå Archivo XML no encontrado: {args.xml}")
        return 1
    
    print(f"üöÄ CAROLINE ULTRA EXTRACTOR - VERSI√ìN H√çBRIDA")
    print(f"üéØ OBJETIVO: {ULTRA_CONFIG['TARGET_SPEED']:,} p√°ginas/segundo")
    print(f"üìÅ XML: {xml_path.name} ({xml_path.stat().st_size / (1024**3):.1f}GB)")
    print(f"‚ö° TURBO MODE: {'‚úÖ' if ULTRA_CONFIG['TURBO_MODE'] else '‚ùå'}")
    
    try:
        # Crear procesador ultra-optimizado
        processor = UltraOptimizedProcessor()
        
        # Iniciar workers especializados
        processor.start_workers()
        
        # Crear handler SAX ultra-r√°pido
        handler = UltraFastXMLHandler(processor)
        
        # Manejo de se√±ales
        def signal_handler(signum, frame):
            print(f"\n‚ö†Ô∏è Se√±al recibida, finalizando...")
            processor.stop_workers()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # Procesar XML con detecci√≥n autom√°tica de finalizaci√≥n
        start_time = time.time()
        print(f"üöÄ Iniciando procesamiento h√≠brido ultra-optimizado...")
        
        try:
            # SAX parsing (confiable) + workers ultra-optimizados
            xml.sax.parse(str(xml_path), handler)
            print(f"‚úÖ SAX Parser completado exitosamente")
        except Exception as e:
            print(f"‚ö†Ô∏è SAX Parser termin√≥ con excepci√≥n: {e}")
        
        # Finalizar con detecci√≥n inteligente
        print(f"üèÅ XML TERMINADO - Iniciando finalizaci√≥n inteligente...")
        handler.finalize_processing()
        
        print(f"üõë DETENIENDO WORKERS...")
        processor.stop_workers()
        
        # Breve pausa para permitir que los workers terminen limpiamente
        print(f"‚è≥ Pausa de limpieza (2 segundos)...")
        time.sleep(2)
        
        # Estad√≠sticas finales
        elapsed = time.time() - start_time
        final_stats = processor.get_stats()
        
        pages_rate = handler.total_pages_seen / elapsed if elapsed > 0 else 0
        articles_rate = final_stats['articles_processed'] / elapsed if elapsed > 0 else 0
        
        print(f"\nüéâ PROCESAMIENTO H√çBRIDO COMPLETADO:")
        print(f"   üìñ P√°ginas procesadas: {handler.total_pages_seen:,}")
        print(f"   üìö Art√≠culos v√°lidos: {final_stats['articles_processed']:,}")
        print(f"   üì¶ Batches escritos: {final_stats['batches_written']:,}")
        print(f"   ‚è±Ô∏è Tiempo total: {elapsed/3600:.2f}h")
        print(f"   üöÄ Velocidad p√°ginas: {pages_rate:.0f} p/s")
        print(f"   üöÄ Velocidad art√≠culos: {articles_rate:.0f} a/s")
        
        # Evaluar resultado
        if pages_rate >= ULTRA_CONFIG['TARGET_SPEED']:
            print(f"üéØ ‚úÖ OBJETIVO ALCANZADO: {pages_rate:.0f} >= {ULTRA_CONFIG['TARGET_SPEED']:,} p/s")
        else:
            improvement = pages_rate / 600  # vs versi√≥n anterior
            print(f"üìà MEJORA CONSEGUIDA: {improvement:.1f}x m√°s r√°pido")
            print(f"üìä Progreso hacia objetivo: {(pages_rate / ULTRA_CONFIG['TARGET_SPEED']) * 100:.1f}%")
        
        # Verificaci√≥n final de finalizaci√≥n limpia
        import threading
        active_threads = threading.active_count()
        if active_threads <= 5:  # Main + threads del sistema
            print(f"‚úÖ FINALIZACI√ìN LIMPIA - {active_threads} threads activos")
        else:
            print(f"‚ö†Ô∏è FINALIZACI√ìN PARCIAL - {active_threads} threads a√∫n activos")
            print(f"üö® El proceso deber√≠a terminar autom√°ticamente en unos segundos")
        
        # Forzar garbage collection final
        import gc
        gc.collect()
        
        print(f"üéØ PROCESO PRINCIPAL TERMINANDO...")
        return 0
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Interrumpido por usuario")
        processor.stop_workers()
        return 130
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
