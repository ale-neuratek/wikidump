#!/usr/bin/env python3
"""
üß† ADAPTIVE DATASET PROCESSOR - Coordinador de procesamiento optimizado
======================================================================
Responsabilidades:
- Estimaci√≥n y an√°lisis del dataset
- Configuraci√≥n adaptativa de hardware
- Optimizaci√≥n de par√°metros seg√∫n tama√±o
- Coordinaci√≥n del procesamiento
- Logging detallado con timestamps cada 5-10 minutos
- Generaci√≥n de categor√≠a "consciencia" al final (NO conscious.txt)
- Todos los art√≠culos procesados como conversaciones via content_manager
"""

import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
from hardware_configs import get_hardware_config

class AdaptiveProcessor:
    """Procesador adaptativo que optimiza autom√°ticamente seg√∫n el dataset"""
    
    def __init__(self):
        self.start_time = time.time()
        self.log_file = "adaptive_processing.log"
        self.log_interval = 5 * 60  # 5 minutos
        self.last_log_time = time.time()
        
        # Limpiar log anterior
        if Path(self.log_file).exists():
            Path(self.log_file).unlink()
        
    def log(self, message: str, force: bool = False):
        """Log con timestamp si ha pasado el intervalo o es forzado"""
        current_time = time.time()
        
        if force or (current_time - self.last_log_time) >= self.log_interval:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            elapsed = (current_time - self.start_time) / 3600  # horas
            
            log_entry = f"[{timestamp}] T+{elapsed:.1f}h: {message}"
            
            # Log a archivo
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry + '\n')
            
            # Log a consola
            print(log_entry)
            
            if not force:
                self.last_log_time = current_time

    def estimate_dataset_size(self, input_dir: str) -> dict:
        """Estima el tama√±o y caracter√≠sticas del dataset"""
        input_path = Path(input_dir)
        files = list(input_path.glob("*.jsonl"))
        
        if not files:
            return {'total_articles': 0, 'total_files': 0, 'total_size_gb': 0}
        
        self.log(f"üìÇ AN√ÅLISIS DEL DATASET INICIADO", force=True)
        self.log(f"   üìÅ Directorio: {input_dir}", force=True)
        self.log(f"   üìÑ Archivos encontrados: {len(files)}", force=True)
        
        total_articles = 0
        total_size_bytes = 0
        sample_files = min(3, len(files))
        
        for i, file_path in enumerate(files[:sample_files]):
            try:
                file_size = file_path.stat().st_size
                total_size_bytes += file_size
                
                sample_count = 0
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f):
                        if line.strip():
                            sample_count += 1
                        if line_num >= 999:  # Muestra de 1000 l√≠neas
                            break
                
                # Extrapolar para el archivo completo
                if sample_count > 0:
                    lines_per_byte = sample_count / (file_size * min(1000, line_num + 1) / 1000)
                    estimated_in_file = int(file_size * lines_per_byte)
                    total_articles += estimated_in_file
                    
                    file_size_mb = file_size / (1024 * 1024)
                    self.log(f"   üìÑ {file_path.name}: ~{estimated_in_file:,} art√≠culos ({file_size_mb:.1f}MB)", force=True)
            
            except Exception as e:
                self.log(f"   ‚ö†Ô∏è Error estimando {file_path.name}: {e}", force=True)
                # Estimaci√≥n por defecto
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                estimated_in_file = int(file_size_mb * 400)  # ~400 art√≠culos por MB
                total_articles += estimated_in_file
        
        # Extrapolar para todos los archivos
        if len(files) > sample_files:
            # Calcular tama√±o total de todos los archivos
            total_size_bytes = sum(f.stat().st_size for f in files)
            avg_per_file = total_articles / sample_files
            total_articles = int(avg_per_file * len(files))
        
        total_size_gb = total_size_bytes / (1024**3)
        
        dataset_info = {
            'total_articles': total_articles,
            'total_files': len(files),
            'total_size_gb': total_size_gb,
            'total_size_bytes': total_size_bytes
        }
        
        self.log(f"üìä AN√ÅLISIS COMPLETO: {total_articles:,} art√≠culos en {len(files)} archivos ({total_size_gb:.1f}GB)", force=True)
        return dataset_info
    
    def get_optimized_config(self, dataset_info: dict) -> dict:
        """Obtiene configuraci√≥n optimizada seg√∫n el dataset"""
        total_articles = dataset_info['total_articles']
        
        # Obtener configuraci√≥n base del hardware
        base_config = get_hardware_config(dataset_size_articles=total_articles)
        
        # Configuraciones espec√≠ficas para el procesamiento
        optimized_config = {
            **base_config,
            
            # Configuraci√≥n de logging
            'LOG_INTERVAL_MINUTES': 5,
            'LOG_FILE': self.log_file,
            
            # Configuraci√≥n de categor√≠a consciencia
            'INCLUDE_CONSCIENCIA_CATEGORY': True,
            'CONSCIENCIA_CONVERSATIONS_PER_CATEGORY': 2,  # 2 conversaciones por categor√≠a encontrada
            
            # Configuraci√≥n de procesamiento
            'PROCESS_ALL_ARTICLES': True,
            'CONVERSATIONS_PER_ARTICLE': 1,  # 1 conversaci√≥n principal por art√≠culo
            
            # Configuraci√≥n de archivos de salida
            'CONVERSATIONS_PER_OUTPUT_FILE': 50000,  # 50K conversaciones por archivo
            'REMOVE_CONSCIOUS_TXT': True,  # NO generar conscious.txt
            
            # Informaci√≥n del dataset para el procesador
            'DATASET_INFO': dataset_info
        }
        
        self.log(f"‚öôÔ∏è CONFIGURACI√ìN OPTIMIZADA:", force=True)
        self.log(f"   üîÑ Workers: {optimized_config.get('MAX_WORKERS', 'N/A')}", force=True)
        self.log(f"   üì¶ Batch size: {optimized_config.get('BATCH_SIZE', 'N/A'):,}", force=True)
        self.log(f"   üóÇÔ∏è Queue size: {optimized_config.get('QUEUE_SIZE', 'N/A'):,}", force=True)
        self.log(f"   üíæ Memory buffer: {optimized_config.get('MEMORY_BUFFER_GB', 'N/A')}GB", force=True)
        
        return optimized_config

    def run_processing(self, input_dir: str, output_dir: str) -> int:
        """Ejecuta el procesamiento completo con configuraci√≥n optimizada"""
        
        self.log(f"üöÄ PROCESAMIENTO ADAPTATIVO INICIADO", force=True)
        self.log(f"   üìÇ Input: {input_dir}", force=True)
        self.log(f"   üìÅ Output: {output_dir}", force=True)
        
        # Estimar dataset
        dataset_info = self.estimate_dataset_size(input_dir)
        if dataset_info['total_articles'] == 0:
            self.log(f"‚ùå No se encontraron art√≠culos en {input_dir}", force=True)
            return 1
        
        # Obtener configuraci√≥n optimizada
        config = self.get_optimized_config(dataset_info)
        
        # Importar el procesador simplificado
        from simple_processor import MassiveParallelDatasetProcessor
        
        try:
            # Log detalles del inicio
            input_files = list(Path(input_dir).glob("*.jsonl"))
            self.log(f"üìã INICIANDO PROCESAMIENTO DE {len(input_files)} ARCHIVOS", force=True)
            self.log(f"   üì¶ Lotes configurados: {config.get('BATCH_SIZE', 'N/A'):,} art√≠culos por lote", force=True)
            self.log(f"   ‚öôÔ∏è Workers: {config.get('MAX_WORKERS', 'N/A')}", force=True)
            self.log(f"   üö´ NO generar conscious.txt - solo categor√≠a consciencia", force=True)
            
            # Crear procesador con configuraci√≥n optimizada
            processor = MassiveParallelDatasetProcessor(input_dir, output_dir, adaptive_config=config)
            
            # Ejecutar procesamiento principal con logging peri√≥dico
            self.log(f"üèÅ INICIANDO PROCESAMIENTO PRINCIPAL", force=True)
            success = processor.process_all_files()
            
            if success:
                self.log(f"‚úÖ PROCESAMIENTO PRINCIPAL COMPLETADO", force=True)
                
                # Obtener categor√≠as encontradas del procesador
                categories_found = []
                if hasattr(processor, 'categories_stats'):
                    categories_found = list(processor.categories_stats.keys())
                elif hasattr(processor, 'content_manager'):
                    categories_found = processor.content_manager.get_categories()
                else:
                    categories_found = ['arte', 'geografia', 'historia', 'ciencias', 'biologia', 'tecnologia', 'deportes', 'general']
                
                self.log(f"üè∑Ô∏è CATEGOR√çAS ENCONTRADAS: {len(categories_found)} categor√≠as", force=True)
                self.log(f"   üìù Categor√≠as: {', '.join(categories_found[:10])}", force=True)
                
                # Generar categor√≠a consciencia al final usando content_manager
                self.generate_consciencia_category(categories_found, output_dir)
                
                self.log(f"üéØ PROCESAMIENTO COMPLETO FINALIZADO EXITOSAMENTE", force=True)
                return 0
            else:
                self.log(f"‚ùå PROCESAMIENTO PRINCIPAL FALL√ì", force=True)
                return 1
                
        except Exception as e:
            self.log(f"üí• ERROR DURANTE PROCESAMIENTO: {e}", force=True)
            import traceback
            traceback.print_exc()
            return 1

    def generate_consciencia_category(self, categories_found: list, output_dir: str):
        """Genera la categor√≠a consciencia al final con conversaciones sobre categor√≠as encontradas"""
        
        self.log(f"üß† GENERANDO CATEGOR√çA CONSCIENCIA", force=True)
        self.log(f"   üè∑Ô∏è Categor√≠as encontradas: {len(categories_found)}", force=True)
        
        # Importar content_manager para generar conversaciones
        from content_manager import ContentManager
        content_manager = ContentManager()
        
        # Crear directorio consciencia
        consciencia_dir = Path(output_dir) / "consciencia"
        consciencia_dir.mkdir(parents=True, exist_ok=True)
        
        # Crear conversaciones sobre conocimiento y categor√≠as
        consciencia_conversations = []
        
        # 1. Conversaci√≥n principal sobre el conocimiento disponible
        main_article = {
            'title': 'Sistema de Conocimiento',
            'content': f'''Este sistema contiene conocimiento organizado en {len(categories_found)} categor√≠as principales. 
            El conocimiento est√° estructurado para permitir consultas espec√≠ficas sobre diferentes temas. 
            Las categor√≠as disponibles incluyen: {", ".join(categories_found[:10])}. 
            Cada categor√≠a contiene art√≠culos especializados con informaci√≥n detallada y conversaciones generadas 
            para facilitar el aprendizaje interactivo. El sistema est√° dise√±ado para proporcionar respuestas 
            contextuales y permitir exploraci√≥n profunda de cualquier tema de inter√©s.'''
        }
        
        main_result = content_manager.process_article(main_article)
        if main_result:
            for conv in main_result['conversations']:
                consciencia_conversations.append({
                    'question': conv['question'],
                    'answer': conv['answer'],
                    'category': 'consciencia',
                    'subcategory': 'conocimiento_general',
                    'conversation_type': 'descripcion_sistema'
                })
        
        # 2. Conversaciones sobre capacidades del sistema
        capabilities_article = {
            'title': 'Capacidades del Sistema de Conocimiento',
            'content': f'''Las capacidades de este sistema incluyen: an√°lisis de {len(categories_found)} categor√≠as tem√°ticas, 
            generaci√≥n de conversaciones contextuales, categorizaci√≥n inteligente de contenidos, 
            respuestas especializadas por √°rea de conocimiento, y exploraci√≥n interactiva de temas. 
            El sistema puede responder preguntas espec√≠ficas sobre cualquier categor√≠a, 
            proporcionar definiciones, explicaciones hist√≥ricas, datos geogr√°ficos, informaci√≥n cient√≠fica, 
            y an√°lisis tem√°ticos profundos. La informaci√≥n est√° constantemente organizada y accesible.'''
        }
        
        capabilities_result = content_manager.process_article(capabilities_article)
        if capabilities_result:
            for conv in capabilities_result['conversations']:
                consciencia_conversations.append({
                    'question': conv['question'],
                    'answer': conv['answer'],
                    'category': 'consciencia', 
                    'subcategory': 'capacidades_sistema',
                    'conversation_type': 'descripcion_capacidades'
                })
        
        # 3. Conversaciones espec√≠ficas sobre cada categor√≠a encontrada (m√°ximo 2 por categor√≠a)
        for i, category in enumerate(categories_found[:15]):  # Limitar a 15 categor√≠as principales
            category_article = {
                'title': f'Categor√≠a {category.title()}',
                'content': f'''La categor√≠a {category} contiene informaci√≥n especializada sobre este tema. 
                Esta √°rea del conocimiento incluye definiciones, conceptos clave, ejemplos relevantes, 
                y an√°lisis detallados. Los usuarios pueden hacer preguntas espec√≠ficas sobre {category} 
                y recibir respuestas contextuales basadas en el contenido disponible. 
                Esta categor√≠a forma parte del sistema de conocimiento integral y est√° conectada 
                con otras √°reas tem√°ticas para proporcionar una comprensi√≥n completa.'''
            }
            
            category_result = content_manager.process_article(category_article)
            if category_result:
                # Solo tomar las primeras 2 conversaciones
                for conv in category_result['conversations'][:2]:
                    consciencia_conversations.append({
                        'question': conv['question'],
                        'answer': conv['answer'],
                        'category': 'consciencia',
                        'subcategory': f'categoria_{category}',
                        'conversation_type': 'descripcion_categoria'
                    })
        
        # Escribir conversaciones en archivos JSONL
        conversations_per_file = 50000
        file_counter = 0
        
        for i in range(0, len(consciencia_conversations), conversations_per_file):
            file_counter += 1
            output_file = consciencia_dir / f"consciencia_{file_counter:04d}.jsonl"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                batch = consciencia_conversations[i:i + conversations_per_file]
                for conv in batch:
                    # Formato de conversaci√≥n est√°ndar usando content_manager
                    conversation_record = {
                        'conversations': [
                            {'role': 'user', 'content': conv['question']},
                            {'role': 'assistant', 'content': conv['answer']}
                        ],
                        'metadata': {
                            'source_article': 'Sistema de Consciencia',
                            'category': 'consciencia',
                            'subcategory': conv['subcategory'],
                            'conversation_type': conv['conversation_type'],
                            'generation_date': datetime.now().isoformat(),
                            'categories_available': categories_found[:20]  # Primeras 20 categor√≠as
                        }
                    }
                    f.write(json.dumps(conversation_record, ensure_ascii=False) + '\n')
        
        # Crear metadata
        metadata_file = consciencia_dir / "metadata_consciencia.json"
        metadata = {
            'category': 'consciencia',
            'total_conversations': len(consciencia_conversations),
            'total_files': file_counter,
            'categories_found': categories_found,
            'description': 'Conversaciones sobre el conocimiento disponible, capacidades del sistema, y descripciones de categor√≠as',
            'generation_date': datetime.now().isoformat(),
            'conversation_types': ['descripcion_sistema', 'descripcion_capacidades', 'descripcion_categoria'],
            'note': 'Esta categor√≠a NO reemplaza conscious.txt - es una categor√≠a de conversaciones como las dem√°s'
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        self.log(f"üß† Categor√≠a consciencia completada:", force=True)
        self.log(f"   üìù {len(consciencia_conversations)} conversaciones generadas", force=True)
        self.log(f"   üìÅ {file_counter} archivos JSONL creados", force=True)
        self.log(f"   üè∑Ô∏è {len(categories_found)} categor√≠as descritas", force=True)

def main():
    """Funci√≥n principal del procesador adaptativo"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Adaptive Dataset Processor")
    parser.add_argument("--input", default="data_ultra_hybrid", help="Directorio de entrada")
    parser.add_argument("--output", default="consciencia_completa", help="Directorio de salida")
    
    args = parser.parse_args()
    
    # Crear procesador adaptativo
    processor = AdaptiveProcessor()
    
    processor.log("üß† ADAPTIVE DATASET PROCESSOR INICIADO", force=True)
    processor.log("=" * 50, force=True)
    
    # Verificar entrada
    if not Path(args.input).exists():
        processor.log(f"‚ùå Directorio de entrada no encontrado: {args.input}", force=True)
        return 1
    
    try:
        # Ejecutar procesamiento completo
        result = processor.run_processing(args.input, args.output)
        
        elapsed = (time.time() - processor.start_time) / 3600
        processor.log(f"‚è±Ô∏è PROCESAMIENTO TOTAL: {elapsed:.2f} horas", force=True)
        
        return result
        
    except Exception as e:
        processor.log(f"‚ùå Error: {e}", force=True)
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
