#!/usr/bin/env python3
"""
ğŸš€ PROCESADOR MASIVO DE DATASET COMPLETO
=======================================
Ejecuta el pipeline completo con toda la configuraciÃ³n optimizada para datasets masivos
"""

import os
import sys
import time
import subprocess
from pathlib import Path

def run_full_dataset_processing():
    """Ejecuta el procesamiento del dataset completo con configuraciÃ³n optimizada"""
    
    print("ğŸš€ PROCESAMIENTO MASIVO DEL DATASET COMPLETO")
    print("=" * 60)
    
    # Directorios
    input_dir = "/home/ubuntu/wikidump_workspace/wikidump/data_ultra_hybrid"
    output_dir = "/home/ubuntu/wikidump_workspace/wikidump/consciencia_completa"
    
    # Verificar entrada
    if not Path(input_dir).exists():
        print(f"âŒ Directorio de entrada no encontrado: {input_dir}")
        return 1
    
    # Contar archivos de entrada
    input_files = list(Path(input_dir).glob("*.jsonl"))
    if not input_files:
        print(f"âŒ No se encontraron archivos .jsonl en {input_dir}")
        return 1
    
    print(f"ğŸ“‚ Directorio de entrada: {input_dir}")
    print(f"ğŸ“ Directorio de salida: {output_dir}")
    print(f"ğŸ“„ Archivos de entrada: {len(input_files)} archivos")
    
    # Estimar tamaÃ±o total del dataset
    total_size_mb = sum(f.stat().st_size for f in input_files) / (1024 * 1024)
    estimated_articles = int(total_size_mb * 400)  # ~400 artÃ­culos por MB
    
    print(f"ğŸ“Š TamaÃ±o total: {total_size_mb:.1f}MB")
    print(f"ğŸ“Š ArtÃ­culos estimados: {estimated_articles:,}")
    
    # ConfirmaciÃ³n del usuario
    print(f"\nâš ï¸ PROCESAMIENTO MASIVO:")
    print(f"   ğŸ“Š Se procesarÃ¡n ~{estimated_articles:,} artÃ­culos")
    print(f"   â±ï¸ Tiempo estimado: 2-3 horas")
    print(f"   ğŸ’¾ Espacio requerido: ~10-15GB")
    
    confirm = input(f"\nğŸš€ Â¿Proceder con el procesamiento masivo? (s/N): ").strip().lower()
    if confirm not in ['s', 'si', 'y', 'yes']:
        print("âŒ Procesamiento cancelado por el usuario")
        return 0
    
    # Limpiar directorio de salida anterior
    if Path(output_dir).exists():
        print(f"ğŸ—‘ï¸ Limpiando directorio de salida anterior...")
        import shutil
        shutil.rmtree(output_dir)
    
    # Ejecutar adaptive_processor
    print(f"\nğŸš€ INICIANDO PROCESAMIENTO MASIVO...")
    print(f"â° Inicio: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    cmd = [
        "python3", 
        "adaptive_processor.py",
        "--input", input_dir,
        "--output", output_dir
    ]
    
    print(f"ğŸ”§ Comando: {' '.join(cmd)}")
    
    try:
        # Ejecutar el comando con salida en tiempo real
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        # Mostrar output en tiempo real
        for line in process.stdout:
            print(line.rstrip())
        
        # Esperar a que termine
        return_code = process.wait()
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"\nâ° Fin: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸ DuraciÃ³n total: {duration/3600:.2f} horas ({duration:.0f}s)")
        
        if return_code == 0:
            print(f"\nğŸ‰ PROCESAMIENTO COMPLETADO EXITOSAMENTE")
            
            # Verificar resultados
            verify_results(output_dir)
            
            return 0
        else:
            print(f"\nâŒ PROCESAMIENTO FALLÃ“ (cÃ³digo: {return_code})")
            return return_code
            
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ PROCESAMIENTO INTERRUMPIDO POR EL USUARIO")
        process.terminate()
        return 1
    except Exception as e:
        print(f"\nğŸ’¥ ERROR DURANTE EL PROCESAMIENTO: {e}")
        return 1

def verify_results(output_dir: str):
    """Verifica los resultados del procesamiento"""
    
    print(f"\nğŸ” VERIFICANDO RESULTADOS...")
    output_path = Path(output_dir)
    
    if not output_path.exists():
        print(f"âŒ Directorio de salida no existe: {output_dir}")
        return
    
    # Contar categorÃ­as
    categorias_dir = output_path / "categorias"
    if categorias_dir.exists():
        categories = [d for d in categorias_dir.iterdir() if d.is_dir()]
        print(f"ğŸ“ CategorÃ­as creadas: {len(categories)}")
        
        # Contar archivos y conversaciones por categorÃ­a
        total_files = 0
        total_conversations = 0
        
        for cat_dir in categories:
            jsonl_files = list(cat_dir.glob("*.jsonl"))
            total_files += len(jsonl_files)
            
            cat_conversations = 0
            for jsonl_file in jsonl_files:
                try:
                    with open(jsonl_file, 'r') as f:
                        cat_conversations += sum(1 for line in f if line.strip())
                except:
                    pass
            
            total_conversations += cat_conversations
            print(f"   ğŸ“‚ {cat_dir.name}: {len(jsonl_files)} archivos, {cat_conversations:,} conversaciones")
        
        print(f"ğŸ“„ Total archivos JSONL: {total_files}")
        print(f"ğŸ’¬ Total conversaciones: {total_conversations:,}")
    
    # Verificar estadÃ­sticas
    stats_dir = output_path / "estadisticas"
    if stats_dir.exists():
        print(f"ğŸ“Š Directorio de estadÃ­sticas: âœ…")
    
    # Verificar archivo conscious.txt
    conscious_file = output_path / "conscious.txt"
    if conscious_file.exists():
        size_mb = conscious_file.stat().st_size / (1024 * 1024)
        print(f"ğŸ“ Archivo conscious.txt: {size_mb:.1f}MB")
    
    print(f"âœ… VerificaciÃ³n de resultados completada")

def main():
    """FunciÃ³n principal"""
    try:
        return run_full_dataset_processing()
    except Exception as e:
        print(f"ğŸ’¥ Error fatal: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
