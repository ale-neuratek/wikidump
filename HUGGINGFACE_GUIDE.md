# ðŸ¤— GUÃA DE PUBLICACIÃ“N EN HUGGING FACE

## ðŸ“‹ Resumen

Esta integraciÃ³n permite publicar automÃ¡ticamente los datasets procesados de Wikipedia en un repositorio privado de Hugging Face usando SSH y autenticaciÃ³n por token.

## ðŸŽ¯ Repositorio Destino

- **Nombre**: `ale-neuratek/wikidump`
- **Tipo**: Dataset privado
- **Acceso**: Solo usuarios autorizados

## ðŸš€ Opciones de Uso

### 1. Pipeline Completo + PublicaciÃ³n AutomÃ¡tica

```bash
# Procesar Wikipedia completa y publicar automÃ¡ticamente
./init.sh
```

**QuÃ© hace:**
- Descarga dump de Wikipedia
- Procesa XML â†’ JSONL â†’ Conversaciones
- Al finalizar, pregunta si publicar en HF
- Configura autenticaciÃ³n automÃ¡ticamente
- Publica con metadatos completos

### 2. PublicaciÃ³n Independiente

```bash
# Solo publicar datasets ya procesados
./publish_to_hf.sh [directorio] [repositorio]

# Ejemplos:
./publish_to_hf.sh wiki-datasets ale-neuratek/wikidump
./publish_to_hf.sh ../wiki_conversations ale-neuratek/wikidump-v2
```

**Ventajas:**
- No requiere reprocesar datos
- Ãštil para mÃºltiples versiones
- Control granular sobre quÃ© publicar

### 3. ValidaciÃ³n Previa

```bash
# Validar configuraciÃ³n sin subir nada
python3 validate_hf_setup.py [directorio]

# Opciones avanzadas:
python3 validate_hf_setup.py wiki-datasets --test-speed
python3 validate_hf_setup.py data_custom --repo mi-org/mi-dataset
```

**Funciones:**
- Verifica token de HF
- Prueba permisos de escritura
- Analiza datos locales
- Estima tiempo de subida

### 4. DemostraciÃ³n RÃ¡pida

```bash
# Demo con datos de ejemplo
./demo_hf.sh
```

**Incluye:**
- Datos sintÃ©ticos de prueba
- ValidaciÃ³n completa
- Opciones de publicaciÃ³n
- Limpieza automÃ¡tica

## ðŸ” ConfiguraciÃ³n Inicial

### Paso 1: Token de Hugging Face

1. Ve a https://huggingface.co/settings/tokens
2. Crea un token con permisos **Write**
3. El sistema te pedirÃ¡ el token en el primer uso
4. Se guarda automÃ¡ticamente en `~/.cache/huggingface/token`

### Paso 2: Verificar Acceso al Repositorio

```bash
# Verificar que tienes acceso de escritura
python3 validate_hf_setup.py --repo ale-neuratek/wikidump
```

### Paso 3: Personalizar ConfiguraciÃ³n (Opcional)

Edita `hf_config.env` para cambiar:
- Repositorio destino
- ConfiguraciÃ³n de privacidad
- Metadatos del dataset
- Opciones de upload

## ðŸ“Š Estructura de PublicaciÃ³n

### Archivos Generados en HF

```
ale-neuratek/wikidump/
â”œâ”€â”€ README.md                          # Metadatos automÃ¡ticos
â”œâ”€â”€ arte/
â”‚   â”œâ”€â”€ conversaciones_arte_001.jsonl
â”‚   â”œâ”€â”€ conversaciones_arte_002.jsonl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ geografia/
â”‚   â”œâ”€â”€ conversaciones_geografia_001.jsonl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ historia/
â”‚   â””â”€â”€ conversaciones_historia_001.jsonl
â””â”€â”€ ...
```

### README AutomÃ¡tico Incluye:

- **EstadÃ­sticas completas**: Conversaciones, categorÃ­as, tamaÃ±o
- **Metadatos de generaciÃ³n**: Fecha, fuente, versiÃ³n
- **Ejemplos de uso**: CÃ³digo Python para cargar datos
- **InformaciÃ³n de licencia**: CC BY-SA 3.0
- **CategorÃ­as encontradas**: Lista completa organizada

## ðŸ› ï¸ ConfiguraciÃ³n Avanzada

### Cambiar Repositorio Destino

```bash
# OpciÃ³n 1: ParÃ¡metro directo
./publish_to_hf.sh wiki-datasets mi-organizacion/mi-dataset

# OpciÃ³n 2: Editar hf_config.env
nano hf_config.env
# Cambiar: HF_REPO_NAME="mi-organizacion/mi-dataset"
```

### Repositorio PÃºblico vs Privado

```bash
# En hf_config.env
HF_PRIVATE_REPO=false  # PÃºblico
HF_PRIVATE_REPO=true   # Privado (default)
```

### Configurar MÃºltiples Tokens

```bash
# Token especÃ­fico por repositorio
HF_TOKEN=mi_token_especial ./publish_to_hf.sh datos repo-especial
```

## ðŸš¨ SoluciÃ³n de Problemas

### Error: "Token invÃ¡lido"

```bash
# Reconfigurar token
rm ~/.cache/huggingface/token
python3 validate_hf_setup.py
```

### Error: "Repositorio no existe"

```bash
# El script crearÃ¡ automÃ¡ticamente el repositorio
# AsegÃºrate de tener permisos en la organizaciÃ³n
```

### Error: "Git LFS no instalado"

```bash
# Se instala automÃ¡ticamente, pero si falla:
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
git lfs install
```

### Archivos Muy Grandes

El sistema usa Git LFS automÃ¡ticamente para archivos >100MB.
Configurar en `hf_config.env`:

```bash
LFS_THRESHOLD_MB=50  # Usar LFS para archivos >50MB
```

## ðŸ“ˆ Optimizaciones

### Para Datasets Grandes

```bash
# Configurar uploads paralelos
MAX_PARALLEL_UPLOADS=10

# Aumentar timeout de red
NETWORK_TIMEOUT=600
```

### Monitoreo de Progreso

```bash
# Ver logs detallados
tail -f hf_upload.log

# Verificar progreso en HF
# https://huggingface.co/datasets/ale-neuratek/wikidump
```

## ðŸ”’ Consideraciones de Seguridad

- âœ… **Repositorio privado** por defecto
- âœ… **Token seguro** en directorio de usuario
- âœ… **Archivos sensibles** excluidos del git
- âœ… **AutenticaciÃ³n SSH** para git
- âœ… **Sin credenciales** en el cÃ³digo

## ðŸ“ž Soporte

Si encuentras problemas:

1. Ejecuta el validador: `python3 validate_hf_setup.py`
2. Revisa los logs: `cat hf_upload.log`
3. Prueba el demo: `./demo_hf.sh`
4. Verifica permisos en el repositorio HF

---

**Estado**: âœ… Completamente funcional  
**Ãšltima actualizaciÃ³n**: 2025-07-16  
**Autor**: Alejandro Iglesias - Neuratek.cl
