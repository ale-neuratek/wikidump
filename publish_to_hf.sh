#!/bin/bash
"""
ü§ó HUGGING FACE PUBLISHER - Publicaci√≥n independiente de datasets
================================================================
Script independiente para publicar datasets JSONL en Hugging Face
sin necesidad de ejecutar todo el pipeline de procesamiento.

USO:
    ./publish_to_hf.sh [directorio_datos] [nombre_repo]

EJEMPLOS:
    ./publish_to_hf.sh wiki-datasets ale-neuratek/wikidump
    ./publish_to_hf.sh ../wiki_conversations ale-neuratek/wikidump-v2
"""

set -e  # Salir en caso de error

# Valores por defecto
DEFAULT_DATA_DIR="wiki-datasets"
DEFAULT_REPO="ale-neuratek/wikidump"

# Parsear argumentos
DATA_DIR="${1:-$DEFAULT_DATA_DIR}"
REPO_NAME="${2:-$DEFAULT_REPO}"

echo "ü§ó PUBLICADOR DE DATASETS EN HUGGING FACE"
echo "=========================================="
echo "üìÅ Directorio de datos: $DATA_DIR"
echo "üì¶ Repositorio destino: $REPO_NAME"
echo ""

# Verificar que existe el directorio
if [ ! -d "$DATA_DIR" ]; then
    echo "‚ùå Error: Directorio no encontrado: $DATA_DIR"
    echo "üí° Uso: $0 [directorio_datos] [nombre_repo]"
    exit 1
fi

# Funci√≥n para verificar requisitos de Hugging Face
check_huggingface_setup() {
    echo "üîç Verificando configuraci√≥n de Hugging Face..."
    
    # Verificar si git-lfs est√° instalado
    if ! command -v git-lfs &> /dev/null; then
        echo "üì¶ Instalando Git LFS..."
        curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
        sudo apt-get install git-lfs
        git lfs install
    fi
    
    # Verificar si huggingface_hub est√° instalado
    if ! python3 -c "import huggingface_hub" &> /dev/null; then
        echo "üì¶ Instalando Hugging Face Hub..."
        pip3 install huggingface_hub
    fi
    
    echo "‚úÖ Dependencias de Hugging Face verificadas."
}

# Funci√≥n para configurar autenticaci√≥n HF
setup_huggingface_auth() {
    echo ""
    echo "üîê CONFIGURACI√ìN DE HUGGING FACE"
    echo "================================="
    echo ""
    
    # Verificar si ya hay token configurado
    if python3 -c "from huggingface_hub import HfApi; HfApi().whoami()" &> /dev/null; then
        echo "‚úÖ Token de Hugging Face ya configurado."
        return 0
    fi
    
    echo "Para publicar en Hugging Face necesitas:"
    echo "1. Una cuenta en https://huggingface.co"
    echo "2. Un token de acceso con permisos de escritura"
    echo ""
    echo "üìù PASOS PARA OBTENER EL TOKEN:"
    echo "1. Ve a https://huggingface.co/settings/tokens"
    echo "2. Crea un nuevo token con permisos 'Write'"
    echo "3. Copia el token generado"
    echo ""
    
    read -p "¬øTienes un token de Hugging Face? (y/n): " -n 1 -r
    echo
    
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo "‚ùå Sin token de Hugging Face, no se puede continuar."
        exit 1
    fi
    
    read -s -p "üîë Ingresa tu token de Hugging Face: " HF_TOKEN
    echo
    
    # Crear directorio cache si no existe
    mkdir -p "$HOME/.cache/huggingface"
    
    # Configurar token
    echo "$HF_TOKEN" | python3 -c "
from huggingface_hub import HfApi
import sys
import os
token = sys.stdin.read().strip()
try:
    api = HfApi(token=token)
    user = api.whoami()
    print(f'‚úÖ Token v√°lido para usuario: {user[\"name\"]}')
    # Guardar token
    cache_dir = os.path.expanduser('~/.cache/huggingface')
    os.makedirs(cache_dir, exist_ok=True)
    with open(os.path.join(cache_dir, 'token'), 'w') as f:
        f.write(token)
    exit(0)
except Exception as e:
    print(f'‚ùå Token inv√°lido: {e}')
    exit(1)
"
    
    return $?
}

# Funci√≥n para crear/actualizar repositorio HF
publish_to_huggingface() {
    local output_dir="$1"
    local repo_name="$2"
    
    echo "üì§ Publicando datasets en Hugging Face..."
    echo "Repositorio: $repo_name"
    echo "Datos desde: $output_dir"
    echo ""
    
    # Verificar que exista el directorio de salida
    if [ ! -d "$output_dir" ]; then
        echo "‚ùå Error: Directorio de salida no encontrado: $output_dir"
        return 1
    fi
    
    # Contar archivos JSONL
    local jsonl_count=$(find "$output_dir" -name "*.jsonl" | wc -l)
    if [ "$jsonl_count" -eq 0 ]; then
        echo "‚ùå Error: No se encontraron archivos JSONL en $output_dir"
        return 1
    fi
    
    echo "üìä Archivos encontrados: $jsonl_count archivos JSONL"
    
    # Crear script Python para la publicaci√≥n
    cat <<'EOF' > upload_to_hf.py
#!/usr/bin/env python3
import os
import sys
from pathlib import Path
from huggingface_hub import HfApi, create_repo
from datetime import datetime
import json

def upload_dataset(output_dir, repo_name):
    try:
        api = HfApi()
        
        # Verificar que el usuario tenga acceso
        user_info = api.whoami()
        print(f"üë§ Usuario autenticado: {user_info['name']}")
        
        # Crear repositorio si no existe (privado)
        try:
            create_repo(repo_name, private=True, repo_type="dataset")
            print(f"‚úÖ Repositorio {repo_name} creado (privado)")
        except Exception as e:
            if "already exists" in str(e).lower():
                print(f"‚ÑπÔ∏è Repositorio {repo_name} ya existe")
            else:
                print(f"‚ö†Ô∏è Advertencia al crear repositorio: {e}")
        
        # Buscar todos los archivos JSONL
        output_path = Path(output_dir)
        jsonl_files = list(output_path.rglob("*.jsonl"))
        
        if not jsonl_files:
            print(f"‚ùå No se encontraron archivos JSONL en {output_dir}")
            return False
        
        print(f"üìÅ Encontrados {len(jsonl_files)} archivos JSONL")
        
        # Calcular estad√≠sticas del dataset
        total_conversations = 0
        total_size = 0
        categories = set()
        
        print("üìä Analizando contenido del dataset...")
        for jsonl_file in jsonl_files:
            file_size = jsonl_file.stat().st_size
            total_size += file_size
            
            # Analizar contenido del archivo
            try:
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        if line.strip():
                            try:
                                data = json.loads(line)
                                total_conversations += 1
                                if 'category' in data:
                                    categories.add(data['category'])
                            except json.JSONDecodeError:
                                print(f"‚ö†Ô∏è L√≠nea {line_num} en {jsonl_file.name} no es JSON v√°lido")
            except Exception as e:
                print(f"‚ö†Ô∏è Error leyendo {jsonl_file.name}: {e}")
        
        # Crear README.md con metadatos detallados
        readme_content = f"""# Wikipedia Spanish Dataset

## Descripci√≥n
Dataset de Wikipedia en espa√±ol procesado autom√°ticamente para entrenamiento de modelos de lenguaje.
Contiene conversaciones estructuradas extra√≠das de art√≠culos de Wikipedia y organizadas por categor√≠as tem√°ticas.

## Estad√≠sticas
- **Fecha de generaci√≥n**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Total de conversaciones**: {total_conversations:,}
- **Archivos JSONL**: {len(jsonl_files)}
- **Categor√≠as √∫nicas**: {len(categories)}
- **Tama√±o total**: {total_size / 1024 / 1024:.1f} MB
- **Fuente**: Wikipedia dump eswiki-20250601

## Categor√≠as principales
{chr(10).join(f"- {cat}" for cat in sorted(list(categories)[:20]))}
{"..." if len(categories) > 20 else ""}

## Estructura del dataset
Cada archivo JSONL contiene conversaciones estructuradas organizadas por categor√≠as tem√°ticas.
La estructura garantiza m√°ximo 100 carpetas de categor√≠as (top 90 + 10 gen√©ricas).

## Formato de datos
```json
{{
  "conversation": [
    {{"role": "user", "content": "Pregunta sobre el tema"}},
    {{"role": "assistant", "content": "Respuesta basada en el art√≠culo de Wikipedia"}}
  ],
  "category": "categoria_tematica",
  "source": "wikipedia",
  "title": "T√≠tulo del art√≠culo de Wikipedia",
  "timestamp": "2025-01-XX"
}}
```

## Uso recomendado
```python
import json
from pathlib import Path

def load_conversations(dataset_dir):
    conversations = []
    for jsonl_file in Path(dataset_dir).rglob("*.jsonl"):
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    conv = json.loads(line)
                    conversations.append(conv)
    return conversations

# Cargar todas las conversaciones
conversations = load_conversations("path/to/dataset")
print(f"Cargadas {{len(conversations)}} conversaciones")
```

## Optimizaciones aplicadas
- **Hardware de alto rendimiento**: Optimizado para GH200, 8xH100
- **Procesamiento masivo**: Manejo eficiente de >1.3M art√≠culos
- **Categorizaci√≥n inteligente**: Limitaci√≥n autom√°tica a 100 categor√≠as
- **Calidad de datos**: Filtrado y validaci√≥n autom√°tica de contenido

## Licencia
Los datos de Wikipedia est√°n bajo [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/).
El c√≥digo de procesamiento est√° bajo [MIT License](https://opensource.org/licenses/MIT).

## Generado por
- **Pipeline**: Wikidump Processor v2.0
- **Repositorio**: https://github.com/ale-neuratek/wikidump
- **Autor**: Alejandro Iglesias - Neuratek.cl
"""
        
        # Subir README
        print("üìù Creando README.md...")
        api.upload_file(
            path_or_fileobj=readme_content.encode('utf-8'),
            path_in_repo="README.md",
            repo_id=repo_name,
            repo_type="dataset"
        )
        print("‚úÖ README.md subido")
        
        # Subir archivos JSONL manteniendo estructura de carpetas
        print(f"üì§ Subiendo {len(jsonl_files)} archivos...")
        uploaded_count = 0
        
        for jsonl_file in jsonl_files:
            file_size = jsonl_file.stat().st_size
            
            # Calcular ruta relativa para mantener estructura
            rel_path = jsonl_file.relative_to(output_path)
            
            print(f"üì§ [{uploaded_count+1}/{len(jsonl_files)}] {rel_path} ({file_size / 1024 / 1024:.1f} MB)")
            
            api.upload_file(
                path_or_fileobj=str(jsonl_file),
                path_in_repo=str(rel_path),
                repo_id=repo_name,
                repo_type="dataset"
            )
            uploaded_count += 1
        
        print(f"‚úÖ Dataset publicado exitosamente!")
        print(f"üìä Resumen:")
        print(f"   ‚Ä¢ Conversaciones: {total_conversations:,}")
        print(f"   ‚Ä¢ Archivos: {len(jsonl_files)}")
        print(f"   ‚Ä¢ Categor√≠as: {len(categories)}")
        print(f"   ‚Ä¢ Tama√±o: {total_size / 1024 / 1024:.1f} MB")
        print(f"üîó Repositorio: https://huggingface.co/datasets/{repo_name}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error durante la publicaci√≥n: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Uso: python3 upload_to_hf.py <output_dir> <repo_name>")
        sys.exit(1)
    
    output_dir = sys.argv[1]
    repo_name = sys.argv[2]
    
    success = upload_dataset(output_dir, repo_name)
    sys.exit(0 if success else 1)
EOF

    # Ejecutar script de publicaci√≥n
    python3 upload_to_hf.py "$output_dir" "$repo_name"
    local upload_status=$?
    
    # Limpiar script temporal
    rm -f upload_to_hf.py
    
    if [ $upload_status -eq 0 ]; then
        echo ""
        echo "üéâ ¬°Publicaci√≥n en Hugging Face completada!"
        echo "üîó Repositorio: https://huggingface.co/datasets/$repo_name"
        echo ""
        echo "üìã Pr√≥ximos pasos:"
        echo "1. Verificar el dataset en el repositorio"
        echo "2. Ajustar configuraci√≥n de privacidad si es necesario"
        echo "3. Compartir el enlace con tu equipo"
    else
        echo "‚ùå Error durante la publicaci√≥n en Hugging Face"
        return 1
    fi
    
    return $upload_status
}

# ========================
# FLUJO PRINCIPAL
# ========================

echo "üîß Verificando requisitos..."
check_huggingface_setup

echo ""
echo "üîê Configurando autenticaci√≥n..."
setup_huggingface_auth

echo ""
echo "üì§ Iniciando publicaci√≥n..."
publish_to_huggingface "$DATA_DIR" "$REPO_NAME"

echo ""
echo "üèÅ ¬°Proceso completado!"
